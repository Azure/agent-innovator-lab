{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Retrieval-score Check and Web search with [AutoGen](https://github.com/microsoft/autogen)\n",
    "\n",
    "This sample demonstrates how to evaluate the retrieval score of a RAG application and how to use the AutoGen library to search the web for relevant information.\n",
    "\n",
    "> ✨ **_Note_** <br>\n",
    "> Please check the Azure AI Agent Capability in [quick start page](https://learn.microsoft.com/en-us/azure/ai-services/agents/quickstart?pivots=programming-language-python-azure)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Configure a Python virtual environment for 3.10 or later:\n",
    "\n",
    "1.  open the Command Palette (Ctrl+Shift+P).\n",
    "1.  Search for Python: Create Environment.\n",
    "1.  select Venv / Conda and choose where to create the new environment.\n",
    "1.  Select the Python interpreter version. Create with version 3.10 or later.\n",
    "\n",
    "## Set up your environment\n",
    "\n",
    "Git clone the repository to your local machine.\n",
    "\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/Azure/rag-innovator-lab\n",
    "```\n",
    "\n",
    "Create a virtual environment and install the required packages.\n",
    "```bash\n",
    "python3 -m venv your_env_name\n",
    "source your_env_name/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Create an .env file based on the .env-sample file. Copy the new .env file to the folder containing your notebook and update the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔨 Current Support and Limitations (as of 2025-01-12)\n",
    "- Check the available models for bing grounding. Grounding with Bing Search only works with the following Azure OpenAI models: gpt-3.5-turbo-0125, gpt-4-0125-preview, gpt-4-turbo-2024-04-09, gpt-4o-0513 https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/bing-grounding?tabs=python&pivots=overview\n",
    "- Check the region support for the Azure AI Evaluation SDK. https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#region-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\n",
    "from autogen_core import DefaultTopicId, default_subscription, type_subscription\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import BingGroundingTool\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from autogen_core import (\n",
    "    MessageContext,\n",
    "    RoutedAgent,\n",
    "    SingleThreadedAgentRuntime,\n",
    "    TopicId,\n",
    "    TypeSubscription,\n",
    "    message_handler,\n",
    "    type_subscription,\n",
    ")\n",
    "from autogen_core.models import LLMMessage, ChatCompletionClient, SystemMessage, UserMessage, AssistantMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_core.tools import FunctionTool, Tool, ToolSchema\n",
    "from typing_extensions import Annotated\n",
    "from typing import List\n",
    "from autogen_core.tool_agent import ToolAgent, tool_agent_caller_loop\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\")\n",
    "aoai_chat_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "aoai_embedding_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "azure_openai_embedding_dimensions = int(os.getenv(\"AZURE_OPENAI_EMBEDDING_DIMENSIONS\", 1536))\n",
    "bing_connnection_name = os.getenv(\"BING_CONNECTION_NAME\")\n",
    "azure_ai_pjt_connection_str = os.getenv(\"AZURE_AI_PROJECT_CONNECTION_STR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Fun activities and attractions in Cornwall for tourists\"\n"
     ]
    }
   ],
   "source": [
    "# Query rewrite - Langchain based implementation \n",
    "from langchain_openai import AzureChatOpenAI \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = AzureChatOpenAI (\n",
    "    deployment_name=aoai_chat_deployment_name, \n",
    "    azure_endpoint=aoai_api_endpoint,\n",
    "    api_version=azure_openai_api_version,\n",
    "    openai_api_key=aoai_api_key,\n",
    "    )\n",
    "\n",
    "rewriter_prompt_template = \"\"\"\n",
    "Generate search keyword from a user question \n",
    "to be more specific, detailed, and likely to retrieve relevant information, allowing for a more accurate response through web search.\n",
    "Don't include the additional context from the user question.\n",
    "\n",
    "User question: {user_question}\n",
    "Revised web search query:\n",
    "\"\"\"\n",
    "\n",
    "rewriter_prompt = ChatPromptTemplate.from_template(rewriter_prompt_template)\n",
    "rewriter_chain = rewriter_prompt | llm | StrOutputParser()\n",
    "\n",
    "user_question =\"Tell me some fun things I can do in Cornwall\"\n",
    "\n",
    "search_query = rewriter_chain.invoke({\"user_question\": user_question})\n",
    "print(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    content: str\n",
    "    source: str\n",
    "\n",
    "@dataclass\n",
    "class ClassifiedMessage:\n",
    "    intent: str\n",
    "    content: str\n",
    "    source: str\n",
    "\n",
    "default_topic_type = \"default\"\n",
    "general_type = \"GeneralAgent\"\n",
    "websearch_type = \"WebSearchAgent\"\n",
    "user_topic_type = \"User\"\n",
    "retrieve_topic_type = \"RetrieveAgent\"\n",
    "groundness_topic_type = \"EvalAgent\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the azclient\n",
    "aoai_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_endpoint=aoai_api_endpoint,\n",
    "    model = aoai_chat_deployment_name,\n",
    "    api_version=azure_openai_api_version,\n",
    "    api_key=aoai_api_key\n",
    ")\n",
    "\n",
    "from pydantic import BaseModel\n",
    "class IntentType(BaseModel):\n",
    "    type: str\n",
    "\n",
    "# Step 1: Create an client object which will contain the connection string\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    # conn_str='Your Azure AI Foundation Connection String',\n",
    "    # copied from your Azure AI Foundry project.\n",
    "    conn_str=azure_ai_pjt_connection_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@type_subscription(topic_type=default_topic_type)\n",
    "class IntentRouterAgent(RoutedAgent):\n",
    "    def __init__(self, model_client: ChatCompletionClient) -> None:\n",
    "        super().__init__(\"A Intent classification agent.\")\n",
    "        self._system_message = SystemMessage(\n",
    "            content=(\n",
    "                \"\"\"\n",
    "                    You are an expert at routing a user question to LLM or vectorstore or websearch.\n",
    "                    The LLM covers casual topic such as greeting, small talks.\n",
    "                    Use the LLM for questions on casual topics.\n",
    "                    The vectorstore contains documents related to hotel information in New York.\n",
    "                    Use the vectorstore for questions on the hotel related topics. For all else, websearch.\n",
    "                    response inent_type such as LLM, vectorstore, or websearch.\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "        self._model_client = model_client\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_user_question(self, message: UserMessage, ctx: MessageContext) -> None:\n",
    "        print(f\"{'-'*80}\\n{self.id.type}:\\n{message.content}\")\n",
    "        prompt = f\"User question: {message.content}\"\n",
    "        llm_result = await self._model_client.create(\n",
    "            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n",
    "            extra_create_args={\"response_format\": IntentType},\n",
    "            cancellation_token=ctx.cancellation_token\n",
    "        )\n",
    "        response = llm_result.content\n",
    "        assert isinstance(response, str)\n",
    "        print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n",
    "        \n",
    "        await self.publish_message(ClassifiedMessage(intent=json.loads(response)[\"type\"], content=prompt, source=self.id.key), topic_id=TopicId(type=retrieve_topic_type, source=\"default\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@type_subscription(topic_type=retrieve_topic_type)\n",
    "class GeneralAgent(RoutedAgent):\n",
    "    def __init__(self, model_client: ChatCompletionClient) -> None:\n",
    "        super().__init__(\"A general agent.\")\n",
    "        self._system_message = SystemMessage(\n",
    "            content=(\n",
    "                \"\"\"\n",
    "                    You are an helper agent that can answer general questions.\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "        self._model_client = model_client\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_user_question(self, message: ClassifiedMessage, ctx: MessageContext) -> None:\n",
    "        print(message)\n",
    "        if(message.intent == \"LLM\"):\n",
    "            prompt = message.content\n",
    "            llm_result = await self._model_client.create(\n",
    "                messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n",
    "                cancellation_token=ctx.cancellation_token,\n",
    "            )\n",
    "            response = llm_result.content\n",
    "            assert isinstance(response, str)\n",
    "            print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n",
    "            \n",
    "            await self.publish_message(AssistantMessage(content=response, source=self.id.key), topic_id=TopicId(type=user_topic_type, source=self.id.key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and connect a new Grounding with Bing Search resource\n",
    "> ✨ **_Note_** <br>\n",
    "> Your usage of Grounding with Bing Search may incur costs. See the pricing page for details. [pricing](https://www.microsoft.com/en-us/bing/apis/grounding-pricing)\n",
    "\n",
    "1. Create a new Grounding with Bing Search resource in the Azure portal, and select the different fields in the creation form. Make sure you create this Grounding with Bing Search resource in the same resource group as your Azure AI Agent, AI Project, and other resources.\n",
    "![bing grounding](https://learn.microsoft.com/en-us/azure/ai-services/agents/media/tools/bing/resource-azure-portal.png#lightbox)\n",
    "2. Select the Grounding with Bing Search resource you have created and copy any of the API keys.\n",
    "![copy keys](https://learn.microsoft.com/en-us/azure/ai-services/agents/media/tools/bing/key-resource-azure-portal.png#lightbox)\n",
    "3. Go to the Azure AI Foundry > management center > Create Connection > API key \n",
    "- Endpoint: https://api.bing.microsoft.com/\n",
    "- Key: YOUR_API_KEY\n",
    "- Connection name: YOUR_CONNECTION_NAME. This name will be used in the notebook to connect to the Bing Search resource.\n",
    "- Access: you can choose either this project only or shared to all projects.\n",
    "4. Copy your connection name and paste it in the .env file.\n",
    "5. Copy your project connection string and paste it in the .env file.\n",
    "![copy project string](../images/portal-connection-string.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Enable the Grounding with Bing search tool\n",
    "# Azure Open AI connection이 많은 경우 에러가 날 수 있으니 주의 \n",
    "# https://github.com/Azure/azure-sdk-for-python/issues/38921\n",
    "\n",
    "\n",
    "bing_connection = project_client.connections.get(\n",
    "    # connection_name='Your Bing Connection Name'\n",
    "    connection_name=bing_connnection_name\n",
    ")\n",
    "\n",
    "conn_id = bing_connection.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To bing grounding connection Test\n",
    "# once you test the connection, you need to restart the kernel to use the new connection in the other code block\n",
    "\n",
    "bing = BingGroundingTool(connection_id=conn_id)\n",
    "\n",
    "agent = project_client.agents.create_agent(\n",
    "    model=aoai_chat_deployment_name,\n",
    "    name=\"my-assistant\",\n",
    "    instructions=\"\"\"\n",
    "        web search\n",
    "    \"\"\",\n",
    "    tools=bing.definitions,\n",
    "    headers={\"x-ms-enable-preview\": \"true\"}\n",
    ")\n",
    "print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "# Create thread for communication\n",
    "thread = project_client.agents.create_thread()\n",
    "print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "# Create message to thread\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"삼성전자 CES 2025 모니터 신모델\",\n",
    ")\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core.tools import FunctionTool, ToolSchema\n",
    "\n",
    "# Step 3: Create agent using project client with the bing tool and process assistant run\n",
    "async def bing_search_tool(query: str) -> str:\n",
    "    print(\"This is Bing for Azure AI Agent Service .......\")\n",
    "    bing = BingGroundingTool(connection_id=conn_id)\n",
    "    print(f\"query ID: {query}\")\n",
    "    print(project_client)\n",
    "        \n",
    "    \n",
    "    agent = project_client.agents.create_agent(\n",
    "        model=aoai_chat_deployment_name,\n",
    "        name=\"bing-search-assistant\",\n",
    "        instructions=\"\"\"\n",
    "            Your only tool is websearch_tool - use it to find information.\n",
    "            You make only one search call at a time.\n",
    "            Once you have the results, you never do calculations based on them.\n",
    "        \"\"\",\n",
    "        tools=bing.definitions,\n",
    "        headers={\"x-ms-enable-preview\": \"true\"}\n",
    "    )\n",
    "    print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "    # Create thread for communication\n",
    "    thread = project_client.agents.create_thread()\n",
    "    print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "    # Create message to thread\n",
    "    message = project_client.agents.create_message(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=query,\n",
    "    )\n",
    "    print(f\"SMS: {message}\")\n",
    "    # Create and process agent run in thread with tools\n",
    "    run = project_client.agents.create_and_process_run(thread_id=thread.id, assistant_id=agent.id)\n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "    if run.status == \"failed\":\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "    # Delete the assistant when done\n",
    "    project_client.agents.delete_agent(agent.id)\n",
    "    print(\"Deleted agent\")\n",
    "\n",
    "    # Fetch and log all messages\n",
    "    messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "    print(\"Messages:\"+ messages[\"data\"][0][\"content\"][0][\"text\"][\"value\"])\n",
    "    return messages[\"data\"][0][\"content\"][0][\"text\"][\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@type_subscription(topic_type=retrieve_topic_type)\n",
    "class WebSearchUseAgent(RoutedAgent):\n",
    "    def __init__(self, model_client: ChatCompletionClient, tool_schema: List[ToolSchema], tool_agent_type: str) -> None:\n",
    "        super().__init__(\"Use tools to solve tasks.\")\n",
    "        self._model_client = model_client\n",
    "        self._system_message = SystemMessage(\n",
    "            content=(\n",
    "                \"\"\"\n",
    "                You are a tool usage agent who can delegate your task to the ToolAgent.\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "        self._tool_schema = tool_schema\n",
    "        self._tool_agent_id = AgentId(tool_agent_type, self.id.key)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_user_question(self, message: ClassifiedMessage, ctx: MessageContext) -> None:\n",
    "        if(message.intent == \"websearch\"):\n",
    "            \n",
    "            # Create a session of messages.\n",
    "            session: List[LLMMessage] = [UserMessage(content=message.content, source=\"user\")]\n",
    "            # Run the caller loop to handle tool calls.\n",
    "            messages = await tool_agent_caller_loop(\n",
    "                self,\n",
    "                tool_agent_id=self._tool_agent_id,\n",
    "                model_client=self._model_client,\n",
    "                input_messages=session,\n",
    "                tool_schema=self._tool_schema,\n",
    "                cancellation_token=ctx.cancellation_token,\n",
    "            )\n",
    "\n",
    "            # Return the final response.\n",
    "            assert isinstance(messages[-1].content, str)\n",
    "            await self.publish_message(AssistantMessage(content=messages[-1].content, source=self.id.key), topic_id=TopicId(type=user_topic_type, source=self.id.key))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@type_subscription(topic_type=user_topic_type)\n",
    "class UserAgent(RoutedAgent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"A user agent that outputs the final copy to the user.\")\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_final_copy(self, message: AssistantMessage, ctx: MessageContext) -> None:\n",
    "        print(f\"\\n{'-'*80}\\n{self.id.type} received final copy:\\n{message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentType(type='User')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime = SingleThreadedAgentRuntime()\n",
    "\n",
    "tools: List[Tool] = [FunctionTool(bing_search_tool, description=\"web search tool\")]\n",
    "\n",
    "await IntentRouterAgent.register(runtime, type=default_topic_type, factory=lambda: IntentRouterAgent(model_client=aoai_client))\n",
    "\n",
    "await GeneralAgent.register(runtime, type=general_type, factory=lambda: GeneralAgent(model_client=aoai_client))\n",
    "\n",
    "await ToolAgent.register(runtime, \"tool_executor_agent\", lambda: ToolAgent(\"tool_executor_agent\", tools))\n",
    "\n",
    "await WebSearchUseAgent.register(runtime, type=websearch_type, factory=lambda: WebSearchUseAgent(aoai_client, [tool.schema for tool in tools], \"tool_executor_agent\"))\n",
    "\n",
    "await UserAgent.register(runtime, type=user_topic_type, factory=lambda: UserAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "default:\n",
      "삼성전자 CES 2025 모니터 신모델\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "default:\n",
      "{\"type\":\"websearch\"}\n",
      "ClassifiedMessage(intent='websearch', content='User question: 삼성전자 CES 2025 모니터 신모델', source='default')\n",
      "This is Bing for Azure AI Agent Service .......\n",
      "query ID: 삼성전자 CES 2025 모니터 신모델\n",
      "<azure.ai.projects._patch.AIProjectClient object at 0x7f3fc39f3e50>\n",
      "Created agent, ID: asst_idILSFOvwT3GVA1sobIV5KEY\n",
      "Created thread, ID: thread_dQtDTtIwGPWjgOhCBiGSgEr1\n",
      "SMS: {'id': 'msg_4dguem1NYgsOwWjNVyZS6TsP', 'object': 'thread.message', 'created_at': 1736556043, 'assistant_id': None, 'thread_id': 'thread_dQtDTtIwGPWjgOhCBiGSgEr1', 'run_id': None, 'role': 'user', 'content': [{'type': 'text', 'text': {'value': '삼성전자 CES 2025 모니터 신모델', 'annotations': []}}], 'attachments': [], 'metadata': {}}\n",
      "Run finished with status: completed\n",
      "Deleted agent\n",
      "Messages:삼성전자는 CES 2025에서 총 5종의 새로운 모니터를 공개했습니다. 새로운 모니터 모델에는 다음과 같은 제품들이 포함되었습니다:\n",
      "\n",
      "1. **스마트 모니터 M9**: AI 기능이 대폭 향상된 32형 스마트 모니터.\n",
      "2. **오디세이 OLED G8**: 업계 최초로 27형 크기에 4K 해상도와 240Hz 주사율을 지원하는 OLED 모니터.\n",
      "3. **오디세이 OLED G6**: 고성능 OLED 게이밍 모니터.\n",
      "4. **오디세이 3D**: 새로운 3D 기술이 적용된 모니터.\n",
      "5. **뷰피니티 S8**: 상세 모델명과 함께 고급 기능을 제공하는 모니터【0†source】【1†source】【2†source】.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "User received final copy:\n",
      "삼성전자는 CES 2025에서 총 5종의 새로운 모니터를 공개했습니다. 주요 신모델들은 다음과 같습니다:\n",
      "\n",
      "1. **스마트 모니터 M9**: AI 기능이 대폭 향상된 32형 스마트 모니터입니다.\n",
      "2. **오디세이 OLED G8**: 업계 최초로 27형 크기에 4K 해상도와 240Hz 주사율을 지원하는 OLED 모니터입니다.\n",
      "3. **오디세이 OLED G6**: 고성능 OLED 게이밍 모니터입니다.\n",
      "4. **오디세이 3D**: 새로운 3D 기술이 적용된 모니터입니다.\n",
      "5. **뷰피니티 S8**: 상세 모델명과 함께 고급 기능을 제공하는 모니터입니다.\n",
      "\n",
      "이 신모델들은 각기 다른 첨단 기술과 디자인을 도입하여 사용자들에게 다양한 선택지를 제공합니다.\n"
     ]
    }
   ],
   "source": [
    "runtime.start()\n",
    "\n",
    "await runtime.publish_message(UserMessage(content=\"삼성전자 CES 2025 모니터 신모델\", source=\"User\"), topic_id=TopicId(type=default_topic_type, source=\"default\"))\n",
    "\n",
    "await runtime.stop_when_idle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
