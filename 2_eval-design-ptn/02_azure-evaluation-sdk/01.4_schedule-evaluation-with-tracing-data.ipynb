{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule an Online Evaluation with Tracing and AI Inference SDK\n",
    "- You can configure the RecurrenceTrigger based on the class definition here. The code below demonstrates how to configure the RecurrenceTrigger to run the evaluation every 24 hours. You can also configure the trigger to run at a different interval, or at a specific time of day. Check the Trace menu in the Azure AI Foundry to see the results of the evaluation.\n",
    "- reference: https://learn.microsoft.com/en-us/azure/ai-studio/how-to/online-evaluation\n",
    "\n",
    "> âœ¨ ***Note*** <br>\n",
    "> You application insight need to be created in Azure AI Foundry to trace your generative AI application. <br>\n",
    "> Prior to setting up online evaluation, ensure you have first set up [tracing for your generative AI application using Azure AI Inference SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/trace-local-sdk?tabs=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    Dataset,\n",
    "    EvaluatorConfiguration,\n",
    "    ConnectionType,\n",
    "    EvaluationSchedule,\n",
    "    RecurrenceTrigger,\n",
    "    ApplicationInsightsConfiguration\n",
    ")\n",
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    "    F1ScoreEvaluator,\n",
    "    RetrievalEvaluator\n",
    ")\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "azure_ai_project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ.get(\"AZURE_AI_PROJECT_CONN_STR\"),  # At the moment, it should be in the format \"<Region>.api.azureml.ms;<AzureSubscriptionId>;<ResourceGroup>;<HubName>\" Ex: eastus2.api.azureml.ms;xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxxxx;rg-sample;sample-project-eastus2\n",
    ")\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"azure_openai\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application Insights connection string: InstrumentationKey=be5a6225-5366-4ac5-948c-0e7679968305;IngestionEndpoint=https://swedencentral-0.in.applicationinsights.azure.com/;LiveEndpoint=https://swedencentral.livediagnostics.monitor.azure.com/;ApplicationId=d6e7dbde-b332-4538-a623-73a26b12676e\n"
     ]
    }
   ],
   "source": [
    "application_insights_connection_string = azure_ai_project_client.telemetry.get_connection_string()\n",
    "if not application_insights_connection_string:\n",
    "    print(\"Application Insights was not enabled for this project.\")\n",
    "    print(\"Enable it via the 'Tracing' tab in your Azure AI Foundry project page.\")\n",
    "    exit()\n",
    "\n",
    "from azure.core.settings import settings \n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/trace-local-sdk?tabs=python\n",
    "os.environ['AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED'] = 'true'\n",
    "settings.tracing_implementation = \"opentelemetry\" \n",
    "\n",
    "configure_azure_monitor(connection_string=application_insights_connection_string)\n",
    "\n",
    "print(f\"Application Insights connection string: {application_insights_connection_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.tracing import AIInferenceInstrumentor \n",
    "\n",
    "# Instrument AI Inference API to enable trace instrumentation for AI Inference\n",
    "AIInferenceInstrumentor().instrument() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initialized azure_ai_inference_client ===\n",
      "AZURE_AI_INFERENCE_ENDPOINT=https://aoai-services1.services.ai.azure.com/models\n",
      "AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "aoai_inference_endpoint = os.getenv(\"AZURE_AI_INFERENCE_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "try:\n",
    "    azure_ai_inference_client = ChatCompletionsClient(\n",
    "        endpoint = aoai_inference_endpoint,\n",
    "        credential = AzureKeyCredential(aoai_api_key),\n",
    "    )\n",
    "\n",
    "    print(\"=== Initialized azure_ai_inference_client ===\")\n",
    "    print(f\"AZURE_AI_INFERENCE_ENDPOINT={aoai_inference_endpoint}\")\n",
    "    print(f\"AZURE_OPENAI_DEPLOYMENT_NAME={aoai_deployment_name}\")\n",
    "        \n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"no\": 1, \"query\": \"What are the warranty options for Contoso Electronics products?\", \"response\": \"Contoso Electronics offers a one-year standard warranty on all products.\"}\n",
      "{\"no\": 2, \"query\": \"How can I track my order from Contoso Electronics?\", \"response\": \"You can track your order using the tracking link sent to your email.\"}\n",
      "{\"no\": 3, \"query\": \"What is the return policy for Contoso Electronics?\", \"response\": \"You can return products within 30 days for a full refund.\"}\n",
      "{\"no\": 4, \"query\": \"Do you offer technical support for your products?\", \"response\": \"Yes, we provide technical support via phone and online chat.\"}\n",
      "{\"no\": 5, \"query\": \"How do I reset my Contoso Electronics device?\", \"response\": \"To reset your device, press and hold the reset button for 10 seconds.\"}\n",
      "{\"no\": 6, \"query\": \"What payment methods are accepted at Contoso Electronics?\", \"response\": \"We accept credit cards, PayPal, and bank transfers.\"}\n",
      "{\"no\": 7, \"query\": \"Can I change my order after it has been placed?\", \"response\": \"You can change your order within one hour of placing it.\"}\n",
      "{\"no\": 8, \"query\": \"Where can I find the user manual for my product?\", \"response\": \"User manuals are available for download on our website.\"}\n",
      "{\"no\": 9, \"query\": \"Is there a student discount available?\", \"response\": \"Yes, we offer a 10% discount for students with valid ID.\"}\n",
      "{\"no\": 10, \"query\": \"How do I contact customer service?\", \"response\": \"You can contact customer service via phone or email.\"}\n",
      "{\"no\": 11, \"query\": \"What should I do if my product is defective?\", \"response\": \"Please contact customer service for assistance with defective products.\"}\n",
      "{\"no\": 12, \"query\": \"Are there any ongoing promotions at Contoso Electronics?\", \"response\": \"Yes, check our website for the latest promotions and discounts.\"}\n",
      "{\"no\": 13, \"query\": \"How long does shipping take?\", \"response\": \"Shipping typically takes 3 to 5 business days.\"}\n",
      "{\"no\": 14, \"query\": \"Can I pick up my order in-store?\", \"response\": \"Yes, in-store pickup is available for local customers.\"}\n",
      "{\"no\": 15, \"query\": \"What is the process for exchanging a product?\", \"response\": \"To exchange a product, contact customer service for instructions.\"}\n",
      "{\"no\": 16, \"query\": \"Do you offer gift cards?\", \"response\": \"Yes, we offer gift cards in various denominations.\"}\n",
      "{\"no\": 17, \"query\": \"What is the best way to clean my electronics?\", \"response\": \"Use a microfiber cloth and gentle cleaning solution.\"}\n",
      "{\"no\": 18, \"query\": \"How can I update the software on my device?\", \"response\": \"You can update the software through the settings menu.\"}\n",
      "{\"no\": 19, \"query\": \"What are the features of the latest Contoso smartphone?\", \"response\": \"The latest smartphone features a high-resolution camera and long battery life.\"}\n",
      "{\"no\": 20, \"query\": \"Can I return an opened product?\", \"response\": \"Opened products can be returned if they are defective.\"}\n",
      "{\"no\": 21, \"query\": \"What is the maximum storage capacity for your tablets?\", \"response\": \"Our tablets support up to 512GB of storage.\"}\n",
      "{\"no\": 22, \"query\": \"How do I set up my new Contoso laptop?\", \"response\": \"Follow the setup guide included in the box.\"}\n",
      "{\"no\": 23, \"query\": \"Is there a loyalty program for frequent customers?\", \"response\": \"Yes, we have a loyalty program that rewards frequent shoppers.\"}\n",
      "{\"no\": 24, \"query\": \"What accessories are compatible with my device?\", \"response\": \"You can find compatible accessories listed on our website.\"}\n",
      "{\"no\": 25, \"query\": \"How do I enable Bluetooth on my device?\", \"response\": \"Go to settings and toggle the Bluetooth option.\"}\n",
      "{\"no\": 26, \"query\": \"What should I do if I forgot my password?\", \"response\": \"Use the 'Forgot Password' link on the login page.\"}\n",
      "{\"no\": 27, \"query\": \"Are your products environmentally friendly?\", \"response\": \"Yes, we prioritize sustainability in our product design.\"}\n",
      "{\"no\": 28, \"query\": \"How can I unsubscribe from your newsletter?\", \"response\": \"Click the unsubscribe link at the bottom of the newsletter.\"}\n",
      "{\"no\": 29, \"query\": \"What is the battery life of your wireless headphones?\", \"response\": \"Our wireless headphones offer up to\n",
      "Usage Information:\n",
      "Completion Tokens: 1024\n",
      "Prompt Tokens: 215\n",
      "Total Tokens: 1239\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "\n",
    "NUM_SAMPLES = 20\n",
    "\n",
    "LANGUAGE = \"English\"\n",
    "\n",
    "topic = f\"\"\"\n",
    "Contoso Electronics call center QnA related expected spoken utterances for {LANGUAGE} languages. create {NUM_SAMPLES} lines of jsonl of the topic related text.\n",
    "\"\"\"\n",
    "\n",
    "example = \"\"\"\n",
    "Here is examples of the expected format:\n",
    "{\"query\":\"How do you create a run?\",\"context\":\"AML API only\",\"response\":\"To create a run using the Azure Machine Learning API, you first need to create an Experiment. Once you have an experiment, you can create a Run object that is associated with that experiment. Here is some Python code that demonstrates this process:\\\\n\\\\n```\\\\nfrom azureml.core import Experiment, Run\\\\nfrom azureml.core.workspace import Workspace\\\\n\\\\n# Define workspace and experiment\\\\nws = Workspace.from_config()\\\\nexp = Experiment(workspace=ws, name='my_experiment')\\\\n\\\\n# Create a new run\\\\nrun = exp.start_logging()\\\\n```\\\\n\\\\nIn this code, the `from_config()` method reads the configuration file that you created when you set up your Azure Machine Learning workspace. The `Experiment` constructor creates an Experiment object that is associated with your workspace, and the `start_logging()` method creates a new Run object that is associated with the Experiment. Now you can use the `run` object to log metrics, upload files, and track other information related to your machine learning experiment.\",\"ground_truth\":\"Paris is the capital of France.\"}\n",
    "{\"query\":\"How do you log a model?\",\"context\":\"Logging can be done using any OSS Sdk\",\"response\":\"There are a few ways to log models in Azure Machine Learning. \\\\n\\\\nOne way is to use the `register_model()` method of the `Run` object. The `register_model()` method logs a model file in the Azure Machine Learning service workspace and makes it available for deployment. Here's an example:\\\\n\\\\n```python\\\\nfrom azureml.core import Model\\\\n\\\\nmodel_path = './outputs/my_model.pkl'\\\\nmodel = Model.register(workspace=ws, model_path=model_path, model_name='my_model')\\\\n```\\\\n\\\\nThis code registers the model file located at `model_path` to the Azure Machine Learning service workspace with the name `my_model`. \\\\n\\\\nAnother way to log a model is to save it as an output of a `Run`. If your model generation code is part of a script or Jupyter notebook that runs as an Azure Machine Learning experiment, you can save the model file as an output of the `Run` object. Here's an example:\\\\n\\\\n```python\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom azureml.core.run import Run\\\\n\\\\n# Initialize a run object\\\\nrun = Run.get_context()\\\\n\\\\n# Train your model\\\\nX_train, y_train = ...\\\\nlog_reg = LogisticRegression().fit(X_train, y_train)\\\\n\\\\n# Save the model to the Run object's outputs directory\\\\nmodel_path = 'outputs/model.pkl'\\\\njoblib.dump(value=log_reg, filename=model_path)\\\\n\\\\n# Log the model as a run artifact\\\\nrun.upload_file(name=model_path, path_or_stream=model_path)\\\\n```\\\\n\\\\nIn this code, `Run.get_context()` retrieves the current run context object, which you can use to track metadata and metrics for the run. After training your model, you can use `joblib.dump()` to save the model to a file, and then log the file as an artifact of the run using `run.upload_file()`.\",\"ground_truth\":\"Paris is the capital of France.\"}\n",
    "{\"query\":\"What is the capital of France?\",\"context\":\"France is in Europe\",\"response\":\"Paris is the capital of France.\",\"ground_truth\":\"Paris is the capital of France.\"}\n",
    "\"\"\"\n",
    "\n",
    "system_message = \"\"\"\n",
    "Generate plain text sentences of #topic# related text to improve the recognition of domain-specific words and phrases.\n",
    "Domain-specific words can be uncommon or made-up words, but their pronunciation must be straightforward to be recognized. \n",
    "Use text data that's close to the expected spoken utterances. The nummber of utterances per line should be 1. \n",
    "jsonl format is required. use 'no' as number, 'query' as string, 'context' as string, 'response' as string, and 'ground_truth' as string.\n",
    "only include the lines as the result. Do not include ```jsonl, ``` and blank line in the result. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "#topic#: {topic}\n",
    "Example: {example}\n",
    "\"\"\"\n",
    "\n",
    "# Simple API Call\n",
    "response = azure_ai_inference_client.complete(\n",
    "    model=aoai_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    top_p=0.1\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "print(content)\n",
    "print(\"Usage Information:\")\n",
    "#print(f\"Cached Tokens: {response.usage.prompt_tokens_details.cached_tokens}\") #only o1 models support this\n",
    "print(f\"Completion Tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Prompt Tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Total Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up online evaluation schedule\n",
    "\n",
    "Evaluations are only supported in the same regions as AI-assisted risk and safety metrics.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- A new User-assigned Managed Identity in the same resource group and region. Make a note of the clientId; you'll need it later.\n",
    "- An Azure AI Hub in the same resource group and region.\n",
    "- An Azure AI project in this hub, see Create a project in Azure AI Foundry portal.\n",
    "- An Azure Monitor Application Insights resource created in Azure AI Foundry portal.\n",
    "- Navigate to the hub page in Azure portal and add Application Insights resource, see Update Azure Application Insights\n",
    "- Azure OpenAI Deployment with GPT model supporting chat completion, for example gpt-4.\n",
    "- Navigate to your Application Insights resource in the Azure portal and use the Access control (IAM) tab to add the Log Analytics Contributor role to the User-assigned Managed Identity you created previously.\n",
    "? Attach the User-assigned Managed Identity to your project.?\n",
    "- Navigate to your Azure AI Services in the Azure portal and use the Access control (IAM) tab to add the Cognitive Services OpenAI Contributor role to the User-assigned Managed Identity you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id for each evaluator can be found in your AI Studio registry - please see documentation for more information\n",
    "# init_params is the configuration for the model to use to perform the evaluation\n",
    "# data_mapping is used to map the output columns of your query to the names required by the evaluator\n",
    "# Evaluator parameter format - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#evaluator-parameter-format\n",
    "evaluators_cloud = {\n",
    "    \"f1_score\": EvaluatorConfiguration(\n",
    "        id=F1ScoreEvaluator.id,\n",
    "    ),\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=RelevanceEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    \"groundedness\": EvaluatorConfiguration(\n",
    "        id=GroundednessEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    # \"retrieval\": EvaluatorConfiguration(\n",
    "    #     #from azure.ai.evaluation._evaluators._common.math import list_mean_nan_safe\\nModuleNotFoundError: No module named 'azure.ai.evaluation._evaluators._common.math'\n",
    "    #     #id=RetrievalEvaluator.id,\n",
    "    #     id=\"azureml://registries/azureml/models/Retrieval-Evaluator/versions/2\",\n",
    "    #     init_params={\"model_config\": model_config},\n",
    "    #     data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    # ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=CoherenceEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=FluencyEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "     \"similarity\": EvaluatorConfiguration(\n",
    "        # currently bug in the SDK, please use the id below\n",
    "        #id=SimilarityEvaluator.id,\n",
    "        id=\"azureml://registries/azureml/models/Similarity-Evaluator/versions/3\",\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kusto_query = 'let gen_ai_spans=(dependencies | where isnotnull(customDimensions[\"gen_ai.system\"]) | extend response_id = tostring(customDimensions[\"gen_ai.response.id\"]) | project id, operation_Id, operation_ParentId, timestamp, response_id); let gen_ai_events=(traces | where message in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") or tostring(customDimensions[\"event.name\"]) in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") | project id= operation_ParentId, operation_Id, operation_ParentId, user_input = iff(message == \"gen_ai.user.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.user.message\", parse_json(iff(message == \"gen_ai.user.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), system = iff(message == \"gen_ai.system.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.system.message\", parse_json(iff(message == \"gen_ai.system.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), llm_response = iff(message == \"gen_ai.choice\", parse_json(tostring(parse_json(tostring(customDimensions[\"gen_ai.event.content\"])).message)).content, iff(tostring(customDimensions[\"event.name\"]) == \"gen_ai.choice\", parse_json(parse_json(message).message).content, \"\")) | summarize operation_ParentId = any(operation_ParentId), Input = maxif(user_input, user_input != \"\"), System = maxif(system, system != \"\"), Output = maxif(llm_response, llm_response != \"\") by operation_Id, id); gen_ai_spans | join kind=inner (gen_ai_events) on id, operation_Id | project Input, System, Output, operation_Id, operation_ParentId, gen_ai_response_id = response_id'\n",
    "\n",
    "# AzureMSIClientId is the clientID of the User-assigned managed identity created during set-up - see documentation for how to find it\n",
    "properties = {\"AzureMSIClientId\": os.environ.get(\"AZURE_MSI_CLIENT_ID\")}\n",
    "\n",
    "service_name = \"evaluation_sdk_schedule\"\n",
    "\n",
    "# Your Application Insights resource ID\n",
    "# At the moment, it should be something in the format \"/subscriptions/<AzureSubscriptionId>/resourceGroups/<ResourceGroup>/providers/Microsoft.Insights/components/<ApplicationInsights>\"\"\n",
    "#app_insights_resource_id = os.environ.get(\"APP_INSIGHTS_RESOURCE_ID\")\n",
    "app_insights_resource_id = \"/subscriptions/3d4d3dd0-79d4-40cf-a94e-b4154812c6ca/resourceGroups/AOAI-group3/providers/microsoft.insights/components/my-app-insight\"\n",
    "\n",
    "# Connect to your Application Insights resource\n",
    "app_insights_config = ApplicationInsightsConfiguration(\n",
    "    resource_id=app_insights_resource_id, query=kusto_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted the online evaluation schedule creation request - evaluation_sdk_schedule, currently in Creating state.\n"
     ]
    }
   ],
   "source": [
    "# Frequency to run the schedule\n",
    "recurrence_trigger = RecurrenceTrigger(frequency=\"hour\", interval=1)\n",
    "\n",
    "# Configure the online evaluation schedule\n",
    "evaluation_schedule = EvaluationSchedule(\n",
    "    data=app_insights_config,\n",
    "    evaluators=evaluators_cloud,\n",
    "    trigger=recurrence_trigger,\n",
    "    description=f\"scheduled evaluation\",\n",
    "    properties=properties\n",
    ")\n",
    "\n",
    "# Create the online evaluation schedule\n",
    "created_evaluation_schedule = azure_ai_project_client.evaluations.create_or_replace_schedule(service_name, evaluation_schedule)\n",
    "print(\n",
    "    f\"Successfully submitted the online evaluation schedule creation request - {created_evaluation_schedule.name}, currently in {created_evaluation_schedule.provisioning_state} state.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'evaluation_sdk_schedule', 'description': 'scheduled evaluation', 'isEnabled': True, 'provisioningState': 'Succeeded', 'data': {'type': 'AppInsights', 'resourceId': '/subscriptions/3d4d3dd0-79d4-40cf-a94e-b4154812c6ca/resourceGroups/AOAI-group3/providers/microsoft.insights/components/application_insight01', 'query': 'let gen_ai_spans=(dependencies | where isnotnull(customDimensions[\"gen_ai.system\"]) | extend response_id = tostring(customDimensions[\"gen_ai.response.id\"]) | project id, operation_Id, operation_ParentId, timestamp, response_id); let gen_ai_events=(traces | where message in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") or tostring(customDimensions[\"event.name\"]) in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") | project id= operation_ParentId, operation_Id, operation_ParentId, user_input = iff(message == \"gen_ai.user.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.user.message\", parse_json(iff(message == \"gen_ai.user.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), system = iff(message == \"gen_ai.system.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.system.message\", parse_json(iff(message == \"gen_ai.system.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), llm_response = iff(message == \"gen_ai.choice\", parse_json(tostring(parse_json(tostring(customDimensions[\"gen_ai.event.content\"])).message)).content, iff(tostring(customDimensions[\"event.name\"]) == \"gen_ai.choice\", parse_json(parse_json(message).message).content, \"\")) | summarize operation_ParentId = any(operation_ParentId), Input = maxif(user_input, user_input != \"\"), System = maxif(system, system != \"\"), Output = maxif(llm_response, llm_response != \"\") by operation_Id, id); gen_ai_spans | join kind=inner (gen_ai_events) on id, operation_Id | project Input, System, Output, operation_Id, operation_ParentId, gen_ai_response_id = response_id', 'connectionString': '', 'serviceName': '', 'id': ''}, 'tags': {}, 'properties': {'AzureMSIClientId': 'your-azure-msi-client-id'}, 'evaluators': {'f1_score': {'id': 'azureml://registries/azureml/models/F1Score-Evaluator/versions/3', 'initParams': {}, 'dataMapping': {}}, 'relevance': {'id': 'azureml://registries/azureml/models/Relevance-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2025-01-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'groundedness': {'id': 'azureml://registries/azureml/models/Groundedness-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2025-01-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'coherence': {'id': 'azureml://registries/azureml/models/Coherence-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2025-01-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'response': '${data.response}'}}, 'fluency': {'id': 'azureml://registries/azureml/models/Fluency-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2025-01-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'similarity': {'id': 'azureml://registries/azureml/models/Similarity-Evaluator/versions/3', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2025-01-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'response': '${data.response}'}}}, 'trigger': {'type': 'Recurrence', 'frequency': 'Hour', 'interval': 1}, 'systemData': {'createdBy': '', 'modifiedBy': ''}}\n",
      "{'name': 'evaluation_sdk_schedule', 'description': 'scheduled evaluation', 'isEnabled': True, 'provisioningState': 'Succeeded', 'data': {'type': 'AppInsights', 'resourceId': '/subscriptions/3d4d3dd0-79d4-40cf-a94e-b4154812c6ca/resourceGroups/AOAI-group3/providers/microsoft.insights/components/application_insight01', 'query': 'let gen_ai_spans=(dependencies | where isnotnull(customDimensions[\"gen_ai.system\"]) | extend response_id = tostring(customDimensions[\"gen_ai.response.id\"]) | project id, operation_Id, operation_ParentId, timestamp, response_id); let gen_ai_events=(traces | where message in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") or tostring(customDimensions[\"event.name\"]) in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") | project id= operation_ParentId, operation_Id, operation_ParentId, user_input = iff(message == \"gen_ai.user.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.user.message\", parse_json(iff(message == \"gen_ai.user.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), system = iff(message == \"gen_ai.system.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.system.message\", parse_json(iff(message == \"gen_ai.system.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), llm_response = iff(message == \"gen_ai.choice\", parse_json(tostring(parse_json(tostring(customDimensions[\"gen_ai.event.content\"])).message)).content, iff(tostring(customDimensions[\"event.name\"]) == \"gen_ai.choice\", parse_json(parse_json(message).message).content, \"\")) | summarize operation_ParentId = any(operation_ParentId), Input = maxif(user_input, user_input != \"\"), System = maxif(system, system != \"\"), Output = maxif(llm_response, llm_response != \"\") by operation_Id, id); gen_ai_spans | join kind=inner (gen_ai_events) on id, operation_Id | project Input, System, Output, operation_Id, operation_ParentId, gen_ai_response_id = response_id', 'connectionString': '', 'serviceName': '', 'id': ''}, 'tags': {}, 'properties': {'AzureMSIClientId': 'your-azure-msi-client-id'}, 'evaluators': {'f1_score': {'id': 'azureml://registries/azureml/models/F1Score-Evaluator/versions/3', 'initParams': {}, 'dataMapping': {}}, 'relevance': {'id': 'azureml://registries/azureml/models/Relevance-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2025-01-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'groundedness': {'id': 'azureml://registries/azureml/models/Groundedness-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2025-01-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'coherence': {'id': 'azureml://registries/azureml/models/Coherence-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2025-01-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'response': '${data.response}'}}, 'fluency': {'id': 'azureml://registries/azureml/models/Fluency-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2025-01-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'similarity': {'id': 'azureml://registries/azureml/models/Similarity-Evaluator/versions/3', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.openai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o-mini', 'api_version': '2025-01-01-preview', 'type': 'azure_openai'}}, 'dataMapping': {'query': '${data.query}', 'response': '${data.response}'}}}, 'trigger': {'type': 'Recurrence', 'frequency': 'Hour', 'interval': 1}, 'systemData': {'createdBy': '', 'modifiedBy': ''}}\n",
      "{'name': 'daily-evaluation', 'description': 'daily-evaluation evaluation schedule', 'isEnabled': False, 'provisioningState': 'Succeeded', 'data': {'type': 'AppInsights', 'resourceId': 'application-insights', 'query': 'let gen_ai_spans=(dependencies | where isnotnull(customDimensions[\"gen_ai.system\"]) | extend response_id = tostring(customDimensions[\"gen_ai.response.id\"]) | project id, operation_Id, operation_ParentId, timestamp, response_id); let gen_ai_events=(traces | where message in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") or tostring(customDimensions[\"event.name\"]) in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") | project id= operation_ParentId, operation_Id, operation_ParentId, user_input = iff(message == \"gen_ai.user.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.user.message\", parse_json(iff(message == \"gen_ai.user.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), system = iff(message == \"gen_ai.system.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.system.message\", parse_json(iff(message == \"gen_ai.system.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), llm_response = iff(message == \"gen_ai.choice\", parse_json(tostring(parse_json(tostring(customDimensions[\"gen_ai.event.content\"])).message)).content, iff(tostring(customDimensions[\"event.name\"]) == \"gen_ai.choice\", parse_json(parse_json(message).message).content, \"\")) | summarize operation_ParentId = any(operation_ParentId), Input = maxif(user_input, user_input != \"\"), System = maxif(system, system != \"\"), Output = maxif(llm_response, llm_response != \"\") by operation_Id, id); gen_ai_spans | join kind=inner (gen_ai_events) on id, operation_Id | project Input, System, Output, operation_Id, operation_ParentId, gen_ai_response_id = response_id', 'connectionString': '', 'serviceName': 'daily-evaluation', 'id': ''}, 'tags': {}, 'properties': {'AzureMSIClientId': 'your_client_id'}, 'evaluators': {'f1_score': {'id': 'azureml://registries/azureml/models/F1Score-Evaluator/versions/3', 'initParams': {}, 'dataMapping': {}}, 'relevance': {'id': 'azureml://registries/azureml/models/Relevance-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.services.ai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o', 'api_version': '2024-12-01-preview'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'groundedness': {'id': 'azureml://registries/azureml/models/Groundedness-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.services.ai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o', 'api_version': '2024-12-01-preview'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'coherence': {'id': 'azureml://registries/azureml/models/Coherence-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.services.ai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o', 'api_version': '2024-12-01-preview'}}, 'dataMapping': {'query': '${data.query}', 'response': '${data.response}'}}, 'fluency': {'id': 'azureml://registries/azureml/models/Fluency-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.services.ai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o', 'api_version': '2024-12-01-preview'}}, 'dataMapping': {'query': '${data.query}', 'context': '${data.context}', 'response': '${data.response}'}}, 'similarity': {'id': 'azureml://registries/azureml/models/Similarity-Evaluator/versions/3', 'initParams': {'model_config': {'azure_endpoint': 'https://aoai-services1.services.ai.azure.com/', 'api_key': '32572ab337444af999eb7242997d56c9', 'azure_deployment': 'gpt-4o', 'api_version': '2024-12-01-preview'}}, 'dataMapping': {'query': '${data.query}', 'response': '${data.response}'}}}, 'trigger': {'type': 'Recurrence', 'frequency': 'Day', 'interval': 1}, 'systemData': {'createdBy': '', 'modifiedBy': ''}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "evaluation_schedule = azure_ai_project_client.evaluations.get_schedule(service_name)\n",
    "print(evaluation_schedule)\n",
    "\n",
    "# Sample for list evaluation schedules\n",
    "for evaluation_schedule in azure_ai_project_client.evaluations.list_schedule():\n",
    "    print(evaluation_schedule)\n",
    "\n",
    "# Sample for disable an evaluation schedule with name\n",
    "# project_client.evaluations.disable_schedule(service_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.evaluation_sdk_schedule [IsEnabled: True]\n",
      "Total evaluation schedules: 1\n",
      "2.daily-evaluation [IsEnabled: False]\n",
      "Total evaluation schedules: 2\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for evaluation_schedule in azure_ai_project_client.evaluations.list_schedule():\n",
    "    count += 1\n",
    "    print(f\"{count}.{evaluation_schedule.name} \"\n",
    "    f\"[IsEnabled: {evaluation_schedule.is_enabled}]\")\n",
    "    print(f\"Total evaluation schedules: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable (soft-delete) online evaluation schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_service_name = service_name\n",
    "# azure_ai_project_client.evaluations.disable_schedule(target_service_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
