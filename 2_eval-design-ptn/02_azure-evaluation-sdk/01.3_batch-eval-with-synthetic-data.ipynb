{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Evaluators with the Azure AI Evaluation SDK\n",
    "The following sample shows the basic way to evaluate a Generative AI application in your development environment with the Azure AI evaluation SDK.\n",
    "\n",
    "> ‚ú® ***Note*** <br>\n",
    "> Please check the reference document before you get started - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## üî® Current Support and Limitations (as of 2025-01-14) \n",
    "- Check the region support for the Azure AI Evaluation SDK. https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#region-support\n",
    "\n",
    "### Region support for evaluations\n",
    "| Region              | Hate and Unfairness, Sexual, Violent, Self-Harm, XPIA, ECI (Text) | Groundedness (Text) | Protected Material (Text) | Hate and Unfairness, Sexual, Violent, Self-Harm, Protected Material (Image) |\n",
    "|---------------------|------------------------------------------------------------------|---------------------|----------------------------|----------------------------------------------------------------------------|\n",
    "| North Central US    | no                                                               | no                  | no                         | yes                                                                        |\n",
    "| East US 2           | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| Sweden Central      | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| US North Central    | yes                                                              | no                  | yes                        | yes                                                                        |\n",
    "| France Central      | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| Switzerland West    | yes                                                              | no                  | no                         | yes                                                                        |\n",
    "\n",
    "### Region support for adversarial simulation\n",
    "| Region            | Adversarial Simulation (Text) | Adversarial Simulation (Image) |\n",
    "|-------------------|-------------------------------|---------------------------------|\n",
    "| UK South          | yes                           | no                              |\n",
    "| East US 2         | yes                           | yes                             |\n",
    "| Sweden Central    | yes                           | yes                             |\n",
    "| US North Central  | yes                           | yes                             |\n",
    "| France Central    | yes                           | no                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úîÔ∏è Pricing and billing\n",
    "- Effective 1/14/2025, Azure AI Safety Evaluations will no longer be free in public preview. It will be billed based on consumption as following:\n",
    "\n",
    "| Service Name              | Safety Evaluations       | Price Per 1K Tokens (USD) |\n",
    "|---------------------------|--------------------------|---------------------------|\n",
    "| Azure Machine Learning    | Input pricing for 3P     | $0.02                     |\n",
    "| Azure Machine Learning    | Output pricing for 3P    | $0.06                     |\n",
    "| Azure Machine Learning    | Input pricing for 1P     | $0.012                    |\n",
    "| Azure Machine Learning    | Output pricing for 1P    | $0.012                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    Dataset,\n",
    "    EvaluatorConfiguration,\n",
    "    ConnectionType,\n",
    "    EvaluationSchedule,\n",
    "    RecurrenceTrigger,\n",
    "    ApplicationInsightsConfiguration\n",
    ")\n",
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    "    F1ScoreEvaluator,\n",
    "    RetrievalEvaluator\n",
    ")\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "azure_ai_project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    #conn_str=os.environ.get(\"AZURE_AI_PROJECT_CONN_STR'),  # At the moment, it should be in the format '<Region>.api.azureml.ms;<AzureSubscriptionId>;<ResourceGroup>;<HubName>' Ex: eastus2.api.azureml.ms;xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxxxx;rg-sample;sample-project-eastus2\n",
    "    conn_str=\"swedencentral.api.azureml.ms;3d4d3dd0-79d4-40cf-a94e-b4154812c6ca;AOAI-group3;aoai-pjt1\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"azure_openai\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run Evaluators in Azure Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic dataset with Azure OpenAI\n",
    "- Use your models to generate custom textual utterances for your purpose in your target language. These utterances serve as a seed for the evaluation creation. By adjusting your prompts, you can produce text tailored to your domain (such as call center Q&A for a tech brand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initialized AzuureOpenAI client ===\n",
      "AZURE_OPENAI_ENDPOINT=https://aoai-services1.openai.azure.com/\n",
      "AZURE_OPENAI_API_VERSION=2025-01-01-preview\n",
      "AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint = aoai_api_endpoint,\n",
    "        api_key        = aoai_api_key,\n",
    "        api_version    = aoai_api_version,\n",
    "    )\n",
    "\n",
    "    print(\"=== Initialized AzuureOpenAI client ===\")\n",
    "    print(f\"AZURE_OPENAI_ENDPOINT={aoai_api_endpoint}\")\n",
    "    print(f\"AZURE_OPENAI_API_VERSION={aoai_api_version}\")\n",
    "    print(f\"AZURE_OPENAI_DEPLOYMENT_NAME={aoai_deployment_name}\")\n",
    "        \n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"no\":\"1\",\"query\":\"What are the latest products from Contoso Electronics?\",\"context\":\"Product catalog inquiry\",\"response\":\"The latest products from Contoso Electronics include the Contoso Smart Speaker, Contoso Ultra HD TV, and the Contoso Fitness Tracker. Each product features cutting-edge technology and user-friendly interfaces.\",\"ground_truth\":\"The latest products from Contoso Electronics include the Contoso Smart Speaker, Contoso Ultra HD TV, and the Contoso Fitness Tracker.\"}\n",
      "{\"no\":\"2\",\"query\":\"How can I reset my Contoso device?\",\"context\":\"Device troubleshooting\",\"response\":\"To reset your Contoso device, locate the reset button on the back or bottom of the device. Press and hold the button for about ten seconds until the device powers off and restarts.\",\"ground_truth\":\"To reset your Contoso device, locate the reset button on the back or bottom of the device.\"}\n",
      "{\"no\":\"3\",\"query\":\"What warranty does Contoso offer?\",\"context\":\"Warranty information\",\"response\":\"Contoso offers a one-year limited warranty on all electronic products, covering defects in materials and workmanship under normal use.\",\"ground_truth\":\"Contoso offers a one-year limited warranty on all electronic products.\"}\n",
      "{\"no\":\"4\",\"query\":\"How do I contact Contoso customer support?\",\"context\":\"Customer support inquiry\",\"response\":\"You can contact Contoso customer support by calling 1-800-CONTOSO or by visiting our website and using the live chat feature.\",\"ground_truth\":\"You can contact Contoso customer support by calling 1-800-CONTOSO.\"}\n",
      "{\"no\":\"5\",\"query\":\"Where can I find the user manual for my Contoso product?\",\"context\":\"User manual location\",\"response\":\"User manuals for Contoso products can be found on our official website under the 'Support' section. Simply enter your product model to download the manual.\",\"ground_truth\":\"User manuals for Contoso products can be found on our official website.\"}\n",
      "{\"no\":\"6\",\"query\":\"What is the return policy for Contoso Electronics?\",\"context\":\"Return policy inquiry\",\"response\":\"Contoso Electronics has a 30-day return policy for unopened products. If the product is opened, it must be returned within 14 days for a refund or exchange.\",\"ground_truth\":\"Contoso Electronics has a 30-day return policy for unopened products.\"}\n",
      "{\"no\":\"7\",\"query\":\"Can I track my Contoso order?\",\"context\":\"Order tracking inquiry\",\"response\":\"Yes, you can track your Contoso order by logging into your account on our website and navigating to the 'Order History' section.\",\"ground_truth\":\"You can track your Contoso order by logging into your account on our website.\"}\n",
      "{\"no\":\"8\",\"query\":\"What accessories are compatible with my Contoso device?\",\"context\":\"Accessory compatibility\",\"response\":\"To find compatible accessories for your Contoso device, visit our website and check the 'Accessories' section for your specific product model.\",\"ground_truth\":\"To find compatible accessories for your Contoso device, visit our website.\"}\n",
      "{\"no\":\"9\",\"query\":\"How do I update the software on my Contoso device?\",\"context\":\"Software update instructions\",\"response\":\"To update the software on your Contoso device, go to the settings menu, select 'Software Update,' and follow the prompts to download and install the latest version.\",\"ground_truth\":\"To update the software on your Contoso device, go to the settings menu.\"}\n",
      "{\"no\":\"10\",\"query\":\"What should I do if my Contoso device is not charging?\",\"context\":\"Charging issue troubleshooting\",\"response\":\"If your Contoso device is not charging, first check the power source and cable. If those are fine, try using a different outlet or cable. If the issue persists, contact customer support.\",\"ground_truth\":\"If your Contoso device is not charging, first check the power source and cable.\"}\n",
      "{\"no\":\"11\",\"query\":\"Are there any discounts available for Contoso products?\",\"context\":\"Discount inquiry\",\"response\":\"Contoso frequently offers discounts and promotions. Check our website or subscribe to our newsletter for the latest deals.\",\"ground_truth\":\"Contoso frequently offers discounts and promotions.\"}\n",
      "{\"no\":\"12\",\"query\":\"How do I set up my new Contoso Smart Speaker?\",\"context\":\"Setup instructions\",\"response\":\"To set up your new Contoso Smart Speaker, plug it in, download the Contoso app, and follow the on-screen instructions to connect it to your Wi-Fi network.\",\"ground_truth\":\"To set up your new Contoso Smart Speaker, plug it in and download the Contoso app.\"}\n",
      "{\"no\":\"13\",\"query\":\"What features does the Contoso Ultra HD TV have?\",\"context\":\"Product features inquiry\",\"response\":\"The Contoso Ultra HD TV features 4K resolution, smart connectivity, voice control, and a variety of streaming apps pre-installed.\",\"ground_truth\":\"The Contoso Ultra HD TV features 4K resolution and smart connectivity.\"}\n",
      "{\"no\":\"14\",\"query\":\"How do I pair my Contoso Fitness Tracker with my phone?\",\"context\":\"Device pairing instructions\",\"response\":\"To pair your Contoso Fitness Tracker with your phone, enable Bluetooth on your phone, open the Contoso app, and follow the prompts to connect your device.\",\"ground_truth\":\"To pair your Contoso Fitness Tracker with your phone, enable Bluetooth on your phone.\"}\n",
      "{\"no\":\"15\",\"query\":\"What should I do if I forgot my Contoso account password?\",\"context\":\"Password recovery\",\"response\":\"If you forgot your Contoso account password, click on the 'Forgot Password?' link on the login page and follow the instructions to reset it.\",\"ground_truth\":\"If you forgot your Contoso account password, click on the 'Forgot Password?' link.\"}\n",
      "{\"no\":\"16\",\"query\":\"Can I use my Contoso device internationally?\",\"context\":\"International usage inquiry\",\"response\":\"Most Contoso devices are designed for international use, but please check the product specifications for voltage and compatibility information.\",\"ground_truth\":\"Most Contoso devices are designed for international use.\"}\n",
      "{\"no\":\"17\",\"query\":\"What is the battery life of the Contoso Fitness Tracker?\",\"context\":\"Battery life inquiry\",\"response\":\"The Contoso Fitness Tracker has a battery life of up to seven days, depending on usage and settings.\",\"ground_truth\":\"The Contoso Fitness Tracker has a battery life of up to seven days.\"}\n",
      "{\"no\":\"18\",\"query\":\"How do I clean my Contoso electronics?\",\"context\":\"Cleaning instructions\",\"response\":\"To clean your Contoso electronics, use a soft, lint-free cloth and avoid using harsh chemicals. For screens, use a screen-safe cleaner.\",\"ground_truth\":\"To clean your Contoso electronics, use a soft, lint-free cloth.\"}\n",
      "{\"no\":\"19\",\"query\":\"What is the best way to store my Contoso devices?\",\"context\":\"Storage recommendations\",\"response\":\"The best way to store your Contoso devices is in a cool, dry place, away from direct sunlight and extreme temperatures.\",\"ground_truth\":\"The best way to store your Contoso devices is in a cool, dry place.\"}\n",
      "{\"no\":\"20\",\"query\":\"How do I register my Contoso product?\",\"context\":\"Product registration process\",\"response\":\"To register your Contoso product, visit our website and fill out the registration form with your product details and purchase information.\",\"ground_truth\":\"To register your Contoso product, visit our website and fill out the registration form.\"}\n",
      "Usage Information:\n",
      "Completion Tokens: 1506\n",
      "Prompt Tokens: 898\n",
      "Total Tokens: 2404\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 20\n",
    "\n",
    "LANGUAGE = \"English\"\n",
    "\n",
    "topic = f\"\"\"\n",
    "Contoso Electronics call center QnA related expected spoken utterances for {LANGUAGE} languages. create {NUM_SAMPLES} lines of jsonl of the topic related text.\n",
    "\"\"\"\n",
    "\n",
    "example = \"\"\"\n",
    "Here is examples of the expected format:\n",
    "{\"query\":\"How do you create a run?\",\"context\":\"AML API only\",\"response\":\"To create a run using the Azure Machine Learning API, you first need to create an Experiment. Once you have an experiment, you can create a Run object that is associated with that experiment. Here is some Python code that demonstrates this process:\\\\n\\\\n```\\\\nfrom azureml.core import Experiment, Run\\\\nfrom azureml.core.workspace import Workspace\\\\n\\\\n# Define workspace and experiment\\\\nws = Workspace.from_config()\\\\nexp = Experiment(workspace=ws, name='my_experiment')\\\\n\\\\n# Create a new run\\\\nrun = exp.start_logging()\\\\n```\\\\n\\\\nIn this code, the `from_config()` method reads the configuration file that you created when you set up your Azure Machine Learning workspace. The `Experiment` constructor creates an Experiment object that is associated with your workspace, and the `start_logging()` method creates a new Run object that is associated with the Experiment. Now you can use the `run` object to log metrics, upload files, and track other information related to your machine learning experiment.\",\"ground_truth\":\"Paris is the capital of France.\"}\n",
    "{\"query\":\"How do you log a model?\",\"context\":\"Logging can be done using any OSS Sdk\",\"response\":\"There are a few ways to log models in Azure Machine Learning. \\\\n\\\\nOne way is to use the `register_model()` method of the `Run` object. The `register_model()` method logs a model file in the Azure Machine Learning service workspace and makes it available for deployment. Here's an example:\\\\n\\\\n```python\\\\nfrom azureml.core import Model\\\\n\\\\nmodel_path = './outputs/my_model.pkl'\\\\nmodel = Model.register(workspace=ws, model_path=model_path, model_name='my_model')\\\\n```\\\\n\\\\nThis code registers the model file located at `model_path` to the Azure Machine Learning service workspace with the name `my_model`. \\\\n\\\\nAnother way to log a model is to save it as an output of a `Run`. If your model generation code is part of a script or Jupyter notebook that runs as an Azure Machine Learning experiment, you can save the model file as an output of the `Run` object. Here's an example:\\\\n\\\\n```python\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom azureml.core.run import Run\\\\n\\\\n# Initialize a run object\\\\nrun = Run.get_context()\\\\n\\\\n# Train your model\\\\nX_train, y_train = ...\\\\nlog_reg = LogisticRegression().fit(X_train, y_train)\\\\n\\\\n# Save the model to the Run object's outputs directory\\\\nmodel_path = 'outputs/model.pkl'\\\\njoblib.dump(value=log_reg, filename=model_path)\\\\n\\\\n# Log the model as a run artifact\\\\nrun.upload_file(name=model_path, path_or_stream=model_path)\\\\n```\\\\n\\\\nIn this code, `Run.get_context()` retrieves the current run context object, which you can use to track metadata and metrics for the run. After training your model, you can use `joblib.dump()` to save the model to a file, and then log the file as an artifact of the run using `run.upload_file()`.\",\"ground_truth\":\"Paris is the capital of France.\"}\n",
    "{\"query\":\"What is the capital of France?\",\"context\":\"France is in Europe\",\"response\":\"Paris is the capital of France.\",\"ground_truth\":\"Paris is the capital of France.\"}\n",
    "\"\"\"\n",
    "\n",
    "system_message = \"\"\"\n",
    "Generate plain text sentences of #topic# related text to improve the recognition of domain-specific words and phrases.\n",
    "Domain-specific words can be uncommon or made-up words, but their pronunciation must be straightforward to be recognized. \n",
    "Use text data that's close to the expected spoken utterances. The nummber of utterances per line should be 1. \n",
    "jsonl format is required. use 'no' as number, 'query' as string, 'context' as string, 'response' as string, and 'ground_truth' as string.\n",
    "only include the lines as the result. Do not include ```jsonl, ``` and blank line in the result. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "#topic#: {topic}\n",
    "Example: {example}\n",
    "\"\"\"\n",
    "\n",
    "# Simple API Call\n",
    "response = client.chat.completions.create(\n",
    "    model=aoai_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    top_p=0.1\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "print(content)\n",
    "print(\"Usage Information:\")\n",
    "#print(f\"Cached Tokens: {response.usage.prompt_tokens_details.cached_tokens}\") #only o1 models support this\n",
    "print(f\"Completion Tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Prompt Tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Total Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'synthetic_text_file' (str)\n"
     ]
    }
   ],
   "source": [
    "synthetic_text_file = \"../data/sythetic_evaluation_data.jsonl\"\n",
    "with open(synthetic_text_file, 'w', encoding='utf-8') as f:\n",
    "    for line in content.split('\\n'):\n",
    "        if line.strip():  # Check if the line is not empty\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "%store synthetic_text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading sythetic_evaluation_data.jsonl\u001b[32m (< 1 MB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.17k/7.17k [00:00<00:00, 693kB/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Upload data for evaluation\n",
    "data_id, _ = azure_ai_project_client.upload_file(\"../data/sythetic_evaluation_data.jsonl\")\n",
    "# data_id = \"azureml://registries/<registry>/data/<dataset>/versions/<version>\"\n",
    "# To use an existing dataset, replace the above line with the following line\n",
    "# data_id = \"<dataset_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Evaluators to Run\n",
    "- The code below demonstrates how to configure the evaluators you want to run. In this example, we use the F1ScoreEvaluator, RelevanceEvaluator and the ViolenceEvaluator, but all evaluators supported by Azure AI Evaluation are supported by cloud evaluation and can be configured here. You can either import the classes from the SDK and reference them with the .id property, or you can find the fully formed id of the evaluator in the AI Studio registry of evaluators, and use it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id for each evaluator can be found in your AI Studio registry - please see documentation for more information\n",
    "# init_params is the configuration for the model to use to perform the evaluation\n",
    "# data_mapping is used to map the output columns of your query to the names required by the evaluator\n",
    "# Evaluator parameter format - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#evaluator-parameter-format\n",
    "evaluators_cloud = {\n",
    "    \"f1_score\": EvaluatorConfiguration(\n",
    "        id=F1ScoreEvaluator.id,\n",
    "    ),\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=RelevanceEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    \"groundedness\": EvaluatorConfiguration(\n",
    "        id=GroundednessEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    # \"retrieval\": EvaluatorConfiguration(\n",
    "    #     #from azure.ai.evaluation._evaluators._common.math import list_mean_nan_safe\\nModuleNotFoundError: No module named 'azure.ai.evaluation._evaluators._common.math'\n",
    "    #     #id=RetrievalEvaluator.id,\n",
    "    #     id=\"azureml://registries/azureml/models/Retrieval-Evaluator/versions/2\",\n",
    "    #     init_params={\"model_config\": model_config},\n",
    "    #     data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    # ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=CoherenceEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=FluencyEvaluator.id,\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"context\": \"${data.context}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "     \"similarity\": EvaluatorConfiguration(\n",
    "        # currently bug in the SDK, please use the id below\n",
    "        #id=SimilarityEvaluator.id,\n",
    "        id=\"azureml://registries/azureml/models/Similarity-Evaluator/versions/3\",\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\"query\": \"${data.query}\", \"response\": \"${data.response}\"},\n",
    "    ),\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud Evaluation\",\n",
    "    description=\"Cloud Evaluation of dataset\",\n",
    "    data=Dataset(id=data_id),\n",
    "    evaluators=evaluators_cloud,\n",
    ")\n",
    "\n",
    "# Create evaluation\n",
    "evaluation_response = azure_ai_project_client.evaluations.create(\n",
    "    evaluation=evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Monitor the status of the run_result\n",
    "def monitor_status(project_client:AIProjectClient, evaluation_response_id:str):\n",
    "    with tqdm(total=3, desc=\"Running Status\", unit=\"step\") as pbar:\n",
    "        status = project_client.evaluations.get(evaluation_response_id).status\n",
    "        if status == \"Queued\":\n",
    "            pbar.update(1)\n",
    "        while status != \"Completed\" and status != \"Failed\":\n",
    "            if status == \"Running\" and pbar.n < 2:\n",
    "                pbar.update(1)\n",
    "            print(f\"Current Status: {status}\")\n",
    "            time.sleep(10)\n",
    "            status = project_client.evaluations.get(evaluation_response_id).status\n",
    "        while(pbar.n < 3):\n",
    "            pbar.update(1)\n",
    "        print(\"Operation Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Status:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 0/3 [00:00<?, ?step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Status: Starting\n",
      "Current Status: Queued\n"
     ]
    }
   ],
   "source": [
    "monitor_status(azure_ai_project_client, evaluation_response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the evaluation result in Azure AI Foundry \n",
    "- After running the evaluation, you can check the evaluation results in Azure AI Foundry. You can find the evaluation results in the Evaluation tab of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Created evaluation, evaluation ID:  04ee9cf1-317a-4e8c-b9c2-2d61dcc4755e\n",
      "Evaluation status:  Completed\n",
      "AI Foundry Portal URI:  https://ai.azure.com/build/evaluation/04ee9cf1-317a-4e8c-b9c2-2d61dcc4755e?wsid=/subscriptions/3d4d3dd0-79d4-40cf-a94e-b4154812c6ca/resourceGroups/AOAI-group3/providers/Microsoft.MachineLearningServices/workspaces/aoai-pjt1\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation\n",
    "get_evaluation_response = azure_ai_project_client.evaluations.get(evaluation_response.id)\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Created evaluation, evaluation ID: \", get_evaluation_response.id)\n",
    "print(\"Evaluation status: \", get_evaluation_response.status)\n",
    "print(\"AI Foundry Portal URI: \", get_evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cloud Evaluation Result](../images/cloud_evaluation_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
