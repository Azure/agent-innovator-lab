{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26ab996",
   "metadata": {},
   "source": [
    "# Adaptive RAG\n",
    "\n",
    "----\n",
    "\n",
    "Adaptive RAG predicts the **complexity of the input question** using a SLM/LLM and selects an appropriate processing workflow accordingly.\n",
    "\n",
    "- **Very simple question (No Retrieval)**: Generates answers without RAG.\n",
    "- **Simple question (Single-shot RAG)**: Efficiently generates answers through a single-step search and generation.\n",
    "- **Complex question (Iterative RAG)**: Provides accurate answers to complex questions through repeated multi-step search and generation.\n",
    "\n",
    "\n",
    "Adaptive-RAG, Self-RAG, and Corrective RAG are similar approach, but they have different focuses.\n",
    "\n",
    "- **Adaptive-RAG**: Dynamically selects appropriate retrieval and generation strategies based on the complexity of the question.\n",
    "- **Self-RAG**: The model determines the need for retrieval on its own, performs retrieval when necessary, and improves the quality through self-reflection on the generated answers.\n",
    "- **Corrective RAG**: Evaluates the quality of retrieved documents, and performs additional retrievals such as web searches to supplement the information if the reliability is low.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "- [Adaptive-RAG paper](https://arxiv.org/abs/2403.14403)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6458235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from collections.abc import Awaitable, Callable\n",
    "from enum import Enum\n",
    "from typing import Annotated, Any, ClassVar\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.agents import (\n",
    "    AssistantAgentThread,\n",
    "    AzureAIAgent,\n",
    "    AzureAIAgentSettings,\n",
    "    AzureAIAgentThread,\n",
    "    AzureResponsesAgent,\n",
    "    ChatCompletionAgent,\n",
    "    ChatHistoryAgentThread,\n",
    "    ResponsesAgentThread,\n",
    ")\n",
    "from semantic_kernel.agents.group_chat.agent_group_chat import AgentGroupChat\n",
    "from semantic_kernel.agents.strategies.termination.termination_strategy import TerminationStrategy\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureTextEmbedding,\n",
    "    OpenAIEmbeddingPromptExecutionSettings,\n",
    ")\n",
    "from semantic_kernel.connectors.memory.azure_ai_search import (\n",
    "    AzureAISearchCollection,\n",
    "    AzureAISearchStore,\n",
    ")\n",
    "from semantic_kernel.connectors.search.bing import BingSearch\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "from semantic_kernel.data import (\n",
    "    VectorSearchOptions,\n",
    "    VectorStoreRecordDataField,\n",
    "    VectorStoreRecordKeyField,\n",
    "    VectorStoreRecordVectorField,\n",
    "    vectorstoremodel,\n",
    ")\n",
    "from semantic_kernel.filters import FilterTypes, FunctionInvocationContext\n",
    "from semantic_kernel.functions import (\n",
    "    KernelArguments,\n",
    "    KernelParameterMetadata,\n",
    "    KernelPlugin,\n",
    "    kernel_function,\n",
    ")\n",
    "from semantic_kernel.kernel_pydantic import KernelBaseSettings, KernelBaseModel\n",
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "from semantic_kernel.processes.kernel_process.kernel_process_step import KernelProcessStep\n",
    "from semantic_kernel.processes.kernel_process.kernel_process_step_context import KernelProcessStepContext\n",
    "from semantic_kernel.processes.kernel_process.kernel_process_step_state import KernelProcessStepState\n",
    "from semantic_kernel.processes.local_runtime.local_event import KernelProcessEvent\n",
    "from semantic_kernel.processes.local_runtime.local_kernel_process import start\n",
    "from semantic_kernel.processes.process_builder import ProcessBuilder\n",
    "from semantic_kernel.processes.process_function_target_builder import ProcessFunctionTargetBuilder\n",
    "from semantic_kernel.contents.streaming_chat_message_content import StreamingChatMessageContent\n",
    "\n",
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    RetrievalEvaluator,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d7dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the environment variables\n",
    "azure_ai_search_endpoint = os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
    "azure_search_admin_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\", \"\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"hotels-sample-index\")\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = (\n",
    "    os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "    if len(os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")) > 0\n",
    "    else None\n",
    ")\n",
    "azure_openai_chat_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_embedding_deployment_name = os.getenv(\n",
    "    \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\", \"text-embedding-ada-002\"\n",
    ")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\")\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": azure_openai_endpoint,\n",
    "    \"api_key\": azure_openai_key,\n",
    "    \"azure_deployment\": azure_openai_chat_deployment_name,\n",
    "    \"api_version\": azure_openai_api_version,\n",
    "    \"type\": \"azure_openai\",\n",
    "}\n",
    "\n",
    "embedding_model_config = {\n",
    "    \"azure_endpoint\": azure_openai_endpoint,\n",
    "    \"api_key\": azure_openai_key,\n",
    "    \"azure_deployment\": azure_openai_embedding_deployment_name,\n",
    "    \"api_version\": azure_openai_api_version,\n",
    "    \"type\": \"azure_openai\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec192a9",
   "metadata": {},
   "source": [
    "## 🧪 Step 1. Test and Construct each module\n",
    "---\n",
    "\n",
    "Before building the entire the graph pipeline, we will test and construct each module separately.\n",
    "\n",
    "- **IntentRouter**\n",
    "- **SearchClient(Retrieval)**\n",
    "- **Retrieval Grader**\n",
    "- **Question Re-writer**\n",
    "- **Answer Generator**\n",
    "- **Groundedness Evaluator**\n",
    "- **Relevance Evaluator**\n",
    "- **Keyword Re-writer**\n",
    "- **Web Search Tool**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d39ad",
   "metadata": {},
   "source": [
    "### Define your LLM\n",
    "\n",
    "This hands-on only uses the `gpt-4o-mini`, but you can utilize multiple models in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d3a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_id = \"adaptive-rag\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "        # api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        # endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        # deployment_name=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "        # api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        # credential=DefaultAzureCredential()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4c24d3",
   "metadata": {},
   "source": [
    "### Intent Router\n",
    "\n",
    "Construct a `intent_router` agent to analyze the intent from user's query to route the query to the appropriate module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4deb3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from enum import Enum\n",
    "\n",
    "class IntentType(str, Enum):\n",
    "    LLM = \"LLM\"\n",
    "    RAG = \"RAG\"\n",
    "    websearch = \"websearch\"    \n",
    "\n",
    "class IntentResponse(BaseModel):\n",
    "    intent_type: IntentType = Field(..., description=\"Processing status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "180e9f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"intent_type\":\"RAG\"}\n",
      "RAG\n"
     ]
    }
   ],
   "source": [
    "if kernel.services.get(\"corrective-rag\") is None:\n",
    "    service_id = \"corrective-rag\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n",
    "req_settings.max_tokens = 2000\n",
    "req_settings.temperature = 0.7\n",
    "req_settings.response_format = IntentResponse\n",
    "\n",
    "#query=\"how are you today?\"\n",
    "query=\"Can you recommend a few hotels with complimentary breakfast?\"\n",
    "#query=\"Can you recommend the newest Openings Hotels in Manhattan Midtown 2025?\"\n",
    "\n",
    "# This prompt provides instructions to the model\n",
    "INTENT_ROUTER_PROMPT=\"\"\"\n",
    "You are an expert at routing a user question to LLM or RAG or websearch.\n",
    "                    The LLM covers casual topic such as greeting, small talks.\n",
    "                    Use the LLM for questions on casual topics.\n",
    "                    The RAG contains documents related to hotel information in New York until Aug, 2024.\n",
    "                    Use the RAG for questions on the hotel related topics. For all else, websearch.\n",
    "                    response inent_type such as LLM, RAG, or websearch.\n",
    "Query: {{$query}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=INTENT_ROUTER_PROMPT,\n",
    "    name=\"intentRouter\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"query\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "intent_router = kernel.add_function(\n",
    "    function_name=\"intentRouterFunc\",\n",
    "    plugin_name=\"intentRouterPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "response = await kernel.invoke(intent_router, query=query)\n",
    "print(response)\n",
    "\n",
    "response_json = json.loads(response.value[0].items[0].text)\n",
    "print(response_json[\"intent_type\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf92a96",
   "metadata": {},
   "source": [
    "### Construct Retrieval Chain based on PDF\n",
    "- We use the hotels-sample-index, which can be created in minutes and runs on any search service tier. This index is created by a wizard using built-in sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4121f7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lion's Den Inn:Full breakfast buffet for 2 for only $1. Excited to show off our room upgrades, faster high speed WiFi, updated corridors & meeting space. Come relax and enjoy your stay.:['laundry service', 'free wifi', 'restaurant']\n",
      "Lakefront Captain Inn:Every stay starts with a warm cookie. Amenities like the Counting Sheep sleep experience, our Wake-up glorious breakfast buffet and spacious workout facilities await.:['restaurant', 'laundry service', 'coffee in lobby']\n",
      "Starlight Suites:Complimentary Airport Shuttle & WiFi. Book Now and save - Spacious All Suite Hotel, Indoor Outdoor Pool, Fitness Center, Florida Green certified, Complimentary Coffee, HDTV:['pool', 'coffee in lobby', 'free wifi']\n"
     ]
    }
   ],
   "source": [
    "azure_ai_search_endpoint = os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
    "azure_search_admin_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\", \"\")\n",
    "search_client = SearchClient(\n",
    "    endpoint=azure_ai_search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(azure_search_admin_key),\n",
    "    semantic_configuration_name=\"my-semantic-config\",\n",
    ")\n",
    "\n",
    "# Query is the question being asked. It's sent to the search engine and the LLM.\n",
    "query = \"Can you recommend a few hotels with complimentary breakfast?\"\n",
    "\n",
    "fields = \"descriptionVector\"  # TODO: Check if this is the correct field name\n",
    "# don't use exhaustive search for large indexes\n",
    "vector_query = VectorizableTextQuery(\n",
    "    text=query, k_nearest_neighbors=2, fields=fields, exhaustive=True\n",
    ")\n",
    "\n",
    "# Search results are created by the search client.\n",
    "# Search results are composed of the top 3 results and the fields selected from the search index.\n",
    "# Search results include the top 3 matches to your query.\n",
    "search_results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=\"Description,HotelName,Tags\",\n",
    "    top=3,\n",
    ")\n",
    "sources_formatted = \"\\n\".join(\n",
    "    [\n",
    "        f'{document[\"HotelName\"]}:{document[\"Description\"]}:{document[\"Tags\"]}'\n",
    "        for document in search_results\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(sources_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbab5f0",
   "metadata": {},
   "source": [
    "### Semantic Kernel Vector Store\n",
    "\n",
    "This hands-on only uses Vector Store as a memory connector and a simple function that uses the add_vector_to_records function to add vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb020a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# The data model used for this sample is based on the hotel data model from the Azure AI Search samples.\n",
    "# When deploying a new index in Azure AI Search using the import wizard you can choose to deploy the 'hotel-samples'\n",
    "# dataset, see here: https://learn.microsoft.com/en-us/azure/search/search-get-started-portal.\n",
    "# This is the dataset used in this sample with some modifications.\n",
    "# This model adds vectors for the 2 descriptions in English and French.\n",
    "# Both are based on the 1536 dimensions of the OpenAI models.\n",
    "# You can adjust this at creation time and then make the change below as well.\n",
    "# This sample assumes the index is deployed, the vector fields can be empty.\n",
    "# If the vector fields are empty, change the first_run parameter to True to add the vectors.\n",
    "###\n",
    "\n",
    "@vectorstoremodel\n",
    "class HotelSampleClass(BaseModel):\n",
    "    HotelId: Annotated[str, VectorStoreRecordKeyField]\n",
    "    HotelName: Annotated[str | None, VectorStoreRecordDataField()] = None\n",
    "    Description: Annotated[\n",
    "        str,\n",
    "        VectorStoreRecordDataField(\n",
    "            has_embedding=True, embedding_property_name=\"DescriptionVector\", is_full_text_searchable=True\n",
    "        ),\n",
    "    ]\n",
    "    DescriptionVector: Annotated[\n",
    "        list[float] | None,\n",
    "        VectorStoreRecordVectorField(\n",
    "            dimensions=1536,\n",
    "            local_embedding=True,\n",
    "            embedding_settings={\"embedding\": OpenAIEmbeddingPromptExecutionSettings(dimensions=1536)},\n",
    "        ),\n",
    "    ] = None\n",
    "    Description_kr: Annotated[\n",
    "        str, VectorStoreRecordDataField(has_embedding=True, embedding_property_name=\"descriptionKOVector\")\n",
    "    ]\n",
    "    descriptionKOVector: Annotated[\n",
    "        list[float] | None,\n",
    "        VectorStoreRecordVectorField(\n",
    "            dimensions=1536,\n",
    "            local_embedding=True,\n",
    "            embedding_settings={\"embedding\": OpenAIEmbeddingPromptExecutionSettings(dimensions=1536)},\n",
    "        ),\n",
    "    ] = None\n",
    "    Category: Annotated[str, VectorStoreRecordDataField()]\n",
    "    Tags: Annotated[list[str], VectorStoreRecordDataField()]\n",
    "    ParkingIncluded: Annotated[bool | None, VectorStoreRecordDataField()] = None\n",
    "    LastRenovationDate: Annotated[str | None, VectorStoreRecordDataField()] = None\n",
    "    Rating: Annotated[float, VectorStoreRecordDataField()]\n",
    "    Location: Annotated[dict[str, Any], VectorStoreRecordDataField()]\n",
    "    Address: Annotated[dict[str, str | None], VectorStoreRecordDataField()]\n",
    "    Rooms: Annotated[list[dict[str, Any]], VectorStoreRecordDataField()]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61da2a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results using text: \n",
      "    27 (in Aventura, USA): Complimentary Airport Shuttle & WiFi. Book Now and save - Spacious All Suite Hotel, Indoor Outdoor Pool, Fitness Center, Florida Green certified, Complimentary Coffee, HDTV (score: 4.735675)\n",
      "    16 (in Seattle, USA): 5 star Luxury Hotel - Biggest Rooms in the city. #1 Hotel in the area listed by Traveler magazine. Free WiFi, Flexible check in/out, Fitness Center & espresso in room. (score: 4.458799)\n",
      "    40 (in Scottsdale, USA): Only 8 miles from Downtown. On-site bar/restaurant, Free hot breakfast buffet, Free wireless internet, All non-smoking hotel. Only 15 miles from airport. (score: 3.9828727)\n",
      "\n",
      "\n",
      "Search results using vector: \n"
     ]
    },
    {
     "ename": "HttpResponseError",
     "evalue": "(InvalidRequestParameter) There's a mismatch in vector dimensions. The vector field 'descriptionVector', with dimension of '1536', expects a length of '1536'. However, the provided vector has a length of '3072'. Please ensure that the vector length matches the expected length of the vector field. Read the following documentation for more details: https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-configure-compression-storage.\r\nParameter name: vector.fields\nCode: InvalidRequestParameter\nMessage: There's a mismatch in vector dimensions. The vector field 'descriptionVector', with dimension of '1536', expects a length of '1536'. However, the provided vector has a length of '3072'. Please ensure that the vector length matches the expected length of the vector field. Read the following documentation for more details: https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-configure-compression-storage.\r\nParameter name: vector.fields\nException Details:\t(InvalidVectorQuery) There's a mismatch in vector dimensions. The vector field 'descriptionVector', with dimension of '1536', expects a length of '1536'. However, the provided vector has a length of '3072'. Please ensure that the vector length matches the expected length of the vector field. Read the following documentation for more details: https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-configure-compression-storage.\n\tCode: InvalidVectorQuery\n\tMessage: There's a mismatch in vector dimensions. The vector field 'descriptionVector', with dimension of '1536', expects a length of '1536'. However, the provided vector has a length of '3072'. Please ensure that the vector length matches the expected length of the vector field. Read the following documentation for more details: https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-configure-compression-storage.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Use vectorized search to search using the vector.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m collection\u001b[38;5;241m.\u001b[39mvectorized_search(\n\u001b[1;32m     36\u001b[0m     vector\u001b[38;5;241m=\u001b[39mquery_vector,\n\u001b[1;32m     37\u001b[0m     options\u001b[38;5;241m=\u001b[39mVectorSearchOptions(vector_field_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescriptionVector\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mresults:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mrecord\u001b[38;5;241m.\u001b[39mHotelId\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mrecord\u001b[38;5;241m.\u001b[39mAddress[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mrecord\u001b[38;5;241m.\u001b[39mAddress[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mrecord\u001b[38;5;241m.\u001b[39mDescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Delete the collection object so that the connection is closed.\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/semantic_kernel/data/vector_search.py:197\u001b[0m, in \u001b[0;36mVectorSearchBase._get_vector_search_results_from_results\u001b[0;34m(self, results, options)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, Sequence):\n\u001b[1;32m    196\u001b[0m     results \u001b[38;5;241m=\u001b[39m desync_list(results)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m         record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeserialize(\n\u001b[1;32m    200\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_record_from_result(result), include_vectors\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39minclude_vectors \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    201\u001b[0m         )\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/azure/search/documents/aio/_paging.py:28\u001b[0m, in \u001b[0;36mAsyncSearchItemPaged.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mby_page()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_page_iterator_instance \u001b[38;5;241m=\u001b[39m cast(AsyncSearchPageIterator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Let it raise StopAsyncIteration\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/azure/search/documents/aio/_paging.py:31\u001b[0m, in \u001b[0;36mAsyncSearchItemPaged.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Let it raise StopAsyncIteration\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/azure/core/async_paging.py:94\u001b[0m, in \u001b[0;36mAsyncPageIterator.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd of paging\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuation_token)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AzureError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m error\u001b[38;5;241m.\u001b[39mcontinuation_token:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/azure/search/documents/aio/_paging.py:109\u001b[0m, in \u001b[0;36mAsyncSearchPageIterator._get_next_cb\u001b[0;34m(self, continuation_token)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_next_cb\u001b[39m(\u001b[38;5;28mself\u001b[39m, continuation_token):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continuation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mdocuments\u001b[38;5;241m.\u001b[39msearch_post(search_request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_query\u001b[38;5;241m.\u001b[39mrequest, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[1;32m    111\u001b[0m     _next_link, next_page_request \u001b[38;5;241m=\u001b[39m unpack_continuation_token(continuation_token)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mdocuments\u001b[38;5;241m.\u001b[39msearch_post(search_request\u001b[38;5;241m=\u001b[39mnext_page_request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/azure/core/tracing/decorator_async.py:125\u001b[0m, in \u001b[0;36mdistributed_trace_async.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m func_tracing_attributes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    124\u001b[0m     span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/azure/search/documents/_generated/aio/operations/_documents_operations.py:397\u001b[0m, in \u001b[0;36mDocumentsOperations.search_post\u001b[0;34m(self, search_request, request_options, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[1;32m    396\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror)\n\u001b[1;32m    399\u001b[0m deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearchDocumentsResult\u001b[39m\u001b[38;5;124m\"\u001b[39m, pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: (InvalidRequestParameter) There's a mismatch in vector dimensions. The vector field 'descriptionVector', with dimension of '1536', expects a length of '1536'. However, the provided vector has a length of '3072'. Please ensure that the vector length matches the expected length of the vector field. Read the following documentation for more details: https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-configure-compression-storage.\r\nParameter name: vector.fields\nCode: InvalidRequestParameter\nMessage: There's a mismatch in vector dimensions. The vector field 'descriptionVector', with dimension of '1536', expects a length of '1536'. However, the provided vector has a length of '3072'. Please ensure that the vector length matches the expected length of the vector field. Read the following documentation for more details: https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-configure-compression-storage.\r\nParameter name: vector.fields\nException Details:\t(InvalidVectorQuery) There's a mismatch in vector dimensions. The vector field 'descriptionVector', with dimension of '1536', expects a length of '1536'. However, the provided vector has a length of '3072'. Please ensure that the vector length matches the expected length of the vector field. Read the following documentation for more details: https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-configure-compression-storage.\n\tCode: InvalidVectorQuery\n\tMessage: There's a mismatch in vector dimensions. The vector field 'descriptionVector', with dimension of '1536', expects a length of '1536'. However, the provided vector has a length of '3072'. Please ensure that the vector length matches the expected length of the vector field. Read the following documentation for more details: https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-configure-compression-storage."
     ]
    }
   ],
   "source": [
    "query = \"Can you recommend a few hotels with complimentary breakfast?\"\n",
    "\n",
    "COLLECTION_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"hotels-sample-index\")\n",
    "kernel = Kernel()\n",
    "# Add the OpenAI text embedding service\n",
    "\n",
    "embeddings = AzureTextEmbedding(\n",
    "    service_id=\"embedding\", \n",
    "    deployment_name=azure_openai_embedding_deployment_name,\n",
    "    endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key,\n",
    "    )\n",
    "kernel.add_service(embeddings)\n",
    "# Create the Azure AI Search collection\n",
    "collection = AzureAISearchCollection[str, HotelSampleClass](\n",
    "    collection_name=COLLECTION_NAME, data_model_type=HotelSampleClass\n",
    ")\n",
    "\n",
    "# Search using just text, by default this will search all the searchable text fields in the index.\n",
    "results = await collection.text_search(search_text=query)\n",
    "print(\"Search results using text: \")\n",
    "async for result in results.results:\n",
    "    print(\n",
    "        f\"    {result.record.HotelId} (in {result.record.Address['City']}, \"\n",
    "        f\"{result.record.Address['Country']}): {result.record.Description} (score: {result.score})\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Generate the vector for the query\n",
    "query_vector = (await embeddings.generate_raw_embeddings([query]))[0]\n",
    "\n",
    "print(\"Search results using vector: \")\n",
    "# Use vectorized search to search using the vector.\n",
    "results = await collection.vectorized_search(\n",
    "    vector=query_vector,\n",
    "    options=VectorSearchOptions(vector_field_name=\"descriptionVector\"),\n",
    ")\n",
    "async for result in results.results:\n",
    "    print(\n",
    "        f\"    {result.record.HotelId} (in {result.record.Address['City']}, \"\n",
    "        f\"{result.record.Address['Country']}): {result.record.Description} (score: {result.score})\"\n",
    "    )\n",
    "\n",
    "# Delete the collection object so that the connection is closed.\n",
    "del collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec10e5",
   "metadata": {},
   "source": [
    "### Question-Retrieval Grader\n",
    "\n",
    "Construct a retrieval grader that evaluates the relevance of the retrieved documents to the input question. The retrieval grader should take the input question and the retrieved documents as input and output a relevance score for each document.<br>\n",
    "Note that the retrieval grader should be able to handle **multiple documents** as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_eval = RetrievalEvaluator(model_config)\n",
    "\n",
    "query_response = dict(query=query, context=sources_formatted)\n",
    "\n",
    "retrieval_score = retrieval_eval(**query_response)\n",
    "print(retrieval_score)\n",
    "retrieval_score[\"retrieval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209a547",
   "metadata": {},
   "source": [
    "### Question Re-writer\n",
    "\n",
    "Construct a `question_rewriter` node to rewrite the question based on the retrieved documents and the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n",
    "req_settings.max_tokens = 2000\n",
    "\n",
    "query = \"Can you recommend a few factories with complimentary breakfast?\"\n",
    "\n",
    "# This prompt provides instructions to the model\n",
    "REWRITE_PROMPT = \"\"\"\n",
    "You a question re-writer that converts an input question to a better version that is optimized\n",
    "for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning based on hotel domain.\n",
    "Query: {{$query}}\n",
    "\"\"\"\n",
    "\n",
    "# Send the search results and the query to the LLM to generate a response based on the prompt.\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=REWRITE_PROMPT,\n",
    "    name=\"rewrite\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"query\", description=\"The user input\", is_required=True)\n",
    "    ],\n",
    "    execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "rewrite = kernel.add_function(\n",
    "    function_name=\"rewriteFunc\",\n",
    "    plugin_name=\"rewritePlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "\n",
    "response = await kernel.invoke(rewrite, query=query)\n",
    "# Here is the response from the chat model.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b7f87",
   "metadata": {},
   "source": [
    "### Answer Generator\n",
    "\n",
    "Construct a LLM Generation node. This is a Naive RAG chain that generates an answer based on the retrieved documents. \n",
    "\n",
    "We recommend you to use more advanced RAG chain for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class HotelInfo(BaseModel):\n",
    "    hotel_name: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "class RecommendationList(BaseModel):\n",
    "    recommendation: List[HotelInfo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n",
    "req_settings.max_tokens = 2000\n",
    "req_settings.temperature = 0.7\n",
    "req_settings.response_format = RecommendationList\n",
    "\n",
    "# This prompt provides instructions to the model\n",
    "GROUNDED_PROMPT = \"\"\"\n",
    "You are a friendly assistant that recommends hotels based on activities and amenities.\n",
    "Answer the query using only the context provided below in a friendly and concise bulleted manner.\n",
    "Answer ONLY with the facts listed in the list of context below.\n",
    "If there isn't enough information below, say you don't know.\n",
    "Generate a response that includes the top 3 results.\n",
    "Do not generate answers that don't use the context below.\n",
    "Query: {{$query}}\n",
    "Context:\\n{{$context}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=GROUNDED_PROMPT,\n",
    "    name=\"recommendation\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"query\", description=\"The user input\", is_required=True),\n",
    "        InputVariable(name=\"context\", description=\"Context to recommend hotels\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "recommendation = kernel.add_function(\n",
    "    function_name=\"recommendationFunc\",\n",
    "    plugin_name=\"recommendationPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "response = await kernel.invoke(recommendation, query=query, context=sources_formatted)\n",
    "\n",
    "# Load the response content as a JSON object\n",
    "response_json = json.loads(response.value[0].items[0].text)\n",
    "\n",
    "# Print the recommendations\n",
    "for hotel in response_json[\"recommendation\"]:\n",
    "    print(f\"Hotel Name: {hotel['hotel_name']}\")\n",
    "    print(f\"Description: {hotel['description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167e23e",
   "metadata": {},
   "source": [
    "### Groundedness Evaluator\n",
    "\n",
    "Construct a `groundedness_grader` node to evaluate the **hallucination** of the generated answer based on the retrieved documents.<br>\n",
    "\n",
    "`yes` means the answer is relevant to the retrieved documents, and `no` means the answer is not relevant to the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d55c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "\n",
    "query_response = dict(query=query, context=sources_formatted, response=response_json)\n",
    "\n",
    "groundedness_score = groundedness_eval(**query_response)\n",
    "print(groundedness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6c80a",
   "metadata": {},
   "source": [
    "### Relevance Evaluator\n",
    "\n",
    "Construct a `relevance_grader` node to evaluate the relevance of the generated answer to the question.<br>\n",
    "`yes` means the answer is relevant to the question, and `no` means the answer is not relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b80ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "\n",
    "query_response = dict(query=query, response=response_json)\n",
    "\n",
    "relevance_score = relevance_eval(**query_response)\n",
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05275c",
   "metadata": {},
   "source": [
    "### Keyword Re-writer\n",
    "\n",
    "Construct a `keyword_rewriter` agent to rewrite the question as the search keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Can you recommend a few hotels with complimentary breakfast?\"\n",
    "\n",
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n",
    "req_settings.max_tokens = 2000\n",
    "req_settings.temperature = 0.7\n",
    "\n",
    "# This prompt provides instructions to the model\n",
    "KEYWORD_REWRITE_PROMPT=\"\"\"\n",
    "You a keyword re-writer that converts an input question to a better version that is optimized for search. \n",
    "Generate search keyword from a user query \n",
    "to be more specific, detailed, and likely to retrieve relevant information, allowing for a more accurate response through web search.\n",
    "Don't include the additional context from the user question.\n",
    "\n",
    "Query: {{$query}}\n",
    "Revised web search query:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=KEYWORD_REWRITE_PROMPT,\n",
    "    name=\"keywordRewrite\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"query\", description=\"The user input\", is_required=True)\n",
    "    ],\n",
    "    execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "recommendation = kernel.add_function(\n",
    "    function_name=\"keywordRewriteFunc\",\n",
    "    plugin_name=\"keywordRewritePlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "response = await kernel.invoke(recommendation, query=query, context=sources_formatted)\n",
    "\n",
    "\n",
    "\n",
    "# Here is the response from the chat model.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7d8b2",
   "metadata": {},
   "source": [
    "### Web Search Tool (azure_genai_utils vs semantic_kernel.connectors.BingSearch)\n",
    "\n",
    "Web search tool is used to enhance the context. <br>\n",
    "\n",
    "It is used when all the documents do not meet the relevance threshold or the evaluator is not confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5444bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure_genai_utils.tools \n",
    "\n",
    "WEB_SEARCH_FORMAT_OUTPUT = False\n",
    "\n",
    "web_search_tool = azure_genai_utils.tools.BingSearch(\n",
    "    max_results=3,\n",
    "    locale=\"en-US\",\n",
    "    include_news=False,\n",
    "    include_entity=False,\n",
    "    format_output=WEB_SEARCH_FORMAT_OUTPUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Newest Openings Hotels in NYC 2024 2025?\"\n",
    "results = web_search_tool.invoke({\"query\": query})\n",
    "print(results[0].get(\"content\", \"No content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = [\n",
    "    \"What is today's weather in South Korea?\"\n",
    "]\n",
    "import semantic_kernel.connectors.search.bing\n",
    "from semantic_kernel.functions import KernelArguments, KernelParameterMetadata, KernelPlugin\n",
    "\n",
    "webplugin = KernelPlugin.from_text_search_with_search(\n",
    "        semantic_kernel.connectors.search.bing.BingSearch(api_key=os.getenv(\"BING_SUBSCRIPTION_KEY\")),\n",
    "        plugin_name=\"bing\",\n",
    "        description=\"Search the web for information.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\",\n",
    "                description=\"The search query.\",\n",
    "                type=\"str\",\n",
    "                is_required=True,\n",
    "                type_object=str,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"The number of results to return.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=1,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"skip\",\n",
    "                description=\"The number of results to skip.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=0,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            # KernelParameterMetadata(\n",
    "            #     name=\"site\",\n",
    "            #     description=\"The site to search.\",\n",
    "            #     default_value=\"https://github.com/microsoft/semantic-kernel/tree/main/python\",\n",
    "            #     type=\"str\",\n",
    "            #     is_required=False,\n",
    "            #     type_object=str,\n",
    "            # ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "        service=AzureChatCompletion(),\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions from web search results. Add the web search reference url to the answer.\",\n",
    "        plugins=[webplugin],\n",
    "    )\n",
    "\n",
    "# 2. Create a thread to hold the conversation\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ChatHistoryAgentThread = None\n",
    "\n",
    "for user_input in USER_INPUTS:\n",
    "    print(f\"# User: {user_input}\")\n",
    "    # 4. Invoke the agent for a response\n",
    "    response = await agent.get_response(messages=user_input, thread=thread)\n",
    "    print(f\"# {response.name}: {response} \")\n",
    "    thread = response.thread\n",
    "\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "# for user_input in USER_INPUTS:\n",
    "#         print(f\"# User: '{user_input}'\")\n",
    "#         first_chunk = True\n",
    "#         # 4. Invoke the agent for the current message and print the response\n",
    "#         async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "#             thread = response.thread\n",
    "#             if first_chunk:\n",
    "#                 print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "#                 first_chunk = False\n",
    "#             print(response.content, end=\"\", flush=True)\n",
    "#         print()\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d720d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 🧪 Step 2. Define the Agentic Architecture\n",
    "- Before building the agentic pipeline, we need to design the message, topic, agent and message routing logic. \n",
    "- You should define the terminate condition for the pipeline.\n",
    "\n",
    "### Message, Topic, Agent Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378d9fd",
   "metadata": {},
   "source": [
    "Visualizing the abstract architecture of the pipeline will help you understand the message flow and the agent's role in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa07a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/urllib3/util/retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/urllib3/connectionpool.py:367\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 29\u001b[0m\n\u001b[1;32m      3\u001b[0m agents \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntentRoutingStep\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     14\u001b[0m interactions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     15\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntentRoutingStep\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     16\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntentRoutingStep\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerateStep\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerates Response\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#(\"EvalAgent\", \"IntentRouterAgent\"),\u001b[39;00m\n\u001b[1;32m     27\u001b[0m ]\n\u001b[0;32m---> 29\u001b[0m \u001b[43mvisualize_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteractions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/azure_genai_utils/graphs.py:88\u001b[0m, in \u001b[0;36mvisualize_agents\u001b[0;34m(agents, interactions, mermaid_file)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmermaid_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Display the generated Mermaid diagram\u001b[39;00m\n\u001b[1;32m     86\u001b[0m display(\n\u001b[1;32m     87\u001b[0m     Image(\n\u001b[0;32m---> 88\u001b[0m         \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmermaid_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     93\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:214\u001b[0m, in \u001b[0;36mdraw_mermaid_png\u001b[0;34m(mermaid_syntax, output_file_path, draw_method, background_color, padding)\u001b[0m\n\u001b[1;32m    208\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    209\u001b[0m         _render_mermaid_using_pyppeteer(\n\u001b[1;32m    210\u001b[0m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m draw_method \u001b[38;5;241m==\u001b[39m MermaidDrawMethod\u001b[38;5;241m.\u001b[39mAPI:\n\u001b[0;32m--> 214\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     supported_methods \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:336\u001b[0m, in \u001b[0;36m_render_mermaid_using_api\u001b[0;34m(mermaid_syntax, output_file_path, background_color, file_type)\u001b[0m\n\u001b[1;32m    330\u001b[0m         background_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m image_url \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://mermaid.ink/img/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmermaid_syntax_encoded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&bgColor=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    338\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/anaconda/envs/venv_agent/lib/python3.11/site-packages/requests/adapters.py:713\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)"
     ]
    }
   ],
   "source": [
    "from azure_genai_utils.graphs import visualize_agents\n",
    "\n",
    "agents = [\n",
    "    \"Start\",\n",
    "    \"IntentRoutingStep\",\n",
    "    \"RAGGraderStep\",\n",
    "    \"QueryRewriteStep\",\n",
    "    \"EvalStep\",\n",
    "    \"KeywordRewriteStep\",\n",
    "    \"WebSearchStep\",\n",
    "    \"GenerateStep\",\n",
    "    \"User\"\n",
    "]\n",
    "interactions = [\n",
    "    (\"Start\", \"IntentRoutingStep\"),\n",
    "    (\"IntentRoutingStep\", \"GenerateStep\", \"Generates Response\"),\n",
    "    (\"IntentRoutingStep\", \"RAGGraderStep\", \"Retrieval Context\"),\n",
    "    (\"IntentRoutingStep\", \"KeywordRewriteStep\", \"Rewrites as keyword for bing search\"),\n",
    "    (\"GenerateStep\", \"EvalStep\"),\n",
    "    (\"EvalStep\", \"User\"),\n",
    "    (\"RAGGraderStep\", \"GenerateStep\", \"Generates Response\"),\n",
    "    (\"RAGGraderStep\", \"QueryRewriteStep\", \"Rewrites Query\"),\n",
    "    (\"QueryRewriteStep\", \"GenerateStep\"),\n",
    "    (\"KeywordRewriteStep\", \"WebSearchStep\"),\n",
    "    (\"WebSearchStep\", \"GenerateStep\")\n",
    "    #(\"EvalAgent\", \"IntentRouterAgent\"),\n",
    "]\n",
    "\n",
    "visualize_agents(agents, interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c98b3eb",
   "metadata": {},
   "source": [
    "This is an example of visualized pipeline\n",
    "\n",
    "![\"adaptive-RAG\"](../../images/adaptive-RAG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d1f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the events for the process\n",
    "class CommonEvents(Enum):\n",
    "    StartProcess = \"StartProcess\"\n",
    "    UserInputReceived = \"UserInputReceived\"\n",
    "    Exit = \"exit\"\n",
    "\n",
    "class ReflectionEvents(Enum):\n",
    "    \n",
    "    LowRAGGrade = \"LowRAGGrade\"\n",
    "    RAGGraderDone = \"RAGGraderDone\"\n",
    "    KeywordRewriteDone = \"KeywordRewriteDone\"\n",
    "    BingSearchDone = \"BingSearchDone\"\n",
    "    ResponseGenerated = \"ResponseGenerated\"\n",
    "    \n",
    "    RAGGraderRequested = \"RAGGraderRequested\"\n",
    "    BingSearchRequested = \"BingSearchRequested\"\n",
    "    GenerateRequested = \"GenerateRequested\"\n",
    "    \n",
    "@vectorstoremodel\n",
    "class HotelSampleClass(BaseModel):\n",
    "    HotelId: Annotated[str, VectorStoreRecordKeyField]\n",
    "    HotelName: Annotated[str | None, VectorStoreRecordDataField()] = None\n",
    "    Description: Annotated[\n",
    "        str,\n",
    "        VectorStoreRecordDataField(\n",
    "            has_embedding=True, embedding_property_name=\"DescriptionVector\", is_full_text_searchable=True\n",
    "        ),\n",
    "    ]\n",
    "    DescriptionVector: Annotated[\n",
    "        list[float] | None,\n",
    "        VectorStoreRecordVectorField(\n",
    "            dimensions=3072,\n",
    "            local_embedding=True,\n",
    "            embedding_settings={\"embedding\": OpenAIEmbeddingPromptExecutionSettings(dimensions=3072)},\n",
    "        ),\n",
    "    ] = None\n",
    "    Description_kr: Annotated[\n",
    "        str, VectorStoreRecordDataField(has_embedding=True, embedding_property_name=\"descriptionKOVector\")\n",
    "    ]\n",
    "    descriptionKOVector: Annotated[\n",
    "        list[float] | None,\n",
    "        VectorStoreRecordVectorField(\n",
    "            dimensions=3072,\n",
    "            local_embedding=True,\n",
    "            embedding_settings={\"embedding\": OpenAIEmbeddingPromptExecutionSettings(dimensions=3072)},\n",
    "        ),\n",
    "    ] = None\n",
    "    Category: Annotated[str, VectorStoreRecordDataField()]\n",
    "    Tags: Annotated[list[str], VectorStoreRecordDataField()]\n",
    "    ParkingIncluded: Annotated[bool | None, VectorStoreRecordDataField()] = None\n",
    "    LastRenovationDate: Annotated[str | None, VectorStoreRecordDataField()] = None\n",
    "    Rating: Annotated[float, VectorStoreRecordDataField()]\n",
    "    Location: Annotated[dict[str, Any], VectorStoreRecordDataField()]\n",
    "    Address: Annotated[dict[str, str | None], VectorStoreRecordDataField()]\n",
    "    Rooms: Annotated[list[dict[str, Any]], VectorStoreRecordDataField()]\n",
    "    \n",
    "\n",
    "\n",
    "class IntentType(str, Enum):\n",
    "    LLM = \"LLM\"\n",
    "    RAG = \"RAG\"\n",
    "    websearch = \"websearch\"    \n",
    "\n",
    "class IntentResponse(BaseModel):\n",
    "    intent_type: IntentType = Field(..., description=\"Processing status\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ee78e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_id = \"adaptive-rag\"\n",
    "import time\n",
    "\n",
    "class UserInputState(KernelBaseModel):\n",
    "    user_inputs: list[str] = []\n",
    "    current_input_index: int = 0\n",
    "\n",
    "class UserInputStep(KernelProcessStep[UserInputState]):\n",
    "    GET_USER_INPUT: ClassVar[str] = \"get_user_input\"\n",
    "\n",
    "    def create_default_state(self) -> \"UserInputState\":\n",
    "        \"\"\"Creates the default UserInputState.\"\"\"\n",
    "        return UserInputState()\n",
    "\n",
    "    def populate_user_inputs(self):\n",
    "        \"\"\"Method to be overridden by the user to populate with custom user messages.\"\"\"\n",
    "        pass\n",
    "\n",
    "    async def activate(self, state: KernelProcessStepState[UserInputState]):\n",
    "        \"\"\"Activates the step and sets the state.\"\"\"\n",
    "        state.state = state.state or self.create_default_state()\n",
    "        self.state = state.state\n",
    "        self.populate_user_inputs()\n",
    "    \n",
    "    @kernel_function(name=GET_USER_INPUT)\n",
    "    async def get_user_input(self, context: KernelProcessStepContext):\n",
    "        \"\"\"Gets the user input.\"\"\"\n",
    "        if not self.state:\n",
    "            raise ValueError(\"State has not been initialized\")\n",
    "\n",
    "        #user_message = input(\"USER: \")\n",
    "        # LLM\n",
    "        #user_message = \"I was wondering if you can tell me the history of Microsoft.\"\n",
    "        # RAG\n",
    "        #user_message = \"Can you recommend a few hotels with complimentary breakfast?\"\n",
    "        # web search\n",
    "        today = time.strftime(\"%Y-%m-%d\")\n",
    "        user_message = f\"what is today's breaking news in South Korea ({today})?\"\n",
    "        #user_message = \"Tell me about the newest hotels in NYC 2025?\"\n",
    "\n",
    "        \n",
    "        if \"exit\" in user_message:\n",
    "            await context.emit_event(process_event=CommonEvents.Exit, data=None)\n",
    "            return\n",
    "\n",
    "        self.state.current_input_index += 1\n",
    "        \n",
    "        data = {\n",
    "            \"query\": user_message\n",
    "        }\n",
    "        \n",
    "        print(f\"{'-'*80}\\n{self.GET_USER_INPUT}:\\n{user_message}\")\n",
    "        await context.emit_event(process_event=ReflectionEvents.RAGGraderRequested, data=data)\n",
    "        \n",
    "class IntentRoutingStep(KernelProcessStep):\n",
    "    INTENT_ROUTING: ClassVar[str] = \"IntentRoutingStep\"\n",
    "    \n",
    "    @kernel_function(name=INTENT_ROUTING)\n",
    "    async def do_it(self, context: KernelProcessStepContext, data:dict, kernel: Kernel):\n",
    "        \n",
    "        query = data.get(\"query\", \"\")\n",
    "        \n",
    "        req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n",
    "        req_settings.max_tokens = 2000\n",
    "        req_settings.temperature = 0.7\n",
    "        req_settings.response_format = IntentResponse\n",
    "        \n",
    "        # This prompt provides instructions to the model\n",
    "        INTENT_ROUTER_PROMPT=\"\"\"\n",
    "        You are an intelligent intent classifier that determines the best processing route for a user's question. Classify each question into one of the following categories:\n",
    "        LLM: Use this for casual conversation, greetings, small talk, or general knowledge questions that do not require up-to-date or domain-specific information.\n",
    "        RAG: Use this for questions related to hotel information, based on documents available up to August 2024.\n",
    "        websearch: Use this for questions that require the most recent information, or if the topic is not covered by LLM or RAG (e.g., breaking news, current events, or updates after August 2024).\n",
    "        Your only output should be the intent_type: either LLM, RAG, or websearch.\n",
    "        Do not explain your reasoning or provide any additional text—only return the intent_type.\n",
    "        Query: {{$query}}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        prompt_template_config = PromptTemplateConfig(\n",
    "            template=INTENT_ROUTER_PROMPT,\n",
    "            name=\"intentRouter\",\n",
    "            template_format=\"semantic-kernel\",\n",
    "            input_variables=[\n",
    "                InputVariable(name=\"query\", description=\"The user input\", is_required=True),\n",
    "            ],\n",
    "            execution_settings=req_settings,\n",
    "        )\n",
    "\n",
    "        intent_router = kernel.add_function(\n",
    "            function_name=\"intentRouterFunc\",\n",
    "            plugin_name=\"intentRouterPlugin\",\n",
    "            prompt_template_config=prompt_template_config,\n",
    "        )\n",
    "        \n",
    "        print(f\"{'-'*80}\\n{self.INTENT_ROUTING}:\\n{query}\")\n",
    "        \n",
    "        response = await kernel.invoke(intent_router, query=query)\n",
    "        print(response)\n",
    "        response_json = json.loads(response.value[0].items[0].text)\n",
    "        \n",
    "        data = {\n",
    "            \"query\": query,\n",
    "            \"intent\": response_json[\"intent_type\"]\n",
    "        }\n",
    "        \n",
    "        if response_json[\"intent_type\"] == \"RAG\":\n",
    "            await context.emit_event(process_event=ReflectionEvents.RAGGraderRequested, data=data)\n",
    "        elif response_json[\"intent_type\"] == \"websearch\":\n",
    "            await context.emit_event(process_event=ReflectionEvents.BingSearchRequested, data=data)\n",
    "        elif response_json[\"intent_type\"] == \"LLM\":\n",
    "            await context.emit_event(process_event=ReflectionEvents.GenerateRequested, data=data)\n",
    "\n",
    "class RAGGraderStep(KernelProcessStep):\n",
    "    RAG_GRADER: ClassVar[str] = \"RAGGraderStep\"\n",
    "    \n",
    "    retrieval_eval: RetrievalEvaluator | None = None\n",
    "    \n",
    "    @kernel_function(name=RAG_GRADER)\n",
    "    async def do_it(self, context: KernelProcessStepContext, data: dict, kernel: Kernel):\n",
    "        \n",
    "        query = data.get(\"query\", \"\")\n",
    "        intent = data.get(\"intent\", \"\")\n",
    "        retrieval_context = data.get(\"context\", \"\")\n",
    "\n",
    "\n",
    "        COLLECTION_NAME = \"hotels-sample-index\"\n",
    "        \n",
    "        \n",
    "        # Create the Azure AI Search collection\n",
    "        collection = AzureAISearchCollection[str, HotelSampleClass](\n",
    "            collection_name=COLLECTION_NAME, data_model_type=HotelSampleClass,\n",
    "        )\n",
    "        \n",
    "        embeddings = kernel.get_service(\"embedding\")\n",
    "                \n",
    "        query_vector = (await embeddings.generate_raw_embeddings(query))[0]\n",
    "        \n",
    "        \n",
    "        if retrieval_context == \"\":\n",
    "            results = await collection.vectorized_search(\n",
    "            vector=query_vector,\n",
    "            options=VectorSearchOptions(vector_field_name=\"descriptionVector\"),\n",
    "            )\n",
    "\n",
    "            async for result in results.results:\n",
    "                retrieval_context += (\n",
    "                    f\"    {result.record.HotelId} (in {result.record.Address['City']}, \"\n",
    "                    f\"{result.record.Address['Country']}): {result.record.Description} (score: {result.score})\\n\"\n",
    "                )\n",
    "\n",
    "        \n",
    "        data = {\n",
    "            \"query\": query,\n",
    "            \"intent\": intent,\n",
    "            \"context\": retrieval_context,\n",
    "        }\n",
    "        \n",
    "        \n",
    "        print(f\"{'-'*80}\\n{self.RAG_GRADER}:\\n{retrieval_context}\")\n",
    "        retrieval_score = self.retrieval_eval(**data)\n",
    "        print(retrieval_score)\n",
    "        \n",
    "        if retrieval_score[\"retrieval\"] >= 3.0:\n",
    "            await context.emit_event(process_event=ReflectionEvents.RAGGraderDone, data=data)\n",
    "        else:\n",
    "            await context.emit_event(process_event=ReflectionEvents.LowRAGGrade, data=data)\n",
    "        \n",
    "\n",
    "\n",
    "class KeywordRewriteStep(KernelProcessStep):\n",
    "    KEYWORD_REWRITE: ClassVar[str] = \"KeywordRewriteStep\"\n",
    "    \n",
    "    @kernel_function(name=KEYWORD_REWRITE)\n",
    "    async def do_it(self, context: KernelProcessStepContext, data:dict, kernel: Kernel):\n",
    "        \n",
    "        query = data.get(\"query\", \"\")\n",
    "        intent = data.get(\"intent\", \"\")\n",
    "        retrieval_context = data.get(\"context\", \"\")\n",
    "        \n",
    "        req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        # This prompt provides instructions to the model\n",
    "        KEYWORD_REWRITE_PROMPT=\"\"\"\n",
    "        You a keyword re-writer that converts an input question to a better version that is optimized for search. \n",
    "        Generate search keyword from a user query \n",
    "        to be more specific and detailed information, allowing for a more accurate response through web search.\n",
    "        Don't include the additional context from the user question such as location, date, etc.\n",
    "\n",
    "        Query: {{$query}}\n",
    "        Revised web search query:\n",
    "        \"\"\"\n",
    "\n",
    "        # Send the search results and the query to the LLM to generate a response based on the prompt.\n",
    "        prompt_template_config = PromptTemplateConfig(\n",
    "            template=KEYWORD_REWRITE_PROMPT,\n",
    "            name=\"keywordrewrite\",\n",
    "            template_format=\"semantic-kernel\",\n",
    "            input_variables=[\n",
    "                InputVariable(name=\"query\", description=\"The user input\", is_required=True)\n",
    "            ],\n",
    "            execution_settings=req_settings,\n",
    "        )\n",
    "\n",
    "        rewrite = kernel.add_function(\n",
    "            function_name=\"keywordrewriteFunc\",\n",
    "            plugin_name=\"keywordrewritePlugin\",\n",
    "            prompt_template_config=prompt_template_config,\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        response = await kernel.invoke(rewrite, query=query, \n",
    "                                       function_name=\"keywordrewriteFunc\")\n",
    "        \n",
    "        data = {\n",
    "            \"query\": response,\n",
    "            \"intent\": intent,\n",
    "            \"context\": retrieval_context,\n",
    "        }\n",
    "        \n",
    "        print(f\"{'-'*80}\\n{self.KEYWORD_REWRITE}:\\n{response}\")\n",
    "        await context.emit_event(process_event=ReflectionEvents.KeywordRewriteDone, data=data)\n",
    "  \n",
    "import azure_genai_utils.tools\n",
    "\n",
    "class BingSearchStep(KernelProcessStep):\n",
    "    BING_SEARCH: ClassVar[str] = \"BingSearchStep\"\n",
    "    web_search_tool: azure_genai_utils.tools.BingSearch | None = None\n",
    "\n",
    "    @kernel_function(name=BING_SEARCH)\n",
    "    async def do_it(self, context: KernelProcessStepContext, data: dict, kernel: Kernel):\n",
    "        \n",
    "        query = data.get(\"query\", \"\")\n",
    "        intent = data.get(\"intent\", \"\")\n",
    "        # Ensure query is a string\n",
    "        if not isinstance(query, str):\n",
    "            query = str(query)\n",
    "\n",
    "        retrieval_context = data.get(\"context\", \"\")\n",
    "\n",
    "        if not self.web_search_tool:\n",
    "            raise ValueError(\"web_search_tool is not initialized\")\n",
    "\n",
    "        results = self.web_search_tool.invoke(input={\"query\": query})\n",
    "        # Check if results is a list and has at least one element\n",
    "        if isinstance(results, list) and len(results) > 0:\n",
    "            retrieval_context = results[0].get(\"content\", \"No content available\") if results else \"No results found\"\n",
    "\n",
    "        data = {\n",
    "            \"query\": query,\n",
    "            \"intent\": intent,\n",
    "            \"context\": retrieval_context,\n",
    "        }\n",
    "        \n",
    "        print(f\"{'-'*80}\\n{self.BING_SEARCH}:\\n{retrieval_context}\")\n",
    "        await context.emit_event(process_event=ReflectionEvents.BingSearchDone, data=data)\n",
    "        \n",
    "class BingSearchPluginStep(KernelProcessStep):\n",
    "    BING_SEARCH_PLUGIN: ClassVar[str] = \"BingSearchPluginStep\"\n",
    "    \n",
    "    @kernel_function(name=BING_SEARCH_PLUGIN)\n",
    "    async def do_it(self, context: KernelProcessStepContext, data: dict, kernel: Kernel):\n",
    "        \n",
    "        query = data.get(\"query\", \"\")\n",
    "        intent = data.get(\"intent\", \"\")\n",
    "        # Ensure query is a string\n",
    "        if not isinstance(query, str):\n",
    "            query = str(query)\n",
    "\n",
    "        retrieval_context = data.get(\"context\", \"\")\n",
    "\n",
    "        bing_search_fuction = kernel.get_function(\"bing\", \"search\")\n",
    "\n",
    "        search_result = await kernel.invoke(\n",
    "            bing_search_fuction,\n",
    "            KernelArguments(\n",
    "                query=query,\n",
    "                top=5,\n",
    "                skip=0,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        retrieval_context = search_result if search_result else \"No results found\"\n",
    "\n",
    "        data = {\n",
    "            \"query\": query,\n",
    "            \"intent\": intent,\n",
    "            \"context\": retrieval_context,\n",
    "        }\n",
    "        \n",
    "        print(f\"{'-'*80}\\n{self.BING_SEARCH_PLUGIN}:\\n{retrieval_context}\")\n",
    "        await context.emit_event(process_event=ReflectionEvents.BingSearchDone, data=data)\n",
    "\n",
    "class GenerateStep(KernelProcessStep):\n",
    "    GENERATE: ClassVar[str] = \"GenerateStep\"\n",
    "    \n",
    "    @kernel_function(name=GENERATE)\n",
    "    async def do_it(self, context: KernelProcessStepContext, data:dict, kernel: Kernel):\n",
    "        \n",
    "        query = data.get(\"query\", \"\")\n",
    "        intent = data.get(\"intent\", \"\")\n",
    "        retrieval_context = data.get(\"context\", \"\")\n",
    "\n",
    "        print(\"\\nData Object:\")\n",
    "        for key, value in data.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n",
    "        \n",
    "        # This prompt provides instructions to the model\n",
    "        if intent == \"LLM\":\n",
    "            GROUNDED_PROMPT = \"\"\"\n",
    "            You are a kind and helpful assistant who answers general questions in a friendly, warm, and easy-to-understand way. Use emojis appropriately to make your response feel more engaging and approachable, just like you're chatting with a friend 😊.\n",
    "            ✨ Guidelines:\n",
    "            Be clear, concise, and helpful.\n",
    "            Add friendly expressions (e.g., “No worries!”, “Sure thing!”, “Happy to help!”).\n",
    "            Use emojis to enhance clarity or emotion, but don’t overuse them (1–3 per response is ideal).\n",
    "            If needed, break down complex concepts into simple terms, using analogies or examples.\n",
    "            End your answer with a positive tone or encouragement (e.g., “Let me know if you have more questions!” 👍).\n",
    "            Query: {{$query}}\n",
    "            \"\"\"\n",
    "        elif intent == \"RAG\":\n",
    "            GROUNDED_PROMPT = \"\"\"\n",
    "            Answer the query using only the context provided below in a friendly and concise bulleted manner.\n",
    "            Answer ONLY with the facts listed in the list of context below.\n",
    "            If there isn't enough information below, say you don't know.\n",
    "            Do not generate answers that don't use the context below.\n",
    "            Query: {{$query}}\n",
    "            Context:\\n{{$context}}\n",
    "            \"\"\"\n",
    "        elif intent == \"websearch\":\n",
    "            GROUNDED_PROMPT = \"\"\"\n",
    "            You are a kind and helpful assistant who answers general questions using only the context provided below in a friendly, warm, and easy-to-understand way. Use emojis appropriately to make your response feel more engaging and approachable, just like you're chatting with a friend 😊.\n",
    "            ✨ Guidelines:\n",
    "            Be clear, concise, and helpful.\n",
    "            Add friendly expressions (e.g., “No worries!”, “Sure thing!”, “Happy to help!”).\n",
    "            Use emojis to enhance clarity or emotion, but don’t overuse them (1–3 per response is ideal).\n",
    "            If needed, break down complex concepts into simple terms, using analogies or examples.\n",
    "            End your answer with a positive tone or encouragement (e.g., “Let me know if you have more questions!” 👍).\n",
    "            Query: {{$query}}\n",
    "            Context:\\n{{$context}}\n",
    "            \"\"\"\n",
    "\n",
    "        \n",
    "        # Send the search results and the query to the LLM to generate a response based on the prompt.\n",
    "        prompt_template_config = PromptTemplateConfig(\n",
    "            template=GROUNDED_PROMPT,\n",
    "            name=\"generate\",\n",
    "            template_format=\"semantic-kernel\",\n",
    "            input_variables=[\n",
    "                InputVariable(name=\"query\", description=\"The user input\", is_required=True),\n",
    "                InputVariable(name=\"context\", description=\"context\", is_required=True)\n",
    "            ],\n",
    "            execution_settings=req_settings,\n",
    "        )\n",
    "\n",
    "        generate = kernel.add_function(\n",
    "            function_name=\"generateFunc\",\n",
    "            plugin_name=\"generatePlugin\",\n",
    "            prompt_template_config=prompt_template_config,\n",
    "        )\n",
    "\n",
    "        print(f\"{'-'*80}\\n{self.GENERATE}:\\n\")\n",
    "        \n",
    "        \n",
    "        ########################################\n",
    "        # invoke_stream mode \n",
    "        ########################################\n",
    "        \n",
    "        response = kernel.invoke_stream(plugin_name=\"generatePlugin\", function_name=\"generateFunc\", query=query, context=retrieval_context)\n",
    "        all_chunks_str = \"\"\n",
    "        print(\"Assistant:> \", end=\"\")\n",
    "        async for chunk in response:\n",
    "            if isinstance(chunk[0], StreamingChatMessageContent) and chunk[0].role == AuthorRole.ASSISTANT:\n",
    "                all_chunks_str += str(chunk[0])\n",
    "                print(str(chunk[0]), end=\"\")\n",
    "            \n",
    "        data = {\n",
    "            \"query\": query,\n",
    "            \"intent\": intent,\n",
    "            \"context\": retrieval_context,\n",
    "            \"response\": all_chunks_str\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\")\n",
    "                \n",
    "        \n",
    "        await context.emit_event(process_event=ReflectionEvents.ResponseGenerated, data=data)\n",
    "        \n",
    "class EvalStep(KernelProcessStep):\n",
    "    EVALSTEP: ClassVar[str] = \"EvalStep\"\n",
    "    \n",
    "    groundedness_eval: GroundednessEvaluator | None = None\n",
    "    relevance_eval: RelevanceEvaluator | None = None\n",
    "    \n",
    "    @kernel_function(name=EVALSTEP)\n",
    "    async def do_it(self, context: KernelProcessStepContext, data: dict, kernel: Kernel):\n",
    "        \n",
    "        INCORRECT_ANSWER = \"\"\"\n",
    "        Hello, and thank you for bringing this to our attention! I may have provided an inaccurate or misleading response, and I sincerely apologize for the confusion.\n",
    "        As an AI, I aim to deliver helpful and accurate information, but sometimes I might misinterpret or generate an incorrect response. Your feedback is invaluable and helps me improve.\n",
    "\n",
    "        If you'd like, feel free to share more details or clarify your question, and I’ll do my best to assist you further. Thank you for your understanding and patience! 😊\n",
    "        \"\"\"\n",
    "                    \n",
    "        print(f\"{'-'*80}\\n{self.EVALSTEP}\\n\")\n",
    "        \n",
    "    \n",
    "        groundedness_score = self.groundedness_eval(**data)\n",
    "        print(groundedness_score)\n",
    "        \n",
    "        relevance_score = self.relevance_eval(**data)\n",
    "        print(relevance_score)\n",
    "        \n",
    "        if groundedness_score[\"groundedness\"] < 3.0 or relevance_score[\"relevance\"] < 3.0:\n",
    "            print(INCORRECT_ANSWER)\n",
    "        else:\n",
    "            print(\"Groundedness and relevance scores are acceptable.\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8fce6e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 🧪 Step 3. Execute the Workflow\n",
    "---\n",
    "\n",
    "### Execute the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c329af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding AzureTextEmbedding service\n",
      "Adding AzureChatCompletion service\n",
      "--------------------------------------------------------------------------------\n",
      "get_user_input:\n",
      "what is today's breaking news in South Korea (2025-04-10)?\n",
      "--------------------------------------------------------------------------------\n",
      "IntentRoutingStep:\n",
      "what is today's breaking news in South Korea (2025-04-10)?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"intent_type\":\"websearch\"}\n",
      "--------------------------------------------------------------------------------\n",
      "KeywordRewriteStep:\n",
      "breaking news in South Korea April 2025\n",
      "--------------------------------------------------------------------------------\n",
      "BingSearchPluginStep:\n",
      "South Korea’s top court voted unanimously to dismiss the country’s impeached president, Yoon Suk Yeol, on Friday. The decision clears the way for the election of a new leader after months of...,SEOUL: South Korean President Yoon Suk Yeol was ousted by the Constitutional Court on Friday, which upheld parliament’s impeachment motion over his imposition of martial law last year that ...,In the wake of the ruling, South Korea must now transition toward new political leadership. Under the Constitution, a presidential election must be held within 60 days following the removal of a president from office. With Yoon’s dismissal finalized on April 4, 2025, the country is poised for a pivotal electoral contest,Anti-Yoon protesters react after South Korea's Constitutional Court's verdict on the impeachment of South Korea president Yoon Suk Yeol in Seoul on April 4, 2025. South Korea's Constitutional ...,A supporter of impeached South Korean President Yoon Suk Yeol holds up a banner with images of the Constitutional Court of Korea’s judges, from left Lee Mi-son, Chung Kye-sun and Moon Hyung-bae, near the Constitutional Court in Seoul, South Korea, Tuesday, April 1, 2025.\n",
      "\n",
      "Data Object:\n",
      "query: breaking news in South Korea April 2025\n",
      "intent: websearch\n",
      "context: South Korea’s top court voted unanimously to dismiss the country’s impeached president, Yoon Suk Yeol, on Friday. The decision clears the way for the election of a new leader after months of...,SEOUL: South Korean President Yoon Suk Yeol was ousted by the Constitutional Court on Friday, which upheld parliament’s impeachment motion over his imposition of martial law last year that ...,In the wake of the ruling, South Korea must now transition toward new political leadership. Under the Constitution, a presidential election must be held within 60 days following the removal of a president from office. With Yoon’s dismissal finalized on April 4, 2025, the country is poised for a pivotal electoral contest,Anti-Yoon protesters react after South Korea's Constitutional Court's verdict on the impeachment of South Korea president Yoon Suk Yeol in Seoul on April 4, 2025. South Korea's Constitutional ...,A supporter of impeached South Korean President Yoon Suk Yeol holds up a banner with images of the Constitutional Court of Korea’s judges, from left Lee Mi-son, Chung Kye-sun and Moon Hyung-bae, near the Constitutional Court in Seoul, South Korea, Tuesday, April 1, 2025.\n",
      "--------------------------------------------------------------------------------\n",
      "GenerateStep:\n",
      "\n",
      "Assistant:> Wow, that sounds like a significant change happening in South Korea! 🇰🇷 Just to recap, on April 4, 2025, South Korea's top court unanimously decided to dismiss President Yoon Suk Yeol after upholding the impeachment motion related to his imposition of martial law. This means the country is now getting ready for new presidential elections, which are required to happen within 60 days of a president's removal.\n",
      "\n",
      "It’s a pivotal moment for South Korea as they prepare for fresh leadership! 🌟 If you have any other questions or need more details about this situation, feel free to ask! I'm here to help! 😊👍\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "EvalStep\n",
      "\n",
      "{'groundedness': 5.0, 'gpt_groundedness': 5.0, 'groundedness_reason': 'The RESPONSE accurately summarizes the key events from the CONTEXT and directly addresses the QUERY about breaking news in South Korea for April 2025. It includes all relevant details and does not introduce any extraneous information, demonstrating a complete understanding of the situation.'}\n",
      "{'relevance': 5.0, 'gpt_relevance': 5.0, 'relevance_reason': 'The RESPONSE is relevant and provides complete information about the breaking news in South Korea, including the dismissal of the president and the implications for upcoming elections. It directly addresses the QUERY without any inaccuracies or omissions.'}\n",
      "Groundedness and relevance scores are acceptable.\n",
      "Elapsed time: 9.21371677199204 seconds\n"
     ]
    }
   ],
   "source": [
    "kernel = Kernel()\n",
    "\n",
    "if kernel.services.get(\"embedding\") is None:\n",
    "    print(\"Adding AzureTextEmbedding service\")\n",
    "    embeddings = AzureTextEmbedding(\n",
    "        service_id=\"embedding\", \n",
    "        deployment_name=azure_openai_embedding_deployment_name,\n",
    "        endpoint=azure_openai_endpoint,\n",
    "        api_key=azure_openai_key,\n",
    "    )\n",
    "    kernel.add_service(embeddings)\n",
    "    \n",
    "if kernel.services.get(\"adaptive-rag\") is None:\n",
    "    print(\"Adding AzureChatCompletion service\")\n",
    "    chat_completion = AzureChatCompletion(\n",
    "        service_id=\"adaptive-rag\", \n",
    "        deployment_name=azure_openai_chat_deployment_name,\n",
    "        endpoint=azure_openai_endpoint,\n",
    "        api_key=azure_openai_key\n",
    "    )\n",
    "    kernel.add_service(chat_completion)\n",
    "\n",
    "kernel.add_plugin(KernelPlugin.from_text_search_with_search(\n",
    "        BingSearch(),\n",
    "        plugin_name=\"bing\",\n",
    "        description=\"Search the web information using bing search.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\",\n",
    "                description=\"The search query.\",\n",
    "                type=\"str\",\n",
    "                is_required=True,\n",
    "                type_object=str,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"The number of results to return.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=2,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"skip\",\n",
    "                description=\"The number of results to skip.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=0,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"site\",\n",
    "                description=\"The site to search.\",\n",
    "                default_value=\"\",\n",
    "                type=\"str\",\n",
    "                is_required=False,\n",
    "                type_object=str,\n",
    "            ),\n",
    "        ],\n",
    "    ))\n",
    "\n",
    "async def raggrader_step_factory():\n",
    "    retrieval_eval = RetrievalEvaluator(model_config)\n",
    "    rag_step_instance = RAGGraderStep()\n",
    "    rag_step_instance.retrieval_eval = retrieval_eval\n",
    "    return rag_step_instance\n",
    "\n",
    "import azure_genai_utils.tools\n",
    "async def bing_search_step_factory():\n",
    "    web_search_tool=azure_genai_utils.tools.BingSearch(\n",
    "        max_results=3,\n",
    "        locale=\"en-US\",\n",
    "        include_news=False,\n",
    "        include_entity=False,\n",
    "        format_output=False\n",
    "    )\n",
    "    \n",
    "    bing_step_instance = BingSearchStep()\n",
    "    bing_step_instance.web_search_tool = web_search_tool\n",
    "    return bing_step_instance\n",
    "\n",
    "async def eval_step_factory():\n",
    "    groundedness_eval = GroundednessEvaluator(model_config)\n",
    "    relevance_eval = RelevanceEvaluator(model_config)\n",
    "    step_instance = EvalStep()\n",
    "    step_instance.groundedness_eval = groundedness_eval\n",
    "    step_instance.relevance_eval = relevance_eval\n",
    "    return step_instance\n",
    "\n",
    "\n",
    "def build_process():\n",
    "    \n",
    "    process = ProcessBuilder(name=\"Test Process\")\n",
    "    # Add the step types to the builder\n",
    "    user_input_step = process.add_step(step_type=UserInputStep)\n",
    "    intent_routing_step = process.add_step(step_type=IntentRoutingStep)\n",
    "    RAG_grader_step = process.add_step(\n",
    "        step_type=RAGGraderStep, factory_function=raggrader_step_factory\n",
    "    )\n",
    "    keyword_rewrite_step = process.add_step(step_type=KeywordRewriteStep)\n",
    "    bing_search_step = process.add_step(step_type=BingSearchPluginStep)\n",
    "    # To use GenAIUtils BingSearch util, uncomment the line below\n",
    "    # bing_search_step = process.add_step(\n",
    "    #     step_type=BingSearchStep, factory_function=bing_search_step_factory\n",
    "    # )\n",
    "    generate_step = process.add_step(step_type=GenerateStep)\n",
    "    eval_step = process.add_step(step_type=EvalStep, factory_function=eval_step_factory)\n",
    "\n",
    "    \n",
    "    # Define the input event and where to send it to\n",
    "    process.on_input_event(event_id=CommonEvents.StartProcess).send_event_to(target=user_input_step)\n",
    "\n",
    "    # Define the process flow\n",
    "    user_input_step.on_event(event_id=ReflectionEvents.RAGGraderRequested).send_event_to(target=intent_routing_step, parameter_name=\"data\")\n",
    "    \n",
    "    intent_routing_step.on_event(event_id=ReflectionEvents.GenerateRequested).send_event_to(target=generate_step, parameter_name=\"data\")\n",
    "    intent_routing_step.on_event(event_id=ReflectionEvents.RAGGraderRequested).send_event_to(target=RAG_grader_step, parameter_name=\"data\")\n",
    "    intent_routing_step.on_event(event_id=ReflectionEvents.BingSearchRequested).send_event_to(target=keyword_rewrite_step, parameter_name=\"data\")\n",
    "    \n",
    "    RAG_grader_step.on_event(event_id=ReflectionEvents.RAGGraderDone).send_event_to(target=generate_step, parameter_name=\"data\")\n",
    "    RAG_grader_step.on_event(event_id=ReflectionEvents.LowRAGGrade).send_event_to(target=keyword_rewrite_step, parameter_name=\"data\")\n",
    "    \n",
    "    keyword_rewrite_step.on_event(event_id=ReflectionEvents.KeywordRewriteDone).send_event_to(target=bing_search_step, parameter_name=\"data\")\n",
    "    \n",
    "    bing_search_step.on_event(event_id=ReflectionEvents.BingSearchDone).send_event_to(target=generate_step, parameter_name=\"data\")\n",
    "    \n",
    "    generate_step.on_event(event_id=ReflectionEvents.ResponseGenerated).send_event_to(target=eval_step, parameter_name=\"data\")\n",
    "    \n",
    "\n",
    "    return process.build()\n",
    "\n",
    "\n",
    "async def corrective_rag_process():\n",
    "    \n",
    "    # Build the process\n",
    "    kernel_process = build_process()\n",
    "\n",
    "    # Start the process\n",
    "    await start(\n",
    "        process=kernel_process,\n",
    "        kernel=kernel,\n",
    "        initial_event=KernelProcessEvent(id=CommonEvents.StartProcess, data=[]),\n",
    "    )\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "await corrective_rag_process()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf28e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b216f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
