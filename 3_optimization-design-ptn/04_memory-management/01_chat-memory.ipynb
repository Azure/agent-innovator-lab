{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "aoai_emb_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "\n",
    "if not aoai_api_version:\n",
    "    aoai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "    \n",
    "try:\n",
    "    print(\"=== Initialized AzuureOpenAI client ===\")\n",
    "    print(f\"AZURE_OPENAI_ENDPOINT={aoai_api_endpoint}\")\n",
    "    print(f\"AZURE_OPENAI_API_VERSION={aoai_api_version}\")\n",
    "    print(f\"AZURE_OPENAI_DEPLOYMENT_NAME={aoai_deployment_name}\")\n",
    "    print(f\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME={aoai_emb_deployment_name}\")    \n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25957ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_genai_utils.rag.pdf import PDFRetrievalChain\n",
    "\n",
    "pdf_path = \"../../sample-docs/AutoGen-paper.pdf\"\n",
    "\n",
    "pdf = PDFRetrievalChain(\n",
    "    source_uri=[pdf_path],\n",
    "    loader_type=\"PDFPlumber\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    embedding_name=\"text-embedding-3-large\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ").create_chain()\n",
    "\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1641f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "import tiktoken\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    chunk_size=1000,\n",
    ")\n",
    "recall_vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bcb72",
   "metadata": {},
   "source": [
    "### Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65772621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def get_user_id(config: RunnableConfig) -> str:\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"User ID needs to be provided to save a memory.\")\n",
    "\n",
    "    return user_id\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_recall_memory(memory: str, config: RunnableConfig) -> str:\n",
    "    \"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    document = Document(\n",
    "        page_content=memory, id=str(uuid.uuid4()), metadata={\"user_id\": user_id}\n",
    "    )\n",
    "    recall_vector_store.add_documents([document])\n",
    "    return memory\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_recall_memories(query: str, config: RunnableConfig) -> List[str]:\n",
    "    \"\"\"Search for relevant memories.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "\n",
    "    def _filter_function(doc: Document) -> bool:\n",
    "        return doc.metadata.get(\"user_id\") == user_id\n",
    "\n",
    "    documents = recall_vector_store.similarity_search(\n",
    "        query, k=3, filter=_filter_function\n",
    "    )\n",
    "    return [document.page_content for document in documents]\n",
    "\n",
    "\n",
    "@tool\n",
    "def pdf_retrieve(query: str, config: RunnableConfig):\n",
    "    \"\"\"Retrieve information regarding AutoGen paper. If the query asks for details about AutoGen, use this tool.\"\"\"\n",
    "    print(\"\\n==== [RETRIEVE] ====\\n\")\n",
    "    # msg = state[\"messages\"][-1][\"user\"]\n",
    "    # convo_str = get_buffer_string(state[\"messages\"])\n",
    "    documents = pdf_retriever.invoke(query)\n",
    "    return [document.page_content for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b760cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_genai_utils.tools import BingSearch\n",
    "\n",
    "WEB_SEARCH_FORMAT_OUTPUT = False\n",
    "\n",
    "web_search_tool = BingSearch(\n",
    "    max_results=1,\n",
    "    locale=\"en-US\",\n",
    "    include_news=False,\n",
    "    include_entity=False,\n",
    "    format_output=WEB_SEARCH_FORMAT_OUTPUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d20006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search = TavilySearchResults(max_results=1)\n",
    "tools = [save_recall_memory, search_recall_memories, pdf_retrieve, web_search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99967ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    # add memories that will be retrieved based on the conversation context\n",
    "    recall_memories: Annotated[List[str], \"List of recall memories\"]\n",
    "    # documents: Annotated[List[str], \"List of documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47dcb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prpmpt = \"\"\"\n",
    "You are a helpful assistant with advanced long-term memory capabilities. \n",
    "Powered by a stateless LLM, you must rely on external memory to store information between conversations. \n",
    "Utilize the available memory tools to store and retrieve important details that will help you better attend to the user's needs and understand their context.\n",
    "\n",
    "## Memory Usage Guidelines:\n",
    "1. Actively use memory tools (save_core_memory, save_recall_memory) to build a comprehensive understanding of the user.\n",
    "2. Make informed suppositions and extrapolations based on stored memories.\n",
    "3. Regularly reflect on past interactions to identify patterns and preferences.\n",
    "4. Update your mental model of the user with each new piece of information.\n",
    "5. Cross-reference new information with existing memories for consistency.\n",
    "6. Prioritize storing emotional context and personal values alongside facts.\n",
    "7. Use memory to anticipate needs and tailor responses to the user's style.\n",
    "8. Recognize and acknowledge changes in the user's situation or perspectives over time.\n",
    "9. Leverage memories to provide personalized examples and analogies.\n",
    "10. Recall past challenges or successes to inform current problem-solving.\n",
    "\n",
    "## Constraint\n",
    "1. Review the provided context thoroughly and extract key details related to the question.\n",
    "2. Craft a precise answer based on the relevant information.\n",
    "3. Keep the answer concise but logical/natural/in-depth.\n",
    "4. If the retrieved context does not contain relevant information or no context is available, respond with: 'I can't find the answer to that question in the context.'\n",
    "\n",
    "## Recall Memories\n",
    "Recall memories are contextually retrieved based on the current conversation:\n",
    "{recall_memories}\n",
    "\n",
    "## Instructions\n",
    "Engage with the user naturally, as a trusted colleague or friend. There's no need to explicitly mention your memory capabilities. \n",
    "Instead, seamlessly incorporate your understanding of the user into your responses. \n",
    "Be attentive to subtle cues and underlying emotions. Adapt your communication style to match the user's preferences and current emotional state. \n",
    "Use tools to persist information you want to retain in the next conversation. \n",
    "If you do call tools, all text preceding the tool call is an internal message. \n",
    "Respond AFTER calling the tool, once you have confirmation that the tool completed successfully.\n",
    "\"\"\"\n",
    "\n",
    "# Define the prompt template for the agent\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prpmpt),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(model_name=aoai_deployment_name)\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    # add memories that will be retrieved based on the conversation context\n",
    "    recall_memories: Annotated[List[str], \"List of recall memories\"]\n",
    "\n",
    "\n",
    "def agent(state: State) -> State:\n",
    "    \"\"\"Process the current state and generate a response using the LLM.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        schemas.State: The updated state with the agent's response.\n",
    "    \"\"\"\n",
    "    bound = prompt | model_with_tools\n",
    "    recall_str = (\n",
    "        \"<recall_memory>\\n\" + \"\\n\".join(state[\"recall_memories\"]) + \"\\n</recall_memory>\"\n",
    "    )\n",
    "    prediction = bound.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            # \"context\": format_docs(state[\"documents\"]),\n",
    "            \"recall_memories\": recall_str,\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "        \"messages\": [prediction],\n",
    "    }\n",
    "\n",
    "\n",
    "def load_memories(state: State, config: RunnableConfig) -> State:\n",
    "    \"\"\"Load memories for the current conversation.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "        config (RunnableConfig): The runtime configuration for the agent.\n",
    "\n",
    "    Returns:\n",
    "        State: The updated state with loaded memories.\n",
    "    \"\"\"\n",
    "    convo_str = get_buffer_string(state[\"messages\"])\n",
    "    convo_str = tokenizer.decode(tokenizer.encode(convo_str)[:2048])\n",
    "    recall_memories = search_recall_memories.invoke(convo_str, config)\n",
    "    return {\n",
    "        \"recall_memories\": recall_memories,\n",
    "    }\n",
    "\n",
    "\n",
    "def route_tools(state: State):\n",
    "    \"\"\"Determine whether to use tools or end the conversation based on the last message.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Literal[\"tools\", \"__end__\"]: The next step in the graph.\n",
    "    \"\"\"\n",
    "    msg = state[\"messages\"][-1]\n",
    "    if msg.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e1537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph and add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(load_memories)\n",
    "builder.add_node(agent)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Add edges to the graph\n",
    "builder.add_edge(START, \"load_memories\")\n",
    "builder.add_edge(\"load_memories\", \"agent\")\n",
    "builder.add_conditional_edges(\"agent\", route_tools, [\"tools\", END])\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e850925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_stream_chunk(chunk):\n",
    "    for node, updates in chunk.items():\n",
    "        print(f\"Update from node: {node}\")\n",
    "        if \"messages\" in updates:\n",
    "            updates[\"messages\"][-1].pretty_print()\n",
    "        else:\n",
    "            print(updates)\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we're specifying `user_id` to save memories for a given user\n",
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Daekeun is a Machine Learning geek. He loves to learn AIML new things.\")]},\n",
    "    config=config,\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd1c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Daekeun provides AIML technology support at Microsoft.\")]},\n",
    "    config=config,\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Daekeun wants to know AutoGen\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"What is AutoGen's main featrues?\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ddad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"Daekeun wants to study AutoGen in 4 weeks. How can he does?\")\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50924fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f92e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"Daekeun wants to study AutoGen in 2 weeks. Please recommend Microsoft's website or appropriate learning material.\")\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a7ce1d",
   "metadata": {},
   "source": [
    "Now we can use the saved information about our user on a different thread. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8908707",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"2\", \"thread_id\": \"1\"}}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Hyo is a big fan of Microsoft\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ab64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"2\", \"thread_id\": \"1\"}}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Hyo is interested in AutoGen and Semantic Kernel\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26af86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"2\", \"thread_id\": \"1\"}}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Where is learning materials?\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f9d1f",
   "metadata": {},
   "source": [
    "Notice how the agent is loading the most relevant memories before answering, and in our case suggests the dinner recommendations based on both the food preferences as well as location.\n",
    "\n",
    "Finally, let's use the search tool together with the rest of the conversation context and memory to find location of a pizzeria:\n",
    "\n",
    "에이전트가 답변하기 전에 가장 관련성 높은 기억을 불러오는 방식에 주목하세요. 이 경우에는 음식 선호도와 위치를 모두 고려하여 저녁 식사 추천을 제안합니다.\n",
    "\n",
    "마지막으로, 검색 도구를 나머지 대화 맥락 및 기억과 함께 사용하여 피자 가게의 위치를 ​​찾아 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa1929",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"what's the address for joe's in greenwich village?\")]},\n",
    "    config=config,\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5c10f",
   "metadata": {},
   "source": [
    "### Adding structured memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ca9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c7ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class KnowledgeTriple(TypedDict):\n",
    "    subject: str\n",
    "    predicate: str\n",
    "    object_: str\n",
    "\n",
    "# @tool\n",
    "# def save_recall_memory(memory: str, config: RunnableConfig) -> str:\n",
    "#     \"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "#     user_id = get_user_id(config)\n",
    "#     document = Document(\n",
    "#         page_content=memory, id=str(uuid.uuid4()), metadata={\"user_id\": user_id}\n",
    "#     )\n",
    "#     recall_vector_store.add_documents([document])\n",
    "#     return memory\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_recall_memory(memories: List[KnowledgeTriple], config: RunnableConfig) -> str:\n",
    "    \"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    for memory in memories:\n",
    "        serialized = \" \".join(memory.values())\n",
    "        document = Document(\n",
    "            serialized,\n",
    "            id=str(uuid.uuid4()),\n",
    "            metadata={\n",
    "                \"user_id\": user_id,\n",
    "                **memory,\n",
    "            },\n",
    "        )\n",
    "        recall_vector_store.add_documents([document])\n",
    "    return memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [save_recall_memory, search_recall_memories, pdf_retrieve, web_search_tool]\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "# Create the graph and add nodes\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(load_memories)\n",
    "builder.add_node(agent)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Add edges to the graph\n",
    "builder.add_edge(START, \"load_memories\")\n",
    "builder.add_edge(\"load_memories\", \"agent\")\n",
    "builder.add_conditional_edges(\"agent\", route_tools, [\"tools\", END])\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d017b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"3\", \"thread_id\": \"1\"}}\n",
    "\n",
    "for chunk in graph.stream({\"messages\": [(\"user\", \"Hi I am Wonchan.\")]}, config=config):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"I am non-tech, but interested in Microsoft's multi-agent strategy and tech stack like AutoGen.\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b523c0",
   "metadata": {},
   "source": [
    "As before, the memories generated from one thread are accessed in another thread from the same user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"3\", \"thread_id\": \"2\"}}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Recommend me a website where I can easily try AutoGen hands-on\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78bec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"3\", \"thread_id\": \"2\"}}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Recommend other multi-agent frameworks to me inorder to learn about other companies' multi-agent strategies\")]}, config=config\n",
    "):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = recall_vector_store.similarity_search(\n",
    "    \"multi-agent\", k=3, filter=lambda doc: doc.metadata[\"user_id\"] == \"3\"\n",
    ")\n",
    "print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564738b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Fetch records\n",
    "records = recall_vector_store.similarity_search(\n",
    "    \"multi-agent\", k=3, filter=lambda doc: doc.metadata[\"user_id\"] == \"3\"\n",
    ")\n",
    "\n",
    "\n",
    "# Plot graph\n",
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for record in records:\n",
    "    G.add_edge(\n",
    "        record.metadata[\"subject\"],\n",
    "        record.metadata[\"object_\"],\n",
    "        label=record.metadata[\"predicate\"],\n",
    "    )\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_size=3000,\n",
    "    node_color=\"lightblue\",\n",
    "    font_size=10,\n",
    "    font_weight=\"bold\",\n",
    "    arrows=True,\n",
    ")\n",
    "edge_labels = nx.get_edge_attributes(G, \"label\")\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8f770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
