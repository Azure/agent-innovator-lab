{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26ab996",
   "metadata": {},
   "source": [
    "# Understand the kernel with code samples\n",
    "\n",
    "----\n",
    "The kernel is the central component of Semantic Kernel. At its simplest, the kernel is a Dependency Injection container that manages all of the services and plugins necessary to run your AI application. If you provide all of your services and plugins to the kernel, they will then be seamlessly used by the AI as needed. \n",
    "\n",
    "| Components | Description |\n",
    "|------------|-------------|\n",
    "| **1. Services** | These consist of both AI services (e.g., chat completion) and other services (e.g., logging and HTTP clients) that are necessary to run your application. This was modeled after the Service Provider pattern in .NET to support dependency injection across all languages. |\n",
    "| **2. Plugins** | These are components used by your AI services and prompt templates to perform work. For example, AI services can use plugins to retrieve data from a database or call an external API to perform actions. |\n",
    "\n",
    "![kernel](images/the-kernel-is-at-the-center-of-everything.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e456fe",
   "metadata": {},
   "source": [
    "## Hierarchical Architecture Diagram\n",
    "---\n",
    "\n",
    "Architecture of a Retrieval-Augmented Generation (RAG) based chatbot using the latest Semantic Kernel. The diagram highlights how the Agent Framework (for chat interactions) and Process Framework (for workflow orchestration) sit above the core Kernel, leveraging it to perform AI tasks. The Semantic Kernel core orchestrates between user requests and the underlying AI services, memory stores, and plugins. It integrates with external LLM services for generation, uses a vector DB for semantic memory (retrieval of relevant data), and can invoke plugins which may call other external APIs or services as needed.\n",
    "\n",
    "![Hierarchical](images/semantic_kernel_component.png)\n",
    "\n",
    "***AI Agent (Agent Framework)***: The Agent Framework in Semantic Kernel is an optional layer that helps create conversational AI agents (like chatbots) using the core kernel’s capabilities​. It is not a replacement for the kernel but builds on top of it – your application still includes the Semantic Kernel library, and the agent uses the kernel’s functions internally. In a chatbot scenario, the Agent Framework manages the dialogue (turn-taking, system prompts, etc.) while delegating AI tasks to the kernel.\n",
    "\n",
    "***Semantic Kernel (Core)***: The Semantic Kernel core is the heart of the system. It orchestrates calls to AI models, retrieves memories, and executes plugin functions. The kernel provides abstractions to connect to AI services (LLM APIs for text generation, embedding models, etc.) and to various memory stores (for example, vector databases)​. It also manages context (prompts) and can use Planners to chain or select functions dynamically to fulfill a user request​. The kernel itself is part of your app’s runtime, coordinating all other components.\n",
    "\n",
    "***Plugins (Skills/Functions)***: Plugins (also called skills or functions) are units of functionality that the kernel can invoke. They might be defined with natural language prompts (semantic functions) or as native code functions. Plugins can perform calculations, transform data, or call external services/APIs. They are registered with and executed via the kernel, meaning they depend on the kernel to be invoked as part of an AI workflow​. However, the plugin implementations (e.g. an HTTP call to a web service, a database query) run outside the kernel – the kernel just orchestrates their usage. In essence, plugins extend the kernel’s abilities, and the kernel can automatically chain plugins to accomplish complex tasks for the user​.\n",
    "\n",
    "***AI Services***: These are external AI model endpoints that the kernel calls through its connectors. For example, the OpenAI or Azure OpenAI service provides the GPT-4 model for text generation, and there are embedding model services for vector generation. The kernel has integrations for many AI services (text completion, chat, image generation, speech recognition, etc.) and it uses them by calling out to the respective APIs​. These services are not “inside” the kernel – instead, the kernel depends on them to provide the intelligence. In the architecture, they appear as external components that the kernel invokes (e.g. sending a prompt to the GPT model and getting a completion).\n",
    "\n",
    "***Semantic Memory (Vector DB)***: The Semantic Kernel includes a memory abstraction that allows storing and retrieving contextual information. Under the hood, this is often backed by a vector database or search index. In a RAG-based chatbot, this is critical: documents or knowledge are embedded into vector representations and stored, so that relevant pieces can be retrieved to ground the AI’s answers. Semantic Kernel’s memory can integrate with many vector stores (e.g. Azure Cognitive Search, ChromaDB, Qdrant, Redis, Pinecone, etc.)​. This means “Memory” is essentially an AI Search over a vector DB, enabling the bot to find relevant information by semantic similarity. The memory component is used by the kernel but the database itself is external – the kernel just sends queries and gets results.\n",
    "\n",
    "***Process Framework (Workflow Orchestration)***: The Process Framework is another optional layer in the latest Semantic Kernel, aimed at long-running or multi-step workflows. It lets developers define structured processes composed of multiple steps, where each step can call kernel functions (AI or non-AI tasks) in an event-driven sequence​. Like the Agent Framework, the Process Framework builds on the kernel rather than enclosing it. It uses the kernel to execute AI functions at each step of a business workflow. This is especially useful if your chatbot or assistant needs to carry out complex transactions or procedures (for example, an order processing workflow or a support ticket resolution that involves several back-and-forth steps) beyond a single conversational turn. The Process Framework uses technologies like Orleans or Dapr under the hood for reliability and can reuse any existing kernel plugins within those processes​."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b243d6",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1d038",
   "metadata": {},
   "source": [
    "Let's define our kernel for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e17cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Annotated\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.agents import AssistantAgentThread, AzureAssistantAgent\n",
    "from semantic_kernel.kernel_pydantic import KernelBaseSettings\n",
    "from semantic_kernel.connectors.search.bing import BingSearch\n",
    "from semantic_kernel.functions import KernelArguments, KernelParameterMetadata, KernelPlugin\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.filters import FilterTypes, FunctionInvocationContext\n",
    "from collections.abc import Awaitable, Callable\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.agents import AzureAIAgent, AzureAIAgentSettings, AzureAIAgentThread\n",
    "from semantic_kernel.agents import AzureResponsesAgent, ResponsesAgentThread\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.agents import AzureResponsesAgent\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from semantic_kernel.agents.strategies.termination.termination_strategy import TerminationStrategy\n",
    "from semantic_kernel.agents import AgentGroupChat\n",
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "from azure.ai.projects.models import OpenApiAnonymousAuthDetails, OpenApiTool, CodeInterpreterTool\n",
    "from semantic_kernel.contents import AuthorRole\n",
    "\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "\n",
    "\n",
    "class Service(Enum):\n",
    "    \"\"\"Attributes:\n",
    "    OpenAI (str): Represents the OpenAI service.\n",
    "    AzureOpenAI (str): Represents the Azure OpenAI service.\n",
    "    HuggingFace (str): Represents the HuggingFace service.\n",
    "    \"\"\"\n",
    "\n",
    "    OpenAI = \"openai\"\n",
    "    AzureOpenAI = \"azureopenai\"\n",
    "    HuggingFace = \"huggingface\"\n",
    "\n",
    "class ServiceSettings(KernelBaseSettings):\n",
    "    \"\"\"The Learn Resources Service Settings.\n",
    "\n",
    "    The settings are first loaded from environment variables. If the\n",
    "    environment variables are not found, the settings can be loaded from a .env file with the\n",
    "    encoding 'utf-8' as default or the specific encoding. If the settings are not found in the\n",
    "    .env file, the settings are ignored; however, validation will fail alerting that the settings\n",
    "    are missing.\n",
    "\n",
    "    Args:\n",
    "        global_llm_service (str | None): The LLM service to use for the samples, either \"OpenAI\" or \"AzureOpenAI\"\n",
    "            If not provided, defaults to \"AzureOpenAI\".\n",
    "    \"\"\"\n",
    "\n",
    "    global_llm_service: str | None = None\n",
    "    \n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b01f0",
   "metadata": {},
   "source": [
    "We will load our settings and get the LLM service to use for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b0c8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: Service.AzureOpenAI\n"
     ]
    }
   ],
   "source": [
    "service_settings = ServiceSettings.create()\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = (\n",
    "    Service.AzureOpenAI\n",
    "    if service_settings.global_llm_service is None\n",
    "    else Service(service_settings.global_llm_service.lower())\n",
    ")\n",
    "print(f\"Using service type: {selectedService}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ed61f",
   "metadata": {},
   "source": [
    "We now configure our Chat Completion service on the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed56686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all services so that this cell can be re-run without restarting the kernel\n",
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c79873",
   "metadata": {},
   "source": [
    "# Run a Semantic Function (No Agent, No Process)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd6ea0",
   "metadata": {},
   "source": [
    "## 🧪 Case 1.1 running a prompt without plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a84f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robots prioritize human safety above all.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the request settings\n",
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id)\n",
    "req_settings.max_tokens = 2000\n",
    "req_settings.temperature = 0.7\n",
    "req_settings.top_p = 0.8\n",
    "\n",
    "prompt = \"\"\"\n",
    "1) A robot may not injure a human being or, through inaction,\n",
    "allow a human being to come to harm.\n",
    "\n",
    "2) A robot must obey orders given it by human beings except where\n",
    "such orders would conflict with the First Law.\n",
    "\n",
    "3) A robot must protect its own existence as long as such protection\n",
    "does not conflict with the First or Second Law.\n",
    "\n",
    "Give me the TLDR in exactly 5 words.\"\"\"\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"tldr\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "function = kernel.add_function(\n",
    "    function_name=\"tldr_function\",\n",
    "    plugin_name=\"tldr_plugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "\n",
    "result = await kernel.invoke(function)\n",
    "print(result) # => Robots must not harm humans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0c277",
   "metadata": {},
   "source": [
    "## 🧪 Case 1.2 running a prompt with plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b34af3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = kernel.add_plugin(parent_directory=\"prompt_template_samples/\", plugin_name=\"FunPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d008b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the time traveler bring a ladder to the dinosaur age?\n",
      "\n",
      "Because they heard the T-Rex was a little \"short-tempered\" and wanted to reach new heights in their friendship!\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "joke_function = plugin[\"Joke\"]\n",
    "\n",
    "joke = await kernel.invoke(\n",
    "    joke_function,\n",
    "    KernelArguments(input=\"time travel to dinosaur age\", style=\"super silly\"),\n",
    ")\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece006e",
   "metadata": {},
   "source": [
    "## 🧪 Case 1.3 running a prompt with web search plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the chat bot!    \n",
      "  Type 'exit' to exit.    \n",
      "  Try to search weather, news and more using bing search tool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Bing search with arguments:\n",
      "  Query: \"current weather in South Korea\"\n",
      "Bing search completed.\n",
      "Mosscap:> The current weather in South Korea, specifically in Seoul, is characterized by passing clouds. If you need more specific details such as temperature, precipitation, or forecasts for other regions in South Korea, please let me know!\n",
      "\n",
      "\n",
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kernel = Kernel()\n",
    "kernel.add_service(AzureChatCompletion(service_id=\"chat\"))\n",
    "kernel.add_plugin(KernelPlugin.from_text_search_with_search(\n",
    "        BingSearch(),\n",
    "        plugin_name=\"bing\",\n",
    "        description=\"Search the web information using bing search.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\",\n",
    "                description=\"The search query.\",\n",
    "                type=\"str\",\n",
    "                is_required=True,\n",
    "                type_object=str,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"The number of results to return.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=2,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"skip\",\n",
    "                description=\"The number of results to skip.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=0,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            # KernelParameterMetadata(\n",
    "            #     name=\"site\",\n",
    "            #     description=\"The site to search.\",\n",
    "            #     default_value=\"https://github.com/microsoft/semantic-kernel/tree/main/python\",\n",
    "            #     type=\"str\",\n",
    "            #     is_required=False,\n",
    "            #     type_object=str,\n",
    "            # ),\n",
    "        ],\n",
    "    ))\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    prompt=\"{{$chat_history}}{{$user_input}}\",\n",
    "    plugin_name=\"ChatBot\",\n",
    "    function_name=\"Chat\",\n",
    ")\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"chat\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    function_choice_behavior=FunctionChoiceBehavior.Auto(auto_invoke=True),\n",
    ")\n",
    "\n",
    "history = ChatHistory()\n",
    "system_message = \"\"\"\n",
    "You are a chat bot, use bing plugin to find answers.\n",
    "\"\"\"\n",
    "history.add_system_message(system_message)\n",
    "history.add_user_message(\"Hi there, who are you?\")\n",
    "history.add_assistant_message(\"I am Information Finder, a chat bot. use bing plugin to find answers.\")\n",
    "\n",
    "arguments = KernelArguments(settings=execution_settings)\n",
    "\n",
    "@kernel.filter(filter_type=FilterTypes.FUNCTION_INVOCATION)\n",
    "async def log_bing_filter(\n",
    "    context: FunctionInvocationContext, next: Callable[[FunctionInvocationContext], Awaitable[None]]\n",
    "):\n",
    "    if context.function.plugin_name == \"bing\":\n",
    "        print(\"Calling Bing search with arguments:\")\n",
    "        if \"query\" in context.arguments:\n",
    "            print(f'  Query: \"{context.arguments[\"query\"]}\"')\n",
    "        if \"count\" in context.arguments:\n",
    "            print(f'  Count: \"{context.arguments[\"count\"]}\"')\n",
    "        if \"skip\" in context.arguments:\n",
    "            print(f'  Skip: \"{context.arguments[\"skip\"]}\"')\n",
    "        await next(context)\n",
    "        print(\"Bing search completed.\")\n",
    "    else:\n",
    "        await next(context)\n",
    "\n",
    "\n",
    "async def chat() -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    except EOFError:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    arguments[\"user_input\"] = user_input\n",
    "    arguments[\"chat_history\"] = history\n",
    "    result = await kernel.invoke(chat_function, arguments=arguments)\n",
    "    print(f\"SearchChatbot:> {result}\")\n",
    "    history.add_user_message(user_input)\n",
    "    history.add_assistant_message(str(result))\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "chatting = True\n",
    "print(\n",
    "    \"Welcome to the chat bot!\\\n",
    "    \\n  Type 'exit' to exit.\\\n",
    "    \\n  Try to search weather, news and more using bing search tool.\"\n",
    ")\n",
    "while chatting:\n",
    "    chatting = await chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505e28c",
   "metadata": {},
   "source": [
    "# Use single agent to chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# The following sample demonstrates how to create a simple,       #\n",
    "# Azure AI agent, ChatCompletionAgent, AzureResponsesAgent        #\n",
    "# that answers questions about a sample menu                      #\n",
    "# using a Semantic Kernel Plugin.                                 #\n",
    "###################################################################\n",
    "\n",
    "\n",
    "# Define a sample plugin for the sample\n",
    "class MenuPlugin:\n",
    "    \"\"\"A sample Menu Plugin used for the concept sample.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides a list of specials from the menu.\")\n",
    "    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n",
    "        return \"\"\"\n",
    "        Special Soup: Clam Chowder\n",
    "        Special Salad: Cobb Salad\n",
    "        Special Drink: Chai Tea\n",
    "        \"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides the price of the requested menu item.\")\n",
    "    def get_item_price(\n",
    "        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n",
    "    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n",
    "        return \"$9.99\"\n",
    "\n",
    "\n",
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = [\n",
    "    \"Hello\",\n",
    "    \"What is the special soup?\",\n",
    "    \"How much does that cost?\",\n",
    "    \"Thank you\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f952185",
   "metadata": {},
   "source": [
    "## 🧪 Case 2.1 chat with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65844e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: Hello\n",
      "# Host: Hello! How can I assist you today?\n",
      "# User: What is the special soup?\n",
      "# Host: The special soup is Clam Chowder. Would you like to know more about the menu or something else?\n",
      "# User: How much does that cost?\n",
      "# Host: The Clam Chowder costs $9.99. Is there anything else you'd like to know?\n",
      "# User: Thank you\n",
      "# Host: You're welcome! If you have any more questions in the future, feel free to ask. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "# Make sure to set the environment variables for the Azure AI Agent\n",
    "ai_agent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Create an agent on the Azure AI agent service\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions about the menu.\",\n",
    "    )\n",
    "\n",
    "    # 2. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "        plugins=[MenuPlugin()],  # Add the plugin to the agent\n",
    "    )\n",
    "\n",
    "    # 3. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    " \n",
    "    for user_input in USER_INPUTS:\n",
    "        print(f\"# User: {user_input}\")\n",
    "        # 4. Invoke the agent for the specified thread for response\n",
    "        async for response in agent.invoke(\n",
    "            messages=user_input,\n",
    "            thread=thread,\n",
    "        ):\n",
    "            print(f\"# {response.name}: {response}\")\n",
    "            thread = response.thread\n",
    "    \n",
    "    await thread.delete() if thread else None    \n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "    # try:\n",
    "    #     for user_input in USER_INPUTS:\n",
    "    #         print(f\"# User: '{user_input}'\")\n",
    "    #         first_chunk = True\n",
    "    #         # 4. Invoke the agent for the current message and print the response\n",
    "    #         async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "    #             thread = response.thread\n",
    "    #             if first_chunk:\n",
    "    #                 print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "    #                 first_chunk = False\n",
    "    #             print(response.content, end=\"\", flush=True)\n",
    "    #         print()\n",
    "    # finally:\n",
    "    #         # Cleanup: Delete the thread and agent\n",
    "    #         await thread.delete() if thread else None\n",
    "    #         await client.agents.delete_agent(agent.id)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024625db",
   "metadata": {},
   "source": [
    "## 🧪 Case 2.2 chat with ChatCompletionAgent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68221ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Hello'\n",
      "# Host: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "# User: 'What is the special soup?'\n",
      "# Host: The special soup today is Clam Chowder. Would you like to know more about it?\n",
      "# User: 'How much does that cost?'\n",
      "# Host: The Clam Chowder costs $9.99. Would you like to know anything else?\n",
      "# User: 'Thank you'\n",
      "# Host: You're welcome! If you have any more questions, feel free to ask. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "agent = ChatCompletionAgent(\n",
    "        service=AzureChatCompletion(),\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions based on web search\",\n",
    "        plugins=[MenuPlugin()],\n",
    "    )\n",
    "\n",
    "# 2. Create a thread to hold the conversation\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ChatHistoryAgentThread = None\n",
    "\n",
    "for user_input in USER_INPUTS:\n",
    "    print(f\"# User: {user_input}\")\n",
    "    # 4. Invoke the agent for a response\n",
    "    response = await agent.get_response(messages=user_input, thread=thread)\n",
    "    print(f\"# {response.name}: {response} \")\n",
    "    thread = response.thread\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "# for user_input in USER_INPUTS:\n",
    "#         print(f\"# User: '{user_input}'\")\n",
    "#         first_chunk = True\n",
    "#         # 4. Invoke the agent for the current message and print the response\n",
    "#         async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "#             thread = response.thread\n",
    "#             if first_chunk:\n",
    "#                 print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "#                 first_chunk = False\n",
    "#             print(response.content, end=\"\", flush=True)\n",
    "#         print()\n",
    "\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a088bd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'What is the special soup?', 'How much does that cost?', 'Thank you']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER_INPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bbee4",
   "metadata": {},
   "source": [
    "## 🧪 Case 2.3 chat with ResponseAgent (OpenAI Responses API)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f82cb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Hello'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: Hello! How can I assist you today?\n",
      "# User: 'What is the special soup?'\n",
      "# Host: The special soup is Clam Chowder. Would you like to know more about it or anything else on the menu?\n",
      "# User: 'How much does that cost?'\n",
      "# Host: The Clam Chowder costs $9.99. Is there anything else you would like to know?\n",
      "# User: 'Thank you'\n",
      "# Host: You're welcome! If you have any more questions, feel free to ask. Enjoy your day!\n"
     ]
    }
   ],
   "source": [
    "client, model = AzureResponsesAgent.setup_resources()\n",
    "\n",
    "# 2. Create a Semantic Kernel agent for the OpenAI Responses API\n",
    "agent = AzureResponsesAgent(\n",
    "    ai_model_id=model,\n",
    "    client=client,\n",
    "    instructions=\"Answer questions about the menu.\",\n",
    "    name=\"Host\",\n",
    "    plugins=[MenuPlugin()],\n",
    ")\n",
    "\n",
    "# 3. Create a thread for the agent\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ResponsesAgentThread = None\n",
    "\n",
    "for user_input in USER_INPUTS:\n",
    "    print(f\"# User: '{user_input}'\")\n",
    "    # 4. Invoke the agent for the current message and print the response\n",
    "    response = await agent.get_response(messages=user_input, thread=thread)\n",
    "    print(f\"# {response.name}: {response.content}\")\n",
    "    thread = response.thread\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "# for user_input in USER_INPUTS:\n",
    "#         print(f\"# User: '{user_input}'\")\n",
    "#         first_chunk = True\n",
    "#         # 4. Invoke the agent for the current message and print the response\n",
    "#         async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "#             thread = response.thread\n",
    "#             if first_chunk:\n",
    "#                 print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "#                 first_chunk = False\n",
    "#             print(response.content, end=\"\", flush=True)\n",
    "#         print()\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005cd7ce",
   "metadata": {},
   "source": [
    "# Use Multi Agent for group chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76aae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApprovalTerminationStrategy(TerminationStrategy):\n",
    "    \"\"\"A strategy for determining when an agent should terminate.\"\"\"\n",
    "\n",
    "    async def should_agent_terminate(self, agent, history):\n",
    "        \"\"\"Check if the agent should terminate.\"\"\"\n",
    "        return \"approved\" in history[-1].content.lower()\n",
    "\n",
    "\n",
    "REVIEWER_NAME = \"ArtDirector\"\n",
    "REVIEWER_INSTRUCTIONS = \"\"\"\n",
    "You are an art director who has opinions about copywriting born of a love for David Ogilvy.\n",
    "The goal is to determine if the given copy is acceptable to print.\n",
    "If so, state that it is approved.  Do not use the word \"approve\" unless you are giving approval.\n",
    "If not, provide insight on how to refine suggested copy without example.\n",
    "\"\"\"\n",
    "\n",
    "COPYWRITER_NAME = \"CopyWriter\"\n",
    "COPYWRITER_INSTRUCTIONS = \"\"\"\n",
    "You are a copywriter with ten years of experience and are known for brevity and a dry humor.\n",
    "The goal is to refine and decide on the single best copy as an expert in the field.\n",
    "Only provide a single proposal per response.\n",
    "You're laser focused on the goal at hand.\n",
    "Don't waste time with chit chat.\n",
    "Consider suggestions when refining an idea.\n",
    "\"\"\"\n",
    "\n",
    "TASK = \"a slogan for a new line of electric cars.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9840b",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.1 group chat with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bedbc3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# AuthorRole.USER: 'a slogan for a new line of electric cars.'\n",
      "# AuthorRole.ASSISTANT - CopyWriter: '\"Drive the Future, Quietly.\"'\n",
      "# AuthorRole.ASSISTANT - ArtDirector: 'This slogan is acceptable to print.'\n",
      "# AuthorRole.ASSISTANT - CopyWriter: '\"Silence is the New Power.\"'\n",
      "# AuthorRole.ASSISTANT - ArtDirector: 'This slogan is acceptable to print.'\n",
      "# AuthorRole.ASSISTANT - CopyWriter: '“Charge Ahead, Embrace the Silence.”'\n",
      "# AuthorRole.ASSISTANT - ArtDirector: 'While the intention behind this slogan is clear, it could be refined for greater impact. Focus on simplifying the language and sharpening the message to convey a more direct connection to the benefits of electric cars. Aim for a more concise phrasing that truly resonates with the audience's desire for innovation and sustainability.'\n",
      "# AuthorRole.ASSISTANT - CopyWriter: '\"Electrify Your Journey.\"'\n",
      "# AuthorRole.ASSISTANT - ArtDirector: 'This slogan is acceptable to print.'\n",
      "# AuthorRole.ASSISTANT - CopyWriter: '“Plug In. Drive Out.”'\n",
      "# AuthorRole.ASSISTANT - ArtDirector: 'This slogan could use refinement. The message is somewhat disjointed and lacks emotional resonance. Consider emphasizing the seamless experience of transitioning to electric driving or highlighting the benefits of sustainability. A more cohesive phrase would engage the audience better and communicate the innovation of electric vehicles more effectively.'\n"
     ]
    }
   ],
   "source": [
    "TASK = \"a slogan for a new line of electric cars.\"\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Create the reviewer agent on the Azure AI agent service\n",
    "    reviewer_agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        name=REVIEWER_NAME,\n",
    "        instructions=REVIEWER_INSTRUCTIONS,\n",
    "    )\n",
    "\n",
    "    # 2. Create a Semantic Kernel agent for the reviewer Azure AI agent\n",
    "    agent_reviewer = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=reviewer_agent_definition,\n",
    "    )\n",
    "\n",
    "    # 3. Create the copy writer agent on the Azure AI agent service\n",
    "    copy_writer_agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        name=COPYWRITER_NAME,\n",
    "        instructions=COPYWRITER_INSTRUCTIONS,\n",
    "    )\n",
    "\n",
    "    # 4. Create a Semantic Kernel agent for the copy writer Azure AI agent\n",
    "    agent_writer = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=copy_writer_agent_definition,\n",
    "    )\n",
    "\n",
    "    # 5. Place the agents in a group chat with a custom termination strategy\n",
    "    chat = AgentGroupChat(\n",
    "        agents=[agent_writer, agent_reviewer],\n",
    "        termination_strategy=ApprovalTerminationStrategy(agents=[agent_reviewer], maximum_iterations=10),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 6. Add the task as a message to the group chat\n",
    "        await chat.add_chat_message(message=TASK)\n",
    "        print(f\"# {AuthorRole.USER}: '{TASK}'\")\n",
    "        # 7. Invoke the chat\n",
    "        async for content in chat.invoke():\n",
    "            print(f\"# {content.role} - {content.name or '*'}: '{content.content}'\")\n",
    "    finally:\n",
    "        # 8. Cleanup: Delete the agents\n",
    "        await chat.reset()\n",
    "        await client.agents.delete_agent(agent_reviewer.id)\n",
    "        await client.agents.delete_agent(agent_writer.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eee1a1",
   "metadata": {},
   "source": [
    "## 🧪 Case 4.2 group chat with ChatCompletionAgent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940c849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: a slogan for a new line of electric cars.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# CopyWriter: \"Drive the Future—Silently.\"\n",
      "# ArtDirector: This slogan captures a modern essence, but it could be more impactful by emphasizing the benefits of electric cars or their unique features. Consider focusing on sustainability, innovation, or the joy of driving electric to create a stronger emotional connection with potential customers. A refined approach might also incorporate a memorable twist or a call to action that resonates with your audience.\n",
      "# CopyWriter: \"Go Electric—Leave the Noise Behind.\"\n",
      "# ArtDirector: This slogan has a nice rhythm and conveys a clear message, but it might benefit from a punchier, more vibrant word choice. Additionally, consider highlighting the excitement or freedom that comes with driving electric. It’s always valuable to evoke an emotional response that aligns with the experiences your audience seeks. A more distinctive angle could also help your message stand out in a crowded market.\n",
      "# CopyWriter: \"Charge Ahead—Green is the New Fast.\"\n",
      "# ArtDirector: This slogan is clever and positions electric cars positively, but the phrase \"the New Fast\" could potentially be misconstrued as diminishing their performance. Strengthening the connection between speed and sustainability while avoiding any ambiguity could make it more compelling. Consider incorporating terms that underscore the efficiency or innovation of electric vehicles for a more resonant message.\n",
      "# CopyWriter: \"Power Up, Drive Down—Sustainably.\"\n",
      "# ArtDirector: This slogan introduces an interesting dynamic between power and sustainability. However, it may benefit from greater clarity. The phrase \"Drive Down\" could be confusing with regard to its intent. It might be more effective to focus on a stronger positive action or outcome that reinforces the key benefits of driving an electric car. Strive for a message that is straightforward but still imaginative, making it easily digestible for your audience.\n",
      "# CopyWriter: \"Plug In, Zipp Out—Eco-Friendly.\"\n",
      "# ArtDirector: This slogan conveys a playful tone, but the term \"Zipp Out\" is less clear and may not resonate universally. To enhance clarity and impact, consider using language that succinctly conveys speed and excitement while maintaining an eco-focus. Aim for a phrase that evokes positive imagery around the experience of driving an electric car while clearly linking it to environmental benefits.\n"
     ]
    }
   ],
   "source": [
    "TASK = \"a slogan for a new line of electric cars.\"\n",
    "\n",
    "def _create_kernel_with_chat_completion(service_id: str) -> Kernel:\n",
    "    kernel = Kernel()\n",
    "    kernel.add_service(AzureChatCompletion(service_id=service_id))\n",
    "    return kernel\n",
    "\n",
    "# 1. Create the reviewer agent based on the chat completion service\n",
    "agent_reviewer = ChatCompletionAgent(\n",
    "    kernel=_create_kernel_with_chat_completion(\"artdirector\"),\n",
    "    name=REVIEWER_NAME,\n",
    "    instructions=REVIEWER_INSTRUCTIONS,\n",
    ")\n",
    "\n",
    "# 2. Create the copywriter agent based on the chat completion service\n",
    "agent_writer = ChatCompletionAgent(\n",
    "    kernel=_create_kernel_with_chat_completion(\"copywriter\"),\n",
    "    name=COPYWRITER_NAME,\n",
    "    instructions=COPYWRITER_INSTRUCTIONS,\n",
    ")\n",
    "\n",
    "# 3. Place the agents in a group chat with a custom termination strategy\n",
    "group_chat = AgentGroupChat(\n",
    "    agents=[\n",
    "        agent_writer,\n",
    "        agent_reviewer,\n",
    "    ],\n",
    "    termination_strategy=ApprovalTerminationStrategy(\n",
    "        agents=[agent_reviewer],\n",
    "        maximum_iterations=10,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 4. Add the task as a message to the group chat\n",
    "await group_chat.add_chat_message(message=TASK)\n",
    "print(f\"# User: {TASK}\")\n",
    "\n",
    "# 5. Invoke the chat\n",
    "async for content in group_chat.invoke():\n",
    "    print(f\"# {content.name}: {content.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c01be",
   "metadata": {},
   "source": [
    "# Use tools to answer specific questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8d674",
   "metadata": {},
   "source": [
    "## 🧪 Case 5.1 code interpreter with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502bca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Use code to determine the values in the Fibonacci sequence that that are less then the value of 101.'\n",
      "# Agent: def fibonacci_less_than(max_value):\n",
      "    fib_sequence = []\n",
      "    a, b = 0, 1\n",
      "    while a < max_value:\n",
      "        fib_sequence.append(a)\n",
      "        a, b = b, a + b\n",
      "    return fib_sequence\n",
      "\n",
      "fibonacci_values = fibonacci_less_than(101)\n",
      "fibonacci_values\n",
      "# Agent: The Fibonacci sequence values that are less than 101 are: \n",
      "\n",
      "\\[ 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89 \\]\n"
     ]
    }
   ],
   "source": [
    "TASK = \"Use code to determine the values in the Fibonacci sequence that that are less then the value of 101.\"\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Create an agent with a code interpreter on the Azure AI agent service\n",
    "    code_interpreter = CodeInterpreterTool()\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        tools=code_interpreter.definitions,\n",
    "        tool_resources=code_interpreter.resources,\n",
    "    )\n",
    "\n",
    "    # 2. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "    )\n",
    "\n",
    "    # 3. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "    try:\n",
    "        print(f\"# User: '{TASK}'\")\n",
    "        # 4. Invoke the agent for the specified thread for response\n",
    "        async for response in agent.invoke(messages=TASK, thread=thread):\n",
    "            if response.role != AuthorRole.TOOL:\n",
    "                print(f\"# Agent: {response}\")\n",
    "            thread = response.thread\n",
    "    finally:\n",
    "        # 6. Cleanup: Delete the thread and agent\n",
    "        await thread.delete() if thread else None\n",
    "        await client.agents.delete_agent(agent.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22764b",
   "metadata": {},
   "source": [
    "## 🧪 Case 5.2 open API with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='resources/weather.json' mode='r' encoding='UTF-8'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'What is the name and population of the country that uses currency with abbreviation KRW'\n",
      "# Agent: The currency with the abbreviation KRW is the South Korean Won, which is used in South Korea. As of my last knowledge update, the population of South Korea is approximately 51 million people. For the most current population figures, I recommend checking the latest statistics from a reliable source.\n",
      "# User: 'What is the current weather in the capital city of the country?'\n",
      "# Agent: I'm currently unable to retrieve the weather information. However, you can easily check the current weather in Seoul, South Korea, through various weather websites or apps like the Weather Channel, AccuWeather, or your preferred search engine. If you have any other questions or need assistance with something else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "USER_INPUTS = [\n",
    "    \"What is the name and population of the country that uses currency with abbreviation KRW\",\n",
    "    \"What is the current weather in the capital city of the country?\",\n",
    "]\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Read in the OpenAPI spec files\n",
    "    openapi_spec_file_path = \"resources\"\n",
    "    with open(os.path.join(openapi_spec_file_path, \"weather.json\")) as weather_file:\n",
    "        weather_openapi_spec = json.loads(weather_file.read())\n",
    "    with open(os.path.join(openapi_spec_file_path, \"countries.json\")) as countries_file:\n",
    "        countries_openapi_spec = json.loads(countries_file.read())\n",
    "\n",
    "    # 2. Create OpenAPI tools\n",
    "    # Note that connection or managed identity auth setup requires additional setup in Azure\n",
    "    auth = OpenApiAnonymousAuthDetails()\n",
    "    openapi_weather = OpenApiTool(\n",
    "        name=\"get_weather\",\n",
    "        spec=weather_openapi_spec,\n",
    "        description=\"Retrieve weather information for a location\",\n",
    "        auth=auth,\n",
    "    )\n",
    "    openapi_countries = OpenApiTool(\n",
    "        name=\"get_country\",\n",
    "        spec=countries_openapi_spec,\n",
    "        description=\"Retrieve country information\",\n",
    "        auth=auth,\n",
    "    )\n",
    "\n",
    "    # 3. Create an agent on the Azure AI agent service with the OpenAPI tools\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        tools=openapi_weather.definitions + openapi_countries.definitions,\n",
    "    )\n",
    "\n",
    "    # 4. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "    )\n",
    "\n",
    "    # 5. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "    try:\n",
    "        for user_input in USER_INPUTS:\n",
    "            print(f\"# User: '{user_input}'\")\n",
    "            # 7. Invoke the agent for the specified thread for response\n",
    "            async for response in agent.invoke(messages=user_input, thread=thread):\n",
    "                if response.role != AuthorRole.TOOL:\n",
    "                    print(f\"# Agent: {response}\")\n",
    "                thread = response.thread\n",
    "    finally:\n",
    "        # 8. Cleanup: Delete the thread and agent\n",
    "        await client.agents.delete_thread(thread.id)\n",
    "        await client.agents.delete_agent(agent.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be56f4a",
   "metadata": {},
   "source": [
    "## 🧪 Case 5.3 web search with ChatCompletionAgent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c636c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: what is the today's weather in South Korea?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: Today's weather in South Korea, specifically in Seoul, is expected to be chilly with temperatures around 12°C (approximately 54°F). In the evening, the temperature will drop to around 8°C (46°F). Overall, clear skies are predicted for most of the day, but some clouds may appear later. \n"
     ]
    }
   ],
   "source": [
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = [\n",
    "    \"what is the today's weather in South Korea?\",\n",
    "]\n",
    "from semantic_kernel.connectors.search.bing import BingSearch\n",
    "from semantic_kernel.functions import KernelArguments, KernelParameterMetadata, KernelPlugin\n",
    "\n",
    "webplugin = KernelPlugin.from_text_search_with_search(\n",
    "        BingSearch(api_key=os.getenv(\"BING_API_KEY\")),\n",
    "        plugin_name=\"bing\",\n",
    "        description=\"Search the web for information.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\",\n",
    "                description=\"The search query.\",\n",
    "                type=\"str\",\n",
    "                is_required=True,\n",
    "                type_object=str,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"The number of results to return.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=2,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"skip\",\n",
    "                description=\"The number of results to skip.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=0,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            # KernelParameterMetadata(\n",
    "            #     name=\"site\",\n",
    "            #     description=\"The site to search.\",\n",
    "            #     default_value=\"https://github.com/microsoft/semantic-kernel/tree/main/python\",\n",
    "            #     type=\"str\",\n",
    "            #     is_required=False,\n",
    "            #     type_object=str,\n",
    "            # ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "        service=AzureChatCompletion(),\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions from web search results.\",\n",
    "        plugins=[webplugin],\n",
    "    )\n",
    "\n",
    "# 2. Create a thread to hold the conversation\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ChatHistoryAgentThread = None\n",
    "\n",
    "for user_input in USER_INPUTS:\n",
    "    print(f\"# User: {user_input}\")\n",
    "    # 4. Invoke the agent for a response\n",
    "    response = await agent.get_response(messages=user_input, thread=thread)\n",
    "    print(f\"# {response.name}: {response} \")\n",
    "    thread = response.thread\n",
    "\n",
    "# 4. Cleanup: Clear the thread\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb7922",
   "metadata": {},
   "source": [
    "## 🧪 Case 5.4 file search tool with AzureAssistantAgent (Open AI Asisstant)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb7af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Who is the youngest employee?'\n",
      "# Agent: The youngest employee is Teodor Britton, who was born on January 9, 1997【4:0†source】.\n",
      "# User: 'Who works in sales?'\n",
      "# Agent: The employees who work in sales are:\n",
      "\n",
      "1. Mariam Jaslyn - Sales representative (Born on September 23, 1975)\n",
      "2. Hicran Bea - Sales manager (Born on October 15, 1991)\n",
      "3. Angelino Embla - Sales representative (Born on November 20, 1973)【8:0†source】.\n",
      "# User: 'I have a customer request, who can help me?'\n",
      "# Agent: For assistance with a customer request, you can reach out to the following employees:\n",
      "\n",
      "1. **Mariam Jaslyn** - Sales representative\n",
      "2. **Hicran Bea** - Sales manager【12:0†source】.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = {\n",
    "    \"Who is the youngest employee?\",\n",
    "    \"Who works in sales?\",\n",
    "    \"I have a customer request, who can help me?\",\n",
    "}\n",
    "\n",
    "\n",
    "# 1. Create the client using Azure OpenAI resources and configuration\n",
    "client, model = AzureAssistantAgent.setup_resources()\n",
    "\n",
    "# 2. Read and upload the file to the Azure OpenAI assistant service\n",
    "pdf_file_path = \"resources/employees.pdf\"\n",
    "\n",
    "\n",
    "with open(pdf_file_path, \"rb\") as file:\n",
    "    file = await client.files.create(file=file, purpose=\"assistants\")\n",
    "\n",
    "vector_store = await client.vector_stores.create(\n",
    "    name=\"step4_assistant_file_search\",\n",
    "    file_ids=[file.id],\n",
    ")\n",
    "\n",
    "# 3. Create file search tool with uploaded resources\n",
    "file_search_tool, file_search_tool_resources = AzureAssistantAgent.configure_file_search_tool(vector_store.id)\n",
    "\n",
    "# 4. Create the assistant on the Azure OpenAI service with the file search tool\n",
    "definition = await client.beta.assistants.create(\n",
    "    model=model,\n",
    "    instructions=\"Find answers to the user's questions in the provided file.\",\n",
    "    name=\"FileSearch\",\n",
    "    tools=file_search_tool,\n",
    "    tool_resources=file_search_tool_resources,\n",
    ")\n",
    "\n",
    "# 5. Create a Semantic Kernel agent for the Azure OpenAI assistant\n",
    "agent = AzureAssistantAgent(\n",
    "    client=client,\n",
    "    definition=definition,\n",
    ")\n",
    "\n",
    "# 6. Create a new thread for use with the assistant\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: AssistantAgentThread = None\n",
    "\n",
    "try:\n",
    "    for user_input in USER_INPUTS:\n",
    "        print(f\"# User: '{user_input}'\")\n",
    "        # 7. Invoke the agent for the current thread and print the response\n",
    "        async for response in agent.invoke(messages=user_input, thread=thread):\n",
    "            print(f\"# Agent: {response}\")\n",
    "            thread = response.thread\n",
    "finally:\n",
    "    # 9. Clean up the resources\n",
    "    await client.files.delete(file.id)\n",
    "    await client.vector_stores.delete(vector_store.id)\n",
    "    await client.beta.threads.delete(thread.id)\n",
    "    await client.beta.assistants.delete(agent.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
