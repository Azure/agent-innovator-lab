{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138d2cb2",
   "metadata": {},
   "source": [
    "# LLM with Web Search and Crawl\n",
    "\n",
    "Code to crawl the top n pages of a Google search result and serve them to LLM in order to utilize rich context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True) \n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "CHAT_COMPLETIONS_MODEL = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13ea70",
   "metadata": {},
   "source": [
    "bs4 or scrapy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import requests\n",
    "import json\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "import httpx\n",
    "import asyncio\n",
    "from urllib.parse import urljoin\n",
    "from azure.ai.projects.models import MessageRole, BingGroundingTool\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import sys\n",
    "import logging\n",
    "sys.path.append(os.path.abspath('..'))  # Adjust the path as necessary\n",
    "from utils.search_utils import web_search\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "BING_GROUNDING_PROJECT_CONNECTION_STRING = os.getenv(\"BING_GROUNDING_PROJECT_CONNECTION_STRING\")\n",
    "BING_GROUNDING_AGENT_ID = os.getenv(\"BING_GROUNDING_AGENT_ID\")\n",
    "BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME = os.getenv(\"BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME\")\n",
    "BING_GROUNDING_CONNECTION_NAME = os.getenv(\"BING_GROUNDING_CONNECTION_NAME\")\n",
    "# Web search mode: \"google\" or \"bing\"\n",
    "# it can be changed when users want to use different search engine\n",
    "WEB_CRAWLING_MODE = os.getenv(\"WEB_CRAWLING_MODE\") #on or off\n",
    "\n",
    "def extract_text_and_tables_by_bs4(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Extract main text\n",
    "    paragraphs = [p.get_text().strip() for p in soup.find_all(\"p\") if p.get_text().strip()]\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "async def extract_contexts_async(url_snippet_tuples: List[Tuple[str, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Asynchronously extract content from a list of URLs with their snippets.\n",
    "    \n",
    "    Args:\n",
    "        url_snippet_tuples: List of (url, snippet) pairs to process\n",
    "        \n",
    "    Returns:\n",
    "        List of extracted contents\n",
    "    \"\"\"\n",
    "    async def fetch(url: str, snippet: str) -> str:\n",
    "        # Try to get from cache first\n",
    "        \n",
    "        # If not in cache or cache unavailable, fetch the content\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            async with httpx.AsyncClient(timeout=10, follow_redirects=True) as client:\n",
    "                try:\n",
    "                    response = await client.get(url, headers=headers)\n",
    "                    response.raise_for_status()\n",
    "                except httpx.HTTPStatusError as e:\n",
    "                    # Handle redirects manually if needed\n",
    "                    if e.response.status_code == 302 and \"location\" in e.response.headers:\n",
    "                        redirect_url = e.response.headers[\"location\"]\n",
    "                        if not redirect_url.startswith(\"http\"):\n",
    "                            redirect_url = urljoin(url, redirect_url)\n",
    "                        try:\n",
    "                            response = await client.get(redirect_url, headers=headers)\n",
    "                            response.raise_for_status()\n",
    "                        except Exception as e2:\n",
    "                            print(f\"Redirect request failed: {e2}\")\n",
    "                            return f\"{snippet} \"\n",
    "                    else:\n",
    "                        print(f\"Request failed: {e}\")\n",
    "                        return f\"{snippet} \"\n",
    "                except httpx.HTTPError as e:\n",
    "                    print(f\"Request failed: {e}\")\n",
    "                    return f\"{snippet} \"\n",
    "                \n",
    "                # Parse the content\n",
    "                selector = scrapy.Selector(text=response.text)\n",
    "                \n",
    "                # Extract paragraphs\n",
    "                paragraphs = [p.strip() for p in selector.css('p::text, p *::text').getall() if p.strip()]\n",
    "                \n",
    "                # Remove duplicate and very short paragraphs\n",
    "                filtered_paragraphs = []\n",
    "                seen_content = set()\n",
    "                for p in paragraphs:\n",
    "                    # Skip very short paragraphs that are likely UI elements\n",
    "                    if len(p) < 5:\n",
    "                        continue\n",
    "                    # Avoid duplicate content\n",
    "                    if p in seen_content:\n",
    "                        continue\n",
    "                    seen_content.add(p)\n",
    "                    filtered_paragraphs.append(p)\n",
    "                \n",
    "                # Join the filtered paragraphs\n",
    "                text = \"\\n\".join(filtered_paragraphs)\n",
    "                \n",
    "                # If no paragraphs were found, try to get other text content\n",
    "                if not text:\n",
    "                    content_texts = [t.strip() for t in selector.css(\n",
    "                        'article::text, article *::text, .content::text, .content *::text, '\n",
    "                        'main::text, main *::text'\n",
    "                    ).getall() if t.strip()]\n",
    "                    \n",
    "                    if content_texts:\n",
    "                        text = \"\\n\".join(content_texts)\n",
    "                \n",
    "                # Combine snippet with extracted text\n",
    "                snippet_text = f\"{snippet}: {text}\"\n",
    "                \n",
    "                \n",
    "                return snippet_text\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {str(e)}\")\n",
    "            return f\"{snippet} [Error: {str(e)}]\"\n",
    "    \n",
    "    # Create tasks for all URLs\n",
    "    tasks = [asyncio.create_task(fetch(url, snippet)) \n",
    "            for url, snippet in url_snippet_tuples]\n",
    "    \n",
    "    # Execute all tasks concurrently\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Process results\n",
    "    processed_results = []\n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"Error processing URL {url_snippet_tuples[i][0]}: {str(result)}\")\n",
    "            processed_results.append(f\"{url_snippet_tuples[i][1]} [Processing Error]\")\n",
    "        else:\n",
    "            processed_results.append({\"content\": result, \"url_citation\" :{\"link\": url_snippet_tuples[i][0], \"title\": url_snippet_tuples[i][1]}})\n",
    "\n",
    "    return processed_results\n",
    "                    \n",
    "           \n",
    "\n",
    "       \n",
    "QUERY_REWRITE_PROMPT = \"\"\"\n",
    "            <<지시문>>\n",
    "            너는 구글 검색과 LLM 질의 최적화 전문가야. 사용자가 입력한 질문을 두 가지 목적에 맞게 재작성해.\n",
    "\n",
    "            1. Web Search용 Query Rewrite:\n",
    "            - 사용자의 질문을 실제 검색 엔진 검색창에 입력할 수 있도록, 명확하고 간결한 핵심 키워드 중심의 검색어로 재작성해.\n",
    "            - 불필요한 문장, 맥락 설명은 빼고, 검색에 최적화된 형태로 만들어.\n",
    "            - 핵심 키워드를 반복적으로 사용해 검색의 정확도를 높여.\n",
    "\n",
    "            2. LLM Query용 Rewrite:\n",
    "            - 사용자의 질문을 LLM이 더 잘 이해하고 답변할 수 있도록, 맥락과 의도를 명확히 드러내는 자연스러운 문장으로 재작성해.\n",
    "            - 필요한 경우 추가 설명이나 세부 조건을 포함해서 질문의 목적이 분명히 드러나도록 만들어.\n",
    "            - LLM이 답변에 집중할 수 있도록 핵심 단어를 반복 사용해.\n",
    "\n",
    "            <<예시>>\n",
    "            * 질문: 삼성전자 제품 중 2구 말고 다른 인덕션 추천해줘\n",
    "            * 웹 검색용 재작성: 삼성전자 3구 이상 인덕션 추천\n",
    "            * LLM 답변용 재작성: 삼성전자 인덕션 중 2구 모델이 아닌, 3구 이상 또는 다양한 화구 수를 가진 다른 인덕션 제품을 추천해 주세요. 각 모델의 주요 기능과 장점도 함께 알려주세요.\n",
    "\n",
    "            <<질문>>\n",
    "            {user_query}\n",
    "\n",
    "            <<출력포맷>>\n",
    "            반드시 아래와 같이 json 형식으로 출력해.\n",
    "            {\"web_search\": \"웹 검색용 재작성\", \"llm_query\": \"LLM 답변용 재작성\"}\n",
    "        \"\"\"     \n",
    "  \n",
    "def rewrite_query_for_search_and_llm(query, client: AzureOpenAI):\n",
    "        response = client.chat.completions.create(\n",
    "            model=CHAT_COMPLETIONS_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": QUERY_REWRITE_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0.8,\n",
    "            max_tokens=300,\n",
    "            response_format= {\"type\": \"json_object\"},\n",
    "        )\n",
    "        \n",
    "        return json.loads(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "#TODO 날씨나 뉴스, 기타 다른 특정정보는 Function Call\n",
    "# inputs = [\"날씨, 뉴스\"] ##\n",
    "\n",
    "async def process_web_search_call(RESULTS_COUNT, input, web_search_mode):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    contexts = [] \n",
    "    print(f\"Original Input: {input}\")\n",
    "    \n",
    "    query_rewrite = rewrite_query_for_search_and_llm(input, client)\n",
    "    print(f\"Web Search Query: {query_rewrite['web_search']}\")\n",
    "    print(f\"LLM Query: {query_rewrite['llm_query']}\")\n",
    "    \n",
    "    results = web_search(query_rewrite, RESULTS_COUNT, web_search_mode=web_search_mode)\n",
    "    print(f\"Web Search Results: {len(results)}, {results}\")\n",
    "    if web_search_mode == \"bing\" and results and isinstance(results, list) and len(results) > 0:\n",
    "        contexts = [results[i] for i in range(len(results))]\n",
    "        \n",
    "    elif web_search_mode == \"google\" and results and isinstance(results, list) and len(results) > 0:\n",
    "        url_snippet_tuples = [(r[\"link\"], r[\"snippet\"]) for r in results]\n",
    "        contexts = await extract_contexts_async(url_snippet_tuples)\n",
    "        \n",
    "    else:\n",
    "        print(\"No results found or invalid response from web_search.\")\n",
    "        contexts = [] \n",
    "    \n",
    "    # for i, context in enumerate(contexts):\n",
    "    #     print(f\"Context {i+1}: {context}...\")  # Print first 1000 chars of each context\n",
    "    #     print(\"\\n--- End of Context ---\\n\")\n",
    "\n",
    "    now = datetime.now()\n",
    "    year = now.year\n",
    "    month = now.month\n",
    "    day = now.day\n",
    "\n",
    "    \n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "        너는 삼성전자 제품 관련 정보를 제공하는 챗봇이야. \n",
    "        답변은 마크다운으로 이모지를 1~2개 포함해서 작성해줘. \n",
    "        contexts를 최대한 활용하여 풍부하게 답변을 해야해. \n",
    "        사용자가 질문한 내용에 대해 정확하고 유용한 정보를 제공해야 해. contexts가 부족하면 최소한의 안내만 해줘. \n",
    "        url_citation은 사용자가 클릭할 수 있도록 링크를 제공해줘.\n",
    "        \n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "        너는 아래 제공하는 웹검색에서 검색한 contexts를 바탕으로 질문에 대한 답변을 제공해야 해. \n",
    "        현재는 {year}년 {month}월 {day}일이므로 최신의 데이터를 기반으로 답변을 해줘.\n",
    "        웹검색에서 제공한 contexts: {contexts}\n",
    "        질문: {query_rewrite['llm_query']}\n",
    "        \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=CHAT_COMPLETIONS_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                 {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        top_p=0.9,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "\n",
    "    display(Markdown(response.choices[0].message.content))\n",
    "    end_time = time.time()\n",
    "    print(f\"elapsed time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_COUNT = 5\n",
    "\n",
    "inputs = [\n",
    "    \"삼성전자 제품 중 2구 말고 다른 인덕션 추천해줘\",\n",
    "    # \"부모님에게 선물하고 싶은데 삼성전자 TV 추천해줘\",\n",
    "    # \"삼성전자 25년 제품이 작년 대비 좋아진것은\",\n",
    "    # \"삼성전자 JBL과 하만카돈 차이점이 뭐야\",\n",
    "    # \"갤럭시 버즈 이어버드 한쪽을 새로 구매했는데 페어링 어떻게 하나요\",\n",
    "    # \"삼성전자 S25 무게가 S24와 비교 했을때 얼마나 차이나\"\n",
    "]\n",
    "\n",
    "\n",
    "web_search_mode = \"google\"\n",
    "\n",
    "for input in inputs:\n",
    "    \n",
    "    print(f\"Google Search API 사용: {input}\")\n",
    "    await process_web_search_call(RESULTS_COUNT, input, web_search_mode)\n",
    "\n",
    "web_search_mode = \"bing\"\n",
    "\n",
    "for input in inputs:\n",
    "    print(f\"Bing Grounding 검색 사용: {input}\")\n",
    "    await process_web_search_call(RESULTS_COUNT, input, web_search_mode)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5023d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RESULTS_COUNT = 5\n",
    "\n",
    "inputs = [\n",
    "    #\"삼성전자 제품 중 2구 말고 다른 인덕션 추천해줘\",\n",
    "    \"부모님에게 선물하고 싶은데 삼성전자 TV 추천해줘\",\n",
    "    \"삼성전자 25년 제품이 작년 대비 좋아진것은\",\n",
    "    # \"삼성전자 JBL과 하만카돈 차이점이 뭐야\",\n",
    "    # \"갤럭시 버즈 이어버드 한쪽을 새로 구매했는데 페어링 어떻게 하나요\",\n",
    "    # \"삼성전자 S25 무게가 S24와 비교 했을때 얼마나 차이나\"\n",
    "]\n",
    "\n",
    "\n",
    "web_search_mode = \"google\"\n",
    "\n",
    "for input in inputs:\n",
    "    \n",
    "    print(f\"Google Search API 사용: {input}\")\n",
    "    await process_web_search_call(RESULTS_COUNT, input, web_search_mode=web_search_mode)\n",
    "\n",
    "web_search_mode = \"bing\"\n",
    "\n",
    "for input in inputs:\n",
    "    print(f\"Bing Grounding 검색 사용: {input}\")\n",
    "    await process_web_search_call(RESULTS_COUNT, input, web_search_mode=web_search_mode)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa700e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97727298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c04f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agentlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
